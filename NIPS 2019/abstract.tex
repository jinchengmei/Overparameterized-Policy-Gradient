We propose a deep reinforcement learning algorithm that achieves nearly optimal finite time regret in the stochastic bandit setting. In the proposed approach, the agent maintains its action selection strategy as a parametric model. After sufficient exploration, a neural network is trained to minimize an empirical value estimate using gradient updates. The policy is obtained from the exponentiation of the learned logits. While this method differs from standard bandit algorithms, which directly utilize statistics from sampled data, we show that its finite time regret is nearly optimal up to log and constant factors. These results can be generalized to episodic MDPs and the state dependent bandit cases.