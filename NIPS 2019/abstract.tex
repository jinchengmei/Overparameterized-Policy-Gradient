We propose two deep reinforcement learning algorithms, Policy Gradient with Uniform Exploration (PGE), and Logit Learning with $\varepsilon$-Greedy Exploration (LLE). PGE satisfies sublinear regret in the stochastic bandit setting, while LLE achieves nearly optimal finite time regret. In the proposed approaches, the agent maintains its action selection strategy as a parametric model. With sufficient exploration, a neural network is trained to minimize an empirically estimated policy/value based loss  using gradient updates. The policy is obtained from the exponentiation of the learned logits. While the methods differ from standard bandit algorithms, which directly utilize statistics from sampled data, we show that the finite time regret is nearly optimal up to log and constant factors. These results can be generalized to episodic Markov decision processes and the state dependent bandit cases.