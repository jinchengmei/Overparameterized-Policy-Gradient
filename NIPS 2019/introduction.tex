\section{Introduction}
\label{sec:introduction}

Traditional reinforcement learning (RL) algorithms usually enjoy favorable theoretical guarantees under the the tabular cases of the bandit settings and the Markov decision process (MDP) setting \citep{sutton2018reinforcement}.
%In the traditional RL field, without the DL function approximations, there are many algorithms enjoying favorable theoretical guarantees, under the the tabular cases of the bandit settings and the Markov decision process (MDP) setting \citep{sutton2018reinforcement}. 
For example, (nearly) optimal algorithms have been discovered to achieve finite time regret upper bounds under the stochastic and adversarial bandit settings \citep{bubeck2012regret}. Q-learning is proved to be efficient in an episodic MDP setting \cite{jin2018q}. With linear and smooth function approximations, the Gradient Temporal Difference method and its variants have also been proved to converge to the fixed points of the projected Bellman operators, though the performance of the fixed-point policies can be arbitrarily bad, without imposing any further constraints on the function approximation classes \citep{sutton2009fast,sutton2009convergent,bhatnagar2009convergent}.

In recent years, deep reinforcement learning (DRL) methods have further improved traditional RL methods in practice and achieved a number of great successes, including professional level game players in Go \citep{silver2016masteringA,silver2017masteringB}, Atari \citep{mnih2015human}, Poker \citep{moravvcik2017deepstack}, and robotic controls \citep{lillicrap2015continuous,levine2016end}.
In particular, DRL methods optimize RL objectives during learning while employing deep neural networks as function approximations, thus combine the rich representation power of neural networks and the effective learning strategies from RL \citep{sutton2018reinforcement}. 
However, also mainly due to the complex behavior of neural networks, theoretical analyses of DRL methods become extremely difficult.
Existing analyses in the literature usually contain various unverifiable assumptions. \citep{krishnamurthy2016pac} assumes that the global optimal policy is contained in the parametric function class. \citep{dai2018sbeed} assumes having knowledge of the function approximation errors, which cannot be obtained in practice.
Moreover, these works do not particularly focus on neural networks. Although their results can be generally applied to DRL methods, their analyses do not reflect the improvement by using neural networks as function approximations. 

In this paper, we take one step further to theoretically understand the behavior of DRL methods. In particular, we propose two DRL methods, one value based and the other policy based. Our methods, while not exactly the same as popular DRL methods in the literature, catch the essences of value based and policy based DRL methods. To summarize, our contributions are as follows.
\begin{itemize}
	\item We propose a policy based RL method, Policy Gradient with Uniform Exploration (PGE), that is similar to the vanilla policy gradient method, but mixed with an uniform exploration and a momentum on the rewards. We prove that PGE, with over-parametrized neural network as its policy network, enjoys a sub-linear finite time convergence rate under the stochastic bandit setting.
	\item We further propose a value based RL method, Logit Learning with $\varepsilon$-Greedy Exploration (LLE), which, again with over-parametrized neural network as its value network, achieves a nearly optimal regret under the stochastic bandit setting. This algorithm learns value based objectives using the gradient descent. 
	\item We discuss how our results can be easily generalized to many other reinforcement learning settings, such as the state dependent stochastic bandit settings and episodic MDPs.
\end{itemize}

To our best knowledge, this paper is the first result in the literature on the optimization theory of RL methods with neural networks as function approximations. Our results can be seen as an initial step toward understanding the globally convergence for the popular RL methods (the value based learning, and the policy gradient here) with non-linear neural network function approximations, such as Deep Q-Network \cite{mnih2015human}, Asynchronous Actor-Critic Agents \citep{mnih2016asynchronous}, Path Consistency Learning \citep{nachum2017bridging}, and Relative Entropy Policy Search \citep{peters2010relative}, combining with more practical function approximations and training procedures.

Finally, let us comment on the technique side of our results. 
Our results are inspired by the recent progresses on over-parametrized neural networks for supervised learning (regression and classification) \citep{li2018learning,du2018gradientA,du2018gradientB,allen2018convergenceA,allen2018convergenceB}. 
However, because of the different learning settings between supervised learning and reinforcement learning, many challenges still persist. 
One standard challenge is the balance between exploration and exploitation in the RL/bandits problems.
Such balance in our problem becomes more complex, compared to that in the traditional multi-arm bandit setting, as the update of model is performed by gradient descent on the parameters.
The information gained from one arm will affect the evaluation of the other arms through the parameters of the network, which may help or hurdle the exploration of other arms.
Another challenge is that there is no true label provided by the environment, so the learning objective itself needs to be estimated from the collected data.
We propose to use the empirical average rewards instead of a single reward on the current step for the learning objective. 
Note that even so we still do NOT have an accurate estimate on the rewards.
To see that, as we hope that our policy can achieve a sub-linear (or nearly optimal) regret rate, only a limited budget (sub-linear or $O(\ln T )$) is allowed for the sub-optimal arms. Thus, as a sharp contrast to supervised learning with i.i.d. samples, we do NOT have enough samples from the sub-optimal arms for accurate estimates.  


%In the DL field, the well known situation is that the theoretical understanding is far behind with the practical applications \citep{goodfellow2016deep,zhang2016understanding}. Fortunately, there are still continuous progresses in several aspects, including expressiveness \citep{cybenko1989approximation,raghu2017expressive}, optimization \citep{kawaguchi2016deep,li2017convergence,li2018learning,du2018gradientA,du2018gradientB,allen2018convergenceA,allen2018convergenceB}, and generalization \cite{neyshabur2017exploring,allen2018learning} of the DL theory. 
%In particular, very recently, it has been discovered that, for over-parameterized neural networks, i.e., given that the numbers of parameters in the hidden layers are quite large (usually polynomial over the number of training data points), zero training loss can be achieved using the gradient descent and stochastic gradient descent methods, under the supervised learning (regression and classification) settings \citep{li2018learning,du2018gradientA,du2018gradientB,allen2018convergenceA,allen2018convergenceB}. With some additional structured data distribution assumptions, the convergent training loss can be generalized to the testing loss, endowing the learned neural network provable generalization abilities \citep{li2018learning,allen2018learning}.

%However, the recent progresses of the DL theory was developed under the supervised learning settings, which makes it not directly applicable in the RL settings, due to the significant differences between the RL and the supervised learning: (a) when learning RL agents, there is no true label provided by the environment, so the learning objective itself needs to be estimated from the collected data, which makes the RL problems more difficult than optimization problems in the supervised learning; (b) the interaction between the  agent and the environment will also incur losses during learning, which makes the principled ways of exploration matter.

%In this paper, based on the recent work in the optimization theory of over-parameterized neural networks, we take one step forward to theoretically understanding the DRL methods. In particular, our contributions are as follows.
%\begin{itemize}
   % \item We propose Logit Learning with $\varepsilon$-Greedy Exploration, which achieves optimal regret under the stochastic bandit setting. The algorithms learn value based objectives using the gradient descent, with value functions parameterized by neural networks. The results can be generalized to the state dependent stochastic bandit settings, and the episodic MDP settings.
    %\item We prove that the widely used policy gradient methods with uniform exploration also enjoys sublinear finite time regret under the stochastic bandit setting.
%\end{itemize}



The rest of the paper is organized as follows. \cref{subsec:notations} presents the notations.   \cref{sec:background} introduces the background, settings and the network structures. \cref{sec:policy_gradient} proposes the policy gradient algorithm. \cref{sec:logit_learning} proposes the logit learning algorithm. \cref{sec:general_settings} briefly shows the results in several more general settings. \cref{sec:future_work} discusses some open problems and future work. \cref{sec:conclusions} comes to our conclusions. All the formal proofs are deferred to the appendix due to the space limit.

\subsection{Notations}
\label{subsec:notations}
We use bold lowercase letters to refer to vectors, such as $\rvr$, and bold capital letters to refer to matrices, like $\rmW$. Non-bold lowercase letters are scalars. For example, $u_{i,r} \in \sR$ is the $r$th component of vector $\rvu_i \in \sR^m$. $\rvone$ means an all-one vector, and $\rmI$ refers to an identity matrix, with dimensions depend on the contexts. We also denote $[n] \triangleq \left\{ 1,2, \dots, n \right\}$. 

In the sequel, $n$ is the total number of states, and $h$ is the total number of actions that can be taken at each state.
Also, for $i\in[n]$, let $\rvr_i \in \sR^h$ be the true mean reward vector at state $\rvs_i$. $\rvpi_i^* = \argmax_{\rvpi \in \Delta^{h}}{\left\{ \rvpi^\top \rvr_i \right\}}$ is the optimal policy at state $\rvs_i$. Given a sequence of  stochastic policies $\bf{\Pi} \triangleq \left[ \rvpi_1, \rvpi_2, \dots, \rvpi_n \right] \in {\sR}^{h \times n}$, the expected (mean) loss is defined as,
\begin{equation}
\label{eq:expected_loss}
\begin{split}
\frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \left( {\rvpi_i^*}^\top \rvr_i - \rvpi_i^\top \rvr_i \right) }.
\end{split}
\end{equation}

We use two-layer fully connected neural networks with ReLU activation as our value function and policy function approximations, as shown in \cref{fig:nn_policy_value}. 
%The structure of the value network and the policy network  is a 2-layers fully connected neural network with ReLU activation, 
Each neural network takes the state $\rvs \in \sR^d$ as its input. 
For the first layer, we use $\rvs_i \in \sR^d$ for $i \in [n]$ refers to the feature vector of the $i$th state, $\rvw_r \in \sR^d$ for $r \in [m]$ to a weight vector in the first hidden layer, and $u_{i,r} \triangleq \rvw_r^\top \rvs_i$ to the $r$th node value of the first hidden layer.
Let $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right] \in \sR^{d \times m}$ be the weight matrix of the first hidden layer, thus the hidden layer is $\rvu = \rmW\rvs\in\sR^m$.

Similarly let $\rva_k \in \sR^m$ for $k \in [h]$ be a weight vector in the second hidden layer, and $o_{i,k} \triangleq \sum_{r=1}^{m}{a_{k,r} \cdot \sigma\left( u_{i,r} \right)}$ be the logit of the $k$th action for state $\rvs_i$, where $\sigma(\cdot) = \max\left\{ \cdot, 0 \right\}$ is the ReLU activation function. 
Let $\rmA^\top \triangleq \left[ \rva_1, \rva_2, \dots, \rva_h \right] \in \sR^{m \times h}$ be the weight matrix of the second hidden layer, then the logit output $\rvo = \rmA\sigma\left( \rvu\right) \in \sR^h$.

Lastly, the policy network differs the value network with one additional softmax transform layer in order to output probability distributions, where $\rvpi \triangleq f\left( \rvo \right) = f\left( \rmA \sigma\left( \rmW \rvs \right) \right)$ and $f$ is the softmax function: $f\left( o_{i,k} \right) = \exp\left\{ o_{i,k} \right\} \,/\, \sum_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime} \right\}}$.