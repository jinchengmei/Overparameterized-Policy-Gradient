\section{Introduction}
\label{sec:introduction}

In the recent years, the machine learning community has witnessed a number of great successes achieved by the deep reinforcement learning (DRL) methods, including professional level game players in Go \citep{silver2016masteringA,silver2017masteringB}, Atari \citep{mnih2015human}, Poker \citep{moravvcik2017deepstack}, and robotic controls \citep{lillicrap2015continuous,levine2016end}, to name but a few. DRL combines methodologies of deep learning (DL) and reinforcement learning (RL). In particular, RL objectives are optimized during learning, while deep neural networks are employed as function approximations \citep{sutton2018reinforcement}. Also, mainly because of this combination, it is well known that the DRL methods usually do not have theoretical justifications that is able to explain or support its good performances in practice.

In the traditional RL field, without the DL function approximations, there are many algorithms enjoying favorable theoretical guarantees, under the the tabular cases of the bandit settings and the Markov decision process (MDP) setting \citep{sutton2018reinforcement}. For example, (nearly) optimal algorithms have been discovered to achieve finite time regret upper bounds under the stochastic and adversarial bandit settings \citep{bubeck2012regret}. Several traditional RL methods are also developed to work beyond the tabular cases, while the results are much weaker. For example, with linear and smooth non-linear function approximations, the Gradient Temporal Difference (GTD) method and its variants have been proved to converge to the fixed points of the projected Bellman operators \citep{sutton2009fast,sutton2009convergent,bhatnagar2009convergent}. However, the performance of the induced policies at the fixed points can be arbitrarily bad, without imposing any further constraints on the function approximation classes.

In the DL field, the well known situation is that the theoretical understanding is far behind with the practical applications \citep{goodfellow2016deep,zhang2016understanding}. Fortunately, there are still continuous progresses in several aspects, including expressiveness \citep{cybenko1989approximation,raghu2017expressive}, optimization \citep{kawaguchi2016deep,li2017convergence,li2018learning,du2018gradientA,du2018gradientB,allen2018convergenceA,allen2018convergenceB}, and generalization \cite{neyshabur2017exploring,allen2018learning} of the DL theory. In particular, very recently, it has been discovered that, for over-parameterized neural networks, i.e., given that the numbers of parameters in the hidden layers are quite large (usually polynomial over the number of training data points), zero training loss can be achieved using the gradient descent (GD) and stochastic gradient descent (SGD) methods, under the supervised learning (regression and classification) settings \citep{li2018learning,du2018gradientA,du2018gradientB,allen2018convergenceA,allen2018convergenceB}. With some additional structured data distribution assumptions, the convergent training loss can be generalized to the testing loss, endowing the learned neural network provable generalization abilities \citep{li2018learning,allen2018learning}.

However, the recent progresses of the DL theory was developed under the supervised learning settings, which makes it not directly applicable in the RL settings, due to the significant differences between the RL and the supervised learning: (a) when learning RL agents, there is no true label provided by the environment, so the learning objective itself needs to be estimated from the collected data, which makes the RL problems more difficult than optimization problems in the supervised learning; (b) the interaction between the  agent and the environment will also incur losses during learning, which makes the principled ways of exploration matter.

In this paper, based on the recent work in the optimization theory of over-parameterized neural networks, we take one step forward to theoretically understanding the DRL methods. In particular, our contributions are as follows.

\begin{itemize}
    \item We propose two algorithms, i.e., Logit Learning with Uniform Exploration, and Logit Learning with $\epsilon$-Greedy Exploration, which can achieve optimal finite time regret under the stochastic bandit setting. The algorithms learn value related objectives using the gradient descent, with value functions parameterized by neural networks. The results can be generalized to the many state dependent stochastic bandit settings, and the episodic MDP settings.
    \item We prove that the widely used policy gradient methods with uniform exploration also enjoys sublinear finite time regret under the stochastic bandit setting.
\end{itemize}

Our findings provide the globally convergent analysis for the popular RL methods (the value based learning, and the policy gradient here) with non-linear neural network function approximations, which  theoretically support the  the DRL methods. Unlike the existing work, the results here do not contain unverifiable assumptions, such as the global optimal policies are contained in the function approximations \citep{krishnamurthy2016pac}, or knowledge of the function approximation errors \citep{dai2018sbeed}. Our results are at the beginning of understanding the behavior of many other DRL methods, such as Deep Q-Network \cite{mnih2015human}, Asynchronous Actor-Critic Agents \citep{mnih2016asynchronous}, Path Consistency Learing \citep{nachum2017bridging}, and Relative Entropy Policy Search \citep{peters2010relative}, combining with more practical function approximations and training procedures, e.g., less over-parameterized neural networks.

The rest of the paper is organized as follows. \cref{sec:background} introduces the background, including the notations in \cref{subsec:notations}, the stochastic bandit settings in  \cref{subsec:settings}, value and policy neural network structures in  \cref{subsec:nn_value_policy}, and the proposed algorithms in \cref{subsec:logit_learning} and  \cref{subsec:policy_gradient}. \cref{sec:theoretical_analyses} presents our theoretical analyses. \cref{sec:general_settings} briefly shows the results in several more general settings. \cref{sec:future_work} discusses some open problems and future work. \cref{sec:conclusions} comes to our conclusions. The proofs of technical lemmas are deferred to the appendix due to the space limit.

\subsection{Notations}
\label{subsec:notations}

We use bold lowercase letters to refer to vectors, such as $\rvr$, and bold capital letters to refer to matrices, like $\rmW$. non-bold lowercase letters are scalars. For example, $u_{i,r} \in \sR$ is the $r$th component of vector $\rvu_i \in \sR^m$. $\rvone$ means an all-one vector, and $\rmI$ refers to an identity matrix, with dimensions depend on the contexts.

In the sequel, $n$ is the total number of states, while $m$ is the total number of nodes in each hidden layer. $h$ is the total number of actions can be taken at each state.

Denote $[n] \triangleq \left\{ 1,2, \dots, n \right\}$. $\rvs_i \in \sR^d$, $i \in [n]$ refers to the feature vector of the $i$th state. $\rvw_r \in \sR^d$, $r \in [m]$ is a weight vector in the first hidden layer. $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right] \in \sR^{d \times m}$ is the weight matrix of the first hidden layer. $u_{i,r} \triangleq \rvw_r^\top \rvs_i$ is the $r$th node value of the first hidden layer. $\rva_k \in \sR^m$, $k \in [h]$ is a weight vector in the second hidden layer. $\rmA^\top \triangleq \left[ \rva_1, \rva_2, \dots, \rva_h \right] \in \sR^{m \times h}$ is the weight matrix of the second hidden layer. $o_{i,k} \triangleq \sum\limits_{r=1}^{m}{a_{k,r} \cdot \sigma\left( u_{i,r} \right)}$ is the logit of the $k$th action for state $\rvs_i$, where $\sigma(\cdot) \triangleq \max\left\{ \cdot, 0 \right\}$ is the ReLU activation function. $\pi_{i,k} \triangleq f\left( o_{i,k} \right) \triangleq \frac{\exp\left\{ o_{i,k} \right\}}{\sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime} \right\}}}$ is the probability of choosing action $k$ at state $\rvs_i$, where $f(\cdot)$ is the softmax function. $\rvr_i \in \sR^h$ is the true mean reward vector at state $\rvs_i$. $\rvpi_i^* \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \rvr_i \right\}}$ is the optimal policy at state $\rvs_i$. Given stochastic policies $\rvpi \triangleq \left[ \rvpi_1, \rvpi_2, \dots, \rvpi_n \right] \in \sR^{h \times n}$, the expected (mean) loss is defined as,
\begin{equation}
\label{eq:expected_loss}
\begin{split}
\frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \left( {\rvpi_i^*}^\top \rvr_i - \rvpi_i^\top \rvr_i \right) }.
\end{split}
\end{equation}

Without loss of generality, we assume $\rvr_i \in \left[ 0, 1 \right]^h$, and $\rvr_i$ is the mean vector of some random reward vectors,  $\forall i \in [n]$.