\section{Over-Parametrized Logit Learning}
\label{sec:logit_learning}

As discussed in \cref{sec:policy_gradient}, our policy gradient method achieves an unsatisfactory $\tilde{O}(T^{2/3})$ regret.
In this section, we propose an alternative algorithm, called {\bf L}ogit {\bf L}earning with $\varepsilon$-Greedy {\bf E}xploration (LLE), that is similar to value based reinforcement learning methods.
The learning of LLE happens in the dual value space by gradient updates over $\frac{1}{2} \left\| \rvo\left( \rmW_t\right) - \hat{\rvr}_t \right\|_2^2$.
Unlike the policy gradient method, LLE does not require to have a accurate estimate of $\rvr$ but rather to use its finite sampling budget to distinguish the best action from other sub-optimal actions.
%Recall that the optimal rate for stochastic multi-armed bandits is $O(\log(T))$
%such rate is not satisfactory especially for the stochastic bandit setting. 

A complete algorithm is shown in \cref{alg:logit_learning_eps_greedy_exploration}. 
After the random initialization, at each step $t$, the agent executes an $\varepsilon_t$-greedy policy, where with probability $1 - \varepsilon_t$ the agent selects the action with the largest logit, or otherwise it uniformly samples an action.
The algorithm tends to match the logits with the empirical mean rewards, by minimizing a square loss between the logits and the empirical mean rewards.
As discussed in \cref{sec:introduction}, the empirical mean reward $\hat{\rvr}_t$ here is not an accurate estimation of $\rvr$. In fact, it suffers a constant bias as we will see later.
However, with a small portion of exploration, this bias is small enough not to disturb the selection of the best action.
In order to maintain the proper amount of exploration during learning, 
LLE also simultaneously estimates a lower bound, $\hat{\Delta}_t(k)$, on the reward gap between the best action and the $k$th action. 
Such idea is also used in  \cite{seldin2017improved}.
%selects the action with the largest logit with probability $1 - \varepsilon_t$, and it uniformly randomly select actions with the other probability $\varepsilon_t$. 
%After taking action and observing rewards, the agent improves its strategies by minimizing the value loss objective $\frac{1}{2} \left\| \rvo\left( \rmW_t\right) - \hat{\rvr}_t \right\|_2^2$, where $\hat{\rvr}_t$ is the empirical mean reward estimated from sampled rewards up to step $t$. 
Finally, note that In each iteration, the agent only update its value neural network using one gradient descent, therefore the logit learning is in an online fashion.

\begin{algorithm}[t]
	\caption{Logit Learning with $\varepsilon$-Greedy Exploration (LLE)}
	\label{alg:logit_learning_eps_greedy_exploration}
	\begin{algorithmic}
		\STATE {\bfseries Input:} State feature $\rvs$, $\eta > 0$, $\alpha > 0$, $\beta > 0$.
		\STATE $\rvw_r(0) \sim \gN\left( 0, \sigma^2 \cdot \rmI \right)$, $\forall r \in [m]$. $a_{k, r} \sim \unif\left\{-1, +1\right\}$, $\forall k \in [h]$, $\forall r \in [m]$.
		\STATE $\hat{r}_{0}(k) \gets 0$, $n_{0}(k) \gets 0$, $\tilde{\pi}_t(k) \gets \frac{1}{h}$, $\ucb_{0}(k) \gets 1$, $\lcb_{0}(k) \gets 0$, $\forall k \in [h]$.
		\STATE Take every action $k$ once, update $n_{0}(k)$ and $\hat{r}_{0}(k)$.
		\FOR{$t=0$ {\bfseries to} $T-1$}
		
		\STATE Sample action $A_{t} \sim \tilde{\rvpi}_t\left(\cdot \middle| \rvs \right)$. Take action $A_{t}$. Observe reward $R_{ A_{t}}\left(n_{t}\left(A_t\right) \right)$.
		\STATE $\rmW_{t+1} \leftarrow \rmW_t - \eta \cdot \frac{d \left\{ \frac{1}{2} \left\| \rvo\left( \rmW_t\right) - \hat{\rvr}_t \right\|_2^2 \right\}}{d \rmW_t}$.
		\STATE $n_{t+1}(k) \gets \left. 
		    \begin{cases}
		    n_{t}(k) + 1, & \text{if } k = A_t, \\
		    n_{t}(k), & \text{otherwise}.
		    \end{cases}
		    \right. \qquad$
		%\STATE $n_{t+1}\left(A_t\right) \gets n_{t}\left(A_t\right) + 1$.
		%\STATE $n_{t+1}(k) \gets n_{t}(k)$, $\forall k \not= A_t$.
		$\hat{r}_{t+1}(k) \gets \left. 
		    \begin{cases}
		    \frac{n_{t}(k) \cdot \hat{r}_{t}(k) + R_{k}\left(n_{t}(k)\right) }{n_{t+1}(k)}, & \text{if } k = A_t, \\
		    \hat{r}_{t}(k), & \text{otherwise}.
		    \end{cases}
		    \right.$
		%\STATE $\hat{r}_{t+1}\left(A_t\right) \gets \frac{n_{t}\left(A_t\right) \cdot \hat{r}_{t}\left(A_t\right) + R_{A_{t}}\left(n_{t}\left(A_t\right)\right) }{n_{t+1}\left(A_t\right)}$.
		%\STATE $\hat{r}_{t+1}(k) \gets \hat{r}_{t}(k)$, $\forall k \not= A_t$.
		\STATE $\ucb_{t+1}(k) \gets
		    \min{\left\{ 1, \hat{r}_{t+1}(k) + \sqrt{\frac{\alpha \ln{\left( (t+1) h^{\frac{1}{\alpha}}\right)}}{2 n_{t+1}(k)}}\right\}}$, $\forall k \in [h]$.
		%\STATE $\ucb_{t+1}\left(A_t\right) \gets \min{\left\{ 1, \hat{r}_{t+1}\left(A_t\right) + \sqrt{\frac{\alpha \ln{\left( t h^{\frac{1}{\alpha}}\right)}}{2 n_{t+1}\left(A_t\right)}}\right\}}$.
		%\STATE $\ucb_{t+1}(k) \gets \ucb_{t}(k)$, $\forall k \not= A_t$.
		\STATE $\lcb_{t+1}(k) \gets 
		    \max{\left\{ 0, \hat{r}_{t+1}(k) - \sqrt{\frac{\alpha \ln{\left( t h^{\frac{1}{\alpha}}\right)}}{2 n_{t+1}(k)}}\right\}}$, $\forall k \in [h]$.
		%\STATE $\lcb_{t+1}\left(A_t\right) \gets \max{\left\{ 0, \hat{r}_{t+1}\left(A_t\right) - \sqrt{\frac{\alpha \ln{\left( t h^{\frac{1}{\alpha}}\right)}}{2 n_{t+1}\left(A_t\right)}}\right\}}$.
		%\STATE $\lcb_{t+1}(k) \gets \lcb_{t}(k)$, $\forall k \not= A_t$.
		
		\STATE $\hat{\Delta}_{t+1}(k) \gets \min\left\{ 1,  \min\limits_{k^\prime \in \left[h\right]}\left\{ \lcb_{t+1}(k^\prime) \right\}  - \ucb_{t+1}(k)\right\} $, $\forall k \in [h]$.
		\STATE $\xi_{t+1}(k) \gets \frac{\beta \ln{(t+1)}}{(t+1) \hat{\Delta}_{t+1}^2(k)}$, $\varepsilon_{t+1}(k) \gets \min\left\{ \frac{1}{2 h}, \frac{1}{2} \sqrt{\frac{\ln{h}}{(t+1) h}},  \xi_{t+1}(k) \right\}$, $\forall k \in [h]$.
		\STATE $\pi_{t+1}(k) \gets \left. 
		    \begin{cases}
		    1, & \text{if } k = \argmax\limits_{k^\prime \in \left[h\right]}\left\{ o_{t+1}(k^\prime)\right\}, \\
		    0, & \text{otherwise}.
		    \end{cases}
		    \right.$
		%\STATE $\pi_{t}(k) \gets 1$, for $k = \argmax\limits_{k^\prime \in \left[h\right]}\left\{ o_{t}(k^\prime)\right\}$.
		%\STATE $\pi_{t}(k) \gets 0$, $\forall k \not= \argmax\limits_{k^\prime \in \left[h\right]}\left\{ o_{t}(k^\prime)\right\}$.
		\STATE $\tilde{\pi}_{t+1}(k) \gets \left( 1 - \sum\limits_{k^\prime \in [h]}{\varepsilon_{t+1}(k)} \right) \cdot  \pi_{t+1}(k) + \varepsilon_{t+1}(k)$, $\forall k \in [h]$.
		
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

%\subsection{Theoretical analysis}
%\label{subsec:theoretical_analyses_logit_learning}

%It seems difficult to improve \cref{thm:policy_gradient_main_result}, as all the three parts of the regret are almost balanced, i.e., uniform exploration ($ T^{\frac{2}{3} + \beta}$, \cref{eq:total_regret_decomposition}), estimation error ($ T^{\frac{2}{3} + \beta} $, \cref{thm:reward_estimation_hoeffding}), and the cumulative expected loss of neural network policies ($ T^{\frac{2}{3} - \frac{\beta}{2}} $, \cref{thm:dynamic_regret_sublinear}). \cref{alg:policy_gradient_uniform_exploration} learns policies in the primal policy space by maximizing $\rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t$. And it is not necessary that $\rvo\left( \rmW_t\right)$ is close to $\hat{\rvr}_t$, due to the homogeneity invariant property of the softmax, i.e, $\rvpi\left(\rvo + a \cdot \rvone \right) = \rvpi(\rvo)$, $\forall a \in \sR$. To show the convergence of $\rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t$, every action needs to be pulled many times, which leads to the undesirable regret results.

The next theorem shows that LLE has a nearly optimal regret rate in the stochastic multi-armed bandit setting, when the value network is over-parametrized.
\begin{thm}
\label{thm:logit_learning_main_result}
    Given value neural networks as shown in \cref{fig:nn_policy_value}, with number of hidden nodes $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{h m}$,  $\alpha > 3$, and $\beta > 256$, the expected regret of \cref{alg:logit_learning_eps_greedy_exploration} satisfies,
\begin{equation*}
\small
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right)}  \le C + \sum\limits_{k \in [h] : \Delta(k) > 0}{ \frac{ 3 \beta \left( \ln{T} \right)^2}{\Delta^2(k)} }  + \frac{ 3 \ln{T}}{h},
\end{split}
\end{equation*}
%where $C \triangleq 2 \max\limits_{k \in [h]}\left\{ \bar{t}(k) \right\} + \pi^2$ is independent with $T$.
where $C$ is a constant independent to $T$.
\end{thm}

\begin{remk}
	The $O((\ln T)^2)$ regret of LLE is slightly worse than the optimal $O(\ln T)$ rate. 
	This is due to the low efficiency of $\varepsilon$-greedy exploration.
	In particular, to have a proper $\varepsilon$, one needs to estimate the reward gaps $\Delta(k)$ which causes the extra $\ln T$ factor.
	A UCB style of exploration that requires no knowledge about the reward gaps may be able to reduce this extra $\ln T$ factor.
\end{remk}
\begin{proof} [\bf Proof sketch]
		By Proposition 2 of \citep{seldin2017improved}, $\hat{\Delta}_t(k)$ is a valid estimate of the reward gap $\Delta(k)$, in the sense that for large enough $t$ with high probability, 
		\begin{equation*}
		    \Delta(k)/2 \le \hat{\Delta}_t(k) \le 	\Delta(k).
		\end{equation*}
		Now given a proper estimate of $\Delta(k)$, note that the exploration portion is set to be $\frac{\beta\ln t}{t \hat{\Delta}^2(k)}$.
		We can further show that with high probability each action has been tried at least $\frac{25 \ln t}{\Delta^2(k)}$ times with a large enough $\beta$, and therefore with high probability, 
		\begin{equation*}
		    \left| \hat{r}_t(k) - r(k) \right| \le \frac{\Delta(k)}{5}, \quad \forall k \in [h].
		\end{equation*}
	
		The main content of our proof is to show that the estimate error 
		\begin{equation*}
		    \left\| \rvo\left( \rmW_{t} \right) - \hat{\rvr}_{t}\right\|_2 \le \frac{\Delta(k)}{5}, \quad \forall k \in [h].
		\end{equation*}
		for large enough $t$. Therefore 
		\begin{equation*}
		    \left| o\left( \rmW_t \right)(k) - r(k) \right| \le \left\| \rvo\left( \rmW_t \right) - \hat{\rvr}_t\right\|_2 + \left| \hat{r}_t(k) - r(k) \right| \le \frac{2\Delta(k)}{5}, \quad \forall k \in [h],
		\end{equation*}
		i.e., the agent will select the optimal action by greedily acting on the output of the value network. To show that, note that 
		\begin{equation*}
		\begin{split}
		\frac{1}{2} \left\| \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_{t+1}\right\|_2^2 &= \frac{1}{2} \left\| \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_t\right\|_2^2 + \left( \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_t \right)^\top \left( \hat{\rvr}_t - \hat{\rvr}_{t+1} \right) + \frac{1}{2} \left\| \hat{\rvr}_t - \hat{\rvr}_{t+1} \right\|_2^2 \\
		&\le \frac{1}{2} \left\| \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_t\right\|_2^2 + \left\| \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_t\right\|_2 \cdot \frac{\Delta^2(k)}{\ln{t}} + \frac{\Delta^4(k)}{2 \left( \ln{t} \right)^2 },
		\end{split}
		\end{equation*}
		where the last inequality is by $\| \hat{\rvr}_t - \hat{\rvr}_{t+1} \|_2 \le \frac{\Delta^2(k)}{\ln{t}}$ because $\hat{\rvr}_t$  and $\hat{\rvr}_{t+1}$ are two consecutive empirical means and each action has been selected at least $\frac{\ln t}{\Delta^2(k)}$ times. Denote $\delta_t =  \left\| \rvo\left( \rmW_{t} \right) - \hat{\rvr}_{t}\right\|_2$. Combined with \cref{lem:logit_l2_loss_parameter_smoothness}, we have
		\begin{equation*}
		    \delta_{t+1} \le \sqrt{1 - \frac{1}{2 h} } \cdot \delta_{t} + \frac{\Delta^2(k)}{\ln{t}},
		\end{equation*}
		
		based on which one can further show that $\delta_t \le \frac{\Delta(k)}{5}$ for large enough $t$. Finally, the total probability of selecting a sub-optimal action can be bounded by the sum of the exploration probability and the probability of the rare event when not all the above claims hold, which leads to an upper bound on the expected regret.
		\end{proof}
	\begin{lem}
		\label{lem:logit_l2_loss_parameter_smoothness}
		$\rmW_{t+1} \leftarrow \rmW_t - \eta \cdot \frac{d \left\{ \frac{1}{2} \left\| \rvo\left( \rmW_t\right) - \hat{\rvr}_t \right\|_2^2 \right\}}{d \rmW_t}$, where $\eta = \frac{1}{h m}$. $\forall t \le \frac{\tau}{2 \eta} = \frac{1}{2 \eta \sqrt{m}}$,
		\begin{equation*}
		\begin{split}
		\frac{1}{2} \left\| \rvo\left( \rmW_{t+1} \right) - \hat{\rvr}_t \right\|_2^2 \le \left( 1 - \frac{1}{2 h} \right) \cdot \frac{1}{2} \left\| \rvo\left( \rmW_t \right) - \hat{\rvr}_t \right\|_2^2.
		\end{split}
		\end{equation*}
	\end{lem}