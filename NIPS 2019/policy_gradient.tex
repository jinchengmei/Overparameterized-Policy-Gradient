\section{Over-parametrized Policy Gradient}
\label{sec:policy_gradient}

Our first algorithm follows the widely used policy gradient method, but with an extra sub-linear exploration phase as shown in \cref{alg:policy_gradient_uniform_exploration}. In particular, we set the first $T^{\frac{2}{3} + \beta}$ rounds as the ``exploration phase'', during which the agent purely collects rewards without learning the policy neural networks. 
After the exploration phase, the agent samples and takes actions according to the current neural network policy $\rvpi\left(\rmW_t\right)$. In the meanwhile, the agent learns the policy neural network by doing policy gradient updates, with the expected empirically estimated loss calculated from the exploring phase as its objective.

\begin{algorithm}[t]
   \caption{Policy Gradient with Uniform Exploration}
\label{alg:policy_gradient_uniform_exploration}
\begin{algorithmic}
   \STATE {\bfseries Input:} State feature $\rvs$, learning rate $\eta > 0$, $\beta > 0$.
   \STATE $\rvw_r(0) \sim \gN\left( 0, \sigma^2 \cdot \rmI \right)$, $\forall r \in [m]$. $a_{k, r} \sim \gU\left\{-1, +1\right\}$, $\forall k \in [h]$, $\forall r \in [m]$.
   \STATE $\hat{r}_{0}\left(k\right) \gets 0$, $n_{0}\left(k\right) \gets 0$, $\forall k \in [h]$.
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \IF{$t < T^{\frac{2}{3} + \beta}$}
   \STATE (Exploring Phase) Uniformly randomly sample action $A_{t} \in [h]$.
   \STATE $\rmW_{t+1} \leftarrow \rmW_t$.
   \ELSE
   \STATE (Playing-Learning Phase) Sample action $A_{t} \sim \rvpi\left(\rmW_t\right)\left(\cdot \middle| \rvs \right)$.
   \STATE $\rmW_{t+1} \leftarrow \rmW_t + \eta \cdot \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t}$.
   \ENDIF
   \STATE Take action $A_{t}$. Observe reward $R_{ A_{t}}\left(n_{t}\left(A_t\right) \right)$.
   \STATE $n_{t+1}\left(k\right) \gets \left. 
		\begin{cases}
		n_{t}\left(k\right) + 1, & \text{if } k = A_t, \\
		n_{t}\left(k\right), & \text{otherwise}.
		\end{cases}
		\right. \qquad$ 
   %\STATE $n_{t+1}\left(A_t\right) \gets n_{t}\left(A_t\right) + 1$.
   %\STATE $n_{t+1}\left(k\right) \gets n_{t}\left(k\right)$, $\forall k \not= A_t$.
   $\hat{r}_{t+1}\left(k\right) \gets \left. 
		\begin{cases}
		\frac{n_{t}\left(k\right) \cdot \hat{r}_{t}\left(k\right) + R_{k}\left(n_{t}\left(k\right)\right) }{n_{t+1}\left(k\right)}, & \text{if } k = A_t, \\
		\hat{r}_{t}\left(k\right), & \text{otherwise}.
		\end{cases}
		\right.$
   %\STATE $\hat{r}_{t+1}\left(A_t\right) \gets \frac{n_{t}\left(A_t\right) \cdot \hat{r}_{t}\left(A_t\right) + R_{ A_{t}}\left(n_{t}\left(A_t\right)\right) }{n_{t+1}\left(A_t\right)}$.
   %\STATE $\hat{r}_{t+1}\left(k\right) \gets \hat{r}_{t}\left(k\right)$, $\forall k \not= A_t$.
   \ENDFOR
\end{algorithmic}
\end{algorithm}


Our first main results is a $\tilde{O}(T^{2/3})$ regret for the above policy gradient method, as shown in \cref{thm:policy_gradient_main_result}.

\begin{thm}
\label{thm:policy_gradient_main_result}
    Given policy neural networks as shown in \cref{fig:nn_policy_value}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, the expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right)} \le \frac{8 \sqrt{h}}{c} \cdot T^{\frac{2}{3}} + 3 \ln{h} \cdot T^{\frac{2}{3}} \left(\ln{T}\right)^{\frac{1}{3}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
According to \cref{alg:policy_gradient_uniform_exploration}, the uniform policy and $\rvpi\left( \rmW_t \right)$ are used to sample actions in the two phases. Therefore the regret is divided into two parts.
\begin{equation}
\label{eq:total_regret_decomposition}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right) } &= \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{ \left( {\rvpi^*}^\top \rvr - \expectation\limits_{A_t \sim \gU\left[h\right]}{ \left[ r\left(A_t\right) \right] } \right) } + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \rvpi\left( \rmW_t \right)^\top \rvr \right) } \\
    &\le \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{1} + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr } \\
    &= T^{\frac{2}{3} + \beta} + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr }.
\end{split}
\end{equation}
Denote $\rvpi_t^* \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}_t\right\}}$. The last two terms can be decomposed as follows, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation}
\label{eq:playing_learning_phase_regret_decomposition}
\begin{split}
    \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr &= \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t - \left( {\rvpi_t^*} - {\rvpi^*} \right)^\top \hat{\rvr}_t + \left( \rvpi\left( \rmW_t \right) - {\rvpi^*}\right)^\top \left( \hat{\rvr}_t - \rvr \right) \\
    &\le \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t  + \left\| \rvpi\left( \rmW_t \right) - \rvpi^* \right\|_1 \cdot \left\| \hat{\rvr}_t - \rvr \right\|_\infty  \\
    &\le \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t  + 4 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 2 T^{\beta - \frac{1}{3}},
\end{split}
\end{equation}
by the definition of $\rvpi_t^*$, H{\"o}lder's inequality, and \cref{thm:reward_estimation_hoeffding}. Summing up \cref{eq:playing_learning_phase_regret_decomposition} from $t = T^{\frac{2}{3} + \beta}$ to $T - 1$, and
combining \cref{eq:total_regret_decomposition} and \cref{thm:dynamic_regret_sublinear},
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right) } \le  4 T \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 3 T^{\frac{2}{3} + \beta} + \frac{2 \sqrt{h}}{ c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Taking $\beta = \frac{ \ln{\left(\frac{h}{6}\right) + \ln{\ln{T}} } }{ 3 \ln{T}} $ completes the proof.
\end{proof}

\cref{alg:policy_gradient_uniform_exploration} divides $T$ steps into two parts. In the first exploring phase, the agent uniformly samples actions without learning the neural network policy. Intuitively, at the beginning, when the loss estimation is very inaccurate, early updating can probably hurt the neural network policy.  While in the second playing-learning phase, since the loss estimation is good, the neural network policy will keep reducing its dynamic expected loss, which is highly related with its true expected loss, i.e., the expected regret of the playing-learning phase.

\subsection{Exploring Phase}
\label{subsec:exploring_phase}

The exploring phase of \cref{alg:policy_gradient_uniform_exploration} provides us good estimations of the true mean loss/reward as follows.
\begin{thm}
\label{thm:reward_estimation_hoeffding}
    In \cref{alg:policy_gradient_uniform_exploration}, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation*}
    \left\| \hat{\rvr}_t - \rvr \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + T^{\beta - \frac{1}{3}}.
\end{equation*}
\end{thm}
\begin{proof}
    At step $t \ge T^{\frac{2}{3} + \beta}$, the $k$th action is sampled $n_{t}\left(k\right) \ge \frac{T^{\frac{2}{3} + \beta} }{h}$ times because of the exploring phase, $\forall k \in [h]$. By Hoeffding's inequality, $\forall k \in [h]$,
\begin{equation}
\label{eq:loss_estimation_hoeffding}
\begin{split}
    \pr\left\{ \left| \hat{r}_{t}\left(k\right) - r\left(k\right) \right| > T^{\beta - \frac{1}{3}} \right\} &\le 2 \exp\left\{ - 2 n_{t}\left(k\right) \cdot T^{2\beta - \frac{2}{3}} \right\} \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\}.
\end{split}
\end{equation}
Therefore,
\begin{equation*}
\begin{split}
    \left\| \hat{\rvr}_t - \rvr \right\|_\infty &\le \pr\left\{ \left\| \hat{\rvr}_t - \rvr \right\|_\infty \le T^{\beta - \frac{1}{3}} \right\} \cdot T^{\beta - \frac{1}{3}} + \pr\left\{ \left\| \hat{\rvr}_t - \rvr \right\|_\infty > T^{\beta - \frac{1}{3}} \right\} \cdot 1 \\
    &\le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} \cdot 1 + 1 \cdot T^{\beta - \frac{1}{3}},
\end{split}
\end{equation*}
since $\left\| \hat{\rvr}_t - \rvr \right\|_\infty \le 1$, and according to \cref{eq:loss_estimation_hoeffding}, 
\end{proof}

\subsection{Playing-Learning Phase}
\label{subsec:playing_learning_phase}

The good estimation of the true mean loss $\hat{\rvr}_t$ obtained at the end of the exploring phase will be used to train the neural network policy $\rvpi\left( \rmW_t \right)$. However, the recently proposed optimization theory of over-parameterized neural networks is developed for supervized learning settings \citep{li2018learning,allen2018convergenceB}, where the learning objective functions are fixed. There are two main differences between our results and existing work, (a) in \cref{alg:policy_gradient_uniform_exploration}, the objectives are dynamic with respect to step $t$; (b) the optimal action need enough exploration during learning (explained later on in \cref{subsec:exploration_in_policy_learning}). We first prove the cumulative dynamic expected loss of $\rvpi\left( \rmW_t \right)$ is sublinear. Then we show some intuitions with lemmas, the proofs of which can be found in the appendix.
\begin{thm}
\label{thm:dynamic_regret_sublinear}
    Denote $\rvpi_t^* \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}_t\right\}}$. If $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, then the dynamic regret in the playing-learning phase satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{ \left(  {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t } \le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    Denote $\delta_t \triangleq \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t$.
\begin{equation}
\label{eq:dynamic_regret_decomposition}
\begin{split}
    \delta_t &= {\rvpi_t^*}^\top \left( \hat{\rvr}_t - \hat{\rvr}_{t-1}\right) + \left( {\rvpi_t^*} - {\rvpi_{t-1}^*} \right)^\top \hat{\rvr}_{t-1}  + \left( {\rvpi_{t-1}^*} - \rvpi\left( \rmW_{t-1} \right) \right)^\top \hat{\rvr}_{t-1} \\
    &\quad + \left(  \rvpi\left( \rmW_{t-1} \right) - \rvpi\left( \rmW_t \right) 
    \right)^\top \hat{\rvr}_{t-1} + \rvpi\left( \rmW_t \right)^\top \left( \hat{\rvr}_{t-1} - \hat{\rvr}_t \right).
\end{split}
\end{equation}
We upper bound each term in the right hand side. Firstly,
\begin{equation*}
\begin{split}
    {\rvpi_t^*}^\top \left( \hat{\rvr}_t - \hat{\rvr}_{t-1}\right) &= \frac{\pi_{t}^*\left(A_{t-1}\right)}{n_{t}\left(A_{t-1}\right)} \left[ R_{ A_{t-1}}\left( n_{t}\left(A_{t-1}\right) -1 \right) - \hat{r}_{t-1}\left(A_{t-1}\right) \right] \\
    &\le \frac{1}{n_{t}\left(A_{t-1}\right)} \le \frac{h}{T^{\frac{2}{3} + \beta}},
\end{split}
\end{equation*}
by $n_{t}\left(A_{t-1}\right) = n_{t-1}\left(A_{t-1}\right) + 1$, and $n_{t}\left(A_{t-1}\right) \ge \frac{T^{\frac{2}{3} + \beta}}{h}$, $\forall t \ge T^{\frac{2}{3} + \beta}$. By the definition of ${\rvpi_{t-1}^*}$,
\begin{equation*}
    \left( {\rvpi_t^*} - {\rvpi_{t-1}^*} \right)^\top \hat{\rvr}_{t-1} \le 0.
\end{equation*}
Note that according to the definition of $\delta_t$,
\begin{equation*}
    \left( {\rvpi_{t-1}^*} - \rvpi\left( \rmW_{t-1} \right) \right)^\top \hat{\rvr}_{t-1} = \delta_{t-1}.
\end{equation*}
By \cref{lem:empirically_expected_reward_parameter_smoothness}, $\eta = \frac{1}{2 h m}$, and \cref{lem:gradient_lower_bound},
\begin{equation*}
\begin{split}
    \left( \rvpi\left( \rmW_{t-1} \right) - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_{t-1} &\le - \frac{1}{4 h m} \left\| \frac{d \rvpi\left( \rmW_{t-1} \right)^\top \hat{\rvr}_{t-1}}{d \rmW_{t-1}} \right\|_F^2 \\
    &= - \frac{1}{4 h m} \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi\left( \rmW_{t-1} \right)^\top \hat{\rvr}_{t-1}}{d \rvw_r(t-1)} \right\|_2^2 } \\
    &\le - \frac{c^2}{4 h} \left[ \left( {\rvpi_{t-1}^*} - \rvpi\left( \rmW_{t-1} \right) \right)^\top \hat{\rvr}_{t-1}  \right]^2 \\
    &= - \frac{c^2}{4 h} \cdot \delta_{t-1}^2.
\end{split}
\end{equation*}
Using similar arguments,
\begin{equation*}
    \rvpi\left( \rmW_t \right)^\top \left( \hat{\rvr}_{t-1} - \hat{\rvr}_t  \right) \le \frac{h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Plugging the above upper bounds into \cref{eq:dynamic_regret_decomposition},
\begin{equation*}
    \delta_t \le \delta_{t-1} - \frac{c^2}{4 h} \cdot \delta_{t-1}^2 + \frac{2h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Rearranging and summing up from $T^{\frac{2}{3} + \beta} + 1$ to $T$,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2} \le \frac{4 h}{ c^2} \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T} { \left[ \delta_{t-1} - \delta_t + \frac{2h}{T^{\frac{2}{3} + \beta}} \right] } \le \frac{4 h}{ c^2} \cdot T^{\frac{1}{3} - \beta}.
\end{split}
\end{equation*}
By the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{\delta_{t}} \le \sqrt{\left(T  - T^{\frac{2}{3}+ \beta} \right) \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2}} \le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Let $\tau = \sigma$ in \cref{lem:gradient_coupling}. Let $\sigma = \frac{1}{\sqrt{m}}$ and $T \le \frac{\tau}{2 \eta}$ such that \cref{lem:gradient_coupling} can be used during all the playing-learning phase, we have $m \ge \frac{T^2}{h^2}$.
\end{proof}



\cref{thm:dynamic_regret_sublinear} relies on two arguments. First, the dynamic expected loss is smooth in the logit space, and small policy gradient updates preserve the signs of ReLU outputs, therefore highly correlates the logit derivative and the policy gradient. Second, by the over-parameterization theory, gradient norm is lower bounded by expected loss around initialization, which means there is no bad local minima near the randomly initialized neural network policy $\rvpi\left( \rmW_0 \right)$.

\begin{lem}
\label{lem:logit_smoothness}
Let $\rvpi\left( \rvo^\prime \right)$ and $\rvpi\left( \rvo \right)$ be the softmax policies of any logit vectors $\rvo^\prime, \rvo \in \sR^h$, respectively. $\forall \rvr \in \left[ 0, 1\right]^h$,
\begin{equation*}
    \rvpi\left( \rvo^\prime \right)^\top \rvr \le \rvpi\left( \rvo \right)^\top \rvr + \left\langle \frac{d \rvpi\left( \rvo \right)^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle + \left\| \rvo^\prime - \rvo \right\|_2^2.
\end{equation*}
\end{lem}
Let $\rvo_{t+1} = \rvo_{t} + \eta \cdot \frac{d \rvpi\left( \rvo_{t} \right)^\top \hat{\rvr}_t}{d \rvo_{t}}$, and let $\rvr = - \hat{\rvr}_t$, $\eta = \frac{1}{2}$ in \cref{lem:logit_smoothness},
\begin{equation*}
\begin{split}
\small
    \rvpi\left( \rvo_{t+1} \right)^\top \hat{\rvr}_t - \rvpi\left( \rvo_{t} \right)^\top \hat{\rvr}_t \ge\frac{1}{4} \left\| \frac{d \rvpi\left( \rvo_{t} \right)^\top \hat{\rvr}_t}{d \rvo_{t}} \right\|_2^2.
\end{split}
\end{equation*}
Note the logit derivative norm is lower bounded by loss,
\begin{equation}
\label{eq:logit_derivative_lower_bound}
\begin{split}
    \left\| \frac{d \rvpi\left( \rvo_{t} \right)^\top \hat{\rvr}_t}{d \rvo_{t}} \right\|_2 \ge \pi_t\left(\hat{k}_t^*\right) \cdot \left( \hat{r}_t\left(\hat{k}_t^*\right) - \rvpi\left( \rvo_{t} \right)^\top \hat{\rvr}_t \right),
\end{split}
\end{equation}
where $\hat{k}_t^* \triangleq \argmax\limits_{ k \in [h]}{\left\{ \hat{r}_t\left(k\right) \right\}}$. Therefore, by \cref{lem:logit_smoothness} and \cref{eq:logit_derivative_lower_bound}, we can derive similar relationship for $\delta_t$ and sublinear regret for update in the logit space. However, in practice, $\rvpi$ is updated in the parameter space. When the parameters are updated from $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right]$ to ${\rmW^\prime}^\top \triangleq \left[ \rvw_1^\prime, \rvw_2^\prime, \dots, \rvw_m^\prime \right]$,
\begin{equation*}
\begin{split}
    \rvo^\prime - \rvo = \rmA \left[ \sigma \left( \rmW^\prime \rvs \right) - \sigma \left( \rmW \rvs \right) \right].
\end{split}
\end{equation*}
The difficulty is that after updating the parameters, possibly many signs in the ReLU components also change. This can be circumvented by restraining the updates around the initialization \citep{li2018learning}.

\begin{lem}
\label{lem:gradient_coupling}
	Define the pseudo policy gradient at step $t$ as,
\begin{equation*}
\begin{split}
	\frac{d \tilde{\ell}_t}{d \rmW_t} \triangleq \tilde{\rmD} \rmA^\top \rmH\left( \rvpi\left(\rmW_t\right) \right) \hat{\rvr}_t \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD} \in \sR^{h \times h}$ is a diagonal matrix, and  $\tilde{\rmD}_{r,r} \triangleq \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\rmH\left( \rvpi\left(\rmW_t\right) \right)$ is,
\begin{equation*}
    \rmH\left( \rvpi\left(\rmW_t\right) \right) \triangleq \Delta\left( \rvpi\left(\rmW_t\right) \right) - \rvpi\left(\rmW_t\right) \rvpi\left(\rmW_t\right)^\top.
\end{equation*}
The true policy gradient is,
\begin{equation*}
\begin{split}
    \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \triangleq  \rmD(t) \rmA^\top \rmH\left( \rvpi\left(\rmW_t\right) \right) \hat{\rvr}_t \rvs^\top.
\end{split}
\end{equation*}
where $\rmD_{r,r}(t) \triangleq \sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\forall \tau > 0$, $\forall r \in [m]$, with probability at least $1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$, $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
	\frac{d\tilde{\ell}_t}{d \rvw_r(t)} = \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW_t$.
\end{lem}

\cref{lem:gradient_coupling} implies that for bounded numbers of policy gradient updates, the signs of the ReLUs will not change, i.e., $\sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\} = \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$. Combine \cref{lem:logit_smoothness} with \cref{lem:gradient_coupling}, we have the smoothness property of the surrogate expected loss in the parameter space.
\begin{lem}
\label{lem:empirically_expected_reward_parameter_smoothness}
    $\rmW_{t+1} = \rmW_t + \eta \cdot \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t}$. $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
    \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t - \rvpi\left( \rmW_{t+1} \right)^\top \hat{\rvr}_t \le - \left( \eta - h m \eta^2 \right) \cdot \left\| \frac{d \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}

Now by the key insight of the recent progresses of the over-parameterized neural network optimization theory, with constants probability, the pseudo gradient norm is lower bounded by the objective \citep{li2018learning}. However, unlike the supervised learning, RL has exploration issue, as shown in \cref{subsec:exploration_in_policy_learning}. Our result contains an exploration related term, which is consistent with \cref{eq:logit_derivative_lower_bound}, making guarantees for exploring the optimal action necessary during learning.

\begin{lem}
\label{lem:gradient_lower_bound}
	Denote $\hat{k}_t^* \triangleq \argmax\limits_{k \in [h]}\left\{ \hat{r}_{t}\left(k\right) \right\}$, i.e., the optimal action using the estimated reward $ \hat{\rvr}_t$. If $\pi_t\left(\hat{k}_t^*\right) > c > 0$, with probability $\frac{3}{64} \in \Omega\left( 1 \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}_t}{d \rvw_r(t)} \right\|_2 \ge c \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_{t}\left(k\right) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t \right) .
\end{split}
\end{equation*}
\end{lem}

\cref{lem:gradient_lower_bound} generalizes the over-parameterized neural network optimization theory into the RL settings. By \cref{lem:gradient_lower_bound}, whenever the policy empirically expected reward $\rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t$ small comparing with the largest possible empirical reward $\max\limits_{k \in \left[h\right]}\left\{ \hat{r}_{t}\left(k\right) \right\}$, with enough exploration of the suggorate optimal action ($\pi_t\left(\hat{k}_t^*\right) > c > 0$), with constant probability, the pseudo policy gradient norm will also be large. Therefore, combining \cref{lem:gradient_lower_bound} with \cref{lem:gradient_coupling}, the true policy gradient norm is also large, which is necessary for using \cref{lem:empirically_expected_reward_parameter_smoothness}. Applying all the stated lemmas, the policy surrogate expected loss converges as shown in \cref{thm:dynamic_regret_sublinear}.

\subsection{Exploration of the Optimal Action}
\label{subsec:exploration_in_policy_learning}

Consider the logit derivative of the true mean reward,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi\left( \rvo \right)^\top \rvr}{d \rvo} = \left[ \Delta\left( \rvpi \right) - \rvpi \rvpi^\top \right] \rvr,
\end{split}
\end{equation}
where $\Delta\left( \rvpi \right) \in \sR^{h \times h}$ is a diagonal matrix with $\Delta\left( \rvpi \right)_{k,k} = \pi_{k}$, $\forall k \in [h]$. Suppose the $k$th action is worth learning, i.e, $r\left(k\right) - \rvpi^\top \rvr > 0$ is large, meaning this action has mean reward $r\left(k\right)$ much larger than the expected mean reward of the current policy, so the agent should increase its action logit. But if $\pi_{k}$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi_{k}$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi_{k}$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{t}\left(k\right) > c > 0$ should hold for some constant $c$. In particular, to learn an optimal policy, $\pi_{t}\left(k^*\right) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $r\left(k^*\right) = \max\limits_{k \in \left[ h \right]}\left\{ r\left(k\right) \right\}$.

Consider policy update using \cref{eq:logit_derivative}. $\forall \eta > 0$, $\forall k \in [h]$,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o_{t+1}\left(k\right) - o_{t}\left(k\right) = \eta \cdot \pi_{t}\left(k\right) \cdot \left( r\left(k\right) - \rvpi\left( \rvo_{t} \right)^\top \rvr \right),
\end{split}
\end{equation*}
which mean as long as $\pi_{t}\left(k\right) > 0$, for any valuable action $k$ with $r\left(k\right) -  \rvpi\left( \rvo_{t} \right)^\top \rvr > 0$, the action logit will increase. While for any bad actions with $r\left(k\right) -  \rvpi\left( \rvo_{t} \right)^\top \rvr < 0$, their logits will decrease.

Consider the uniform policy $\rvpi\left( \rvo(0) \right)$, with $\pi_{0}\left(k^*\right) = \frac{1}{h} \in \Omega(1)$. Also note $r\left(k^*\right) - \rvpi\left( \rvo(0) \right)^\top \rvr$ is larger than any other action $k$, because $k^*$ is the the optimal action. Therefore, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
Policy update using \cref{eq:logit_derivative} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{t+1}\left(k^*\right) \ge \pi_{t}\left(k^*\right) \in \Omega(1).
\end{equation*}
\end{lem}

\cref{alg:policy_gradient_uniform_exploration} enjoys similar results with \cref{lem:optimal_probability_increse_logit_space}. First, with enough exploration, $\hat{\rvr}_t$ is close to $\rvr$, thus $\rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t$ is close to $\rvpi\left(\rmW_t\right)^\top \rvr$. Second, $\pi_0\left(\hat{k}_t^*\right) \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is around the initialization by \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, which makes the the optimal action logit always large comparing with the suboptimal actions.
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
\cref{alg:policy_gradient_uniform_exploration} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_t\left(\hat{k}_t^*\right) \in \Omega(1).
\end{equation*}
\end{lem}
\cref{lem:optimal_probability_increse_parameter_space} indicates replacing $c$ in \cref{thm:policy_gradient_main_result} will not incur any additional regret dependent on $T$.