\section{Over-parametrized Policy Gradient}
\label{sec:policy_gradient}
Our first algorithm, Policy Gradient with Uniform Exploration (PGE), follows the widely used policy gradient method, as shown in \cref{alg:policy_gradient_uniform_exploration}.
The algorithm updates the policy neural network by doing policy gradient ascent, but with the empirically estimated reward as its objective. 
Note that in contrast a traditional policy gradient method would rather use the current reward as an estimate of the expected reward for its learning objective.
To balance between exploration and exploitation, the agent also has a uniform exploration component in its policy. 
In particular, we set every action with probability at least $t^{ \beta - \frac{1}{3}} \cdot \frac{1}{h}$ to be explored, with $\beta > 0$ to be very small. 
\begin{algorithm}[t]
   \caption{Policy Gradient with $\varepsilon$-Greedy Exploration (PGE)}
\label{alg:policy_gradient_uniform_exploration}
\begin{algorithmic}
   \STATE {\bfseries Input:} State feature $\rvs$, learning rate $\eta > 0$, $\beta > 0$.
   \STATE $\rvw_r(0) \sim \gN\left( 0, \sigma^2 \cdot \rmI \right)$, $\forall r \in [m]$. $a_{k, r} \sim 
   \unif\left\{-1, +1\right\}$, $\forall k \in [h]$, $\forall r \in [m]$.
   \STATE $\hat{r}_{0}(k) \gets 0$, $n_{0}(k) \gets 0$, $\tilde{\pi}_0(k) \gets \frac{1}{h}$, $\forall k \in [h]$.
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \STATE Sample action $A_{t} \sim \tilde{\rvpi}_{t}\left(\cdot \middle| \rvs \right)$. Take action $A_{t}$. Observe reward $R_{ A_{t}}\left(n_{t}\left(A_t\right) \right)$.
   \STATE $\rmW_{t+1} \leftarrow \rmW_t + \eta \cdot \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t}$.
   \STATE $n_{t+1}(k) \gets \left. 
		\begin{cases}
		n_{t}(k) + 1, & \text{if } k = A_t, \\
		n_{t}(k), & \text{otherwise}.
		\end{cases}
		\right. \qquad$ 
   $\hat{r}_{t+1}(k) \gets \left. 
		\begin{cases}
		\frac{n_{t}(k) \cdot \hat{r}_{t}(k) + R_{k}\left(n_{t}(k)\right) }{n_{t+1}(k)}, & \text{if } k = A_t, \\
		\hat{r}_{t}(k), & \text{otherwise}.
		\end{cases}
		\right.$
   \STATE $\tilde{\pi}_{t+1}(k) \gets \left( 1 - \left(t+1\right)^{ \beta - \frac{1}{3}} \right) \cdot \pi\left(\rmW_{t+1}\right)(k) + \left(t+1\right)^{ \beta - \frac{1}{3}} \cdot \frac{1}{h}$, $\forall k \in [h]$.
   \ENDFOR
\end{algorithmic}
\end{algorithm}

Our first main result is a $\tilde{O}(T^{2/3})$ regret for the above policy gradient method, as shown in \cref{thm:policy_gradient_main_result}.
\begin{thm}
	\label{thm:policy_gradient_main_result}
	Given policy neural networks as shown in \cref{fig:nn_policy_value}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, $\beta = \frac{ \ln{\left(\frac{h}{3}\right) + \ln{\ln{t}} } }{ 3 \ln{t}}$, the expected regret of PGE satisfies,
	\begin{equation*}
	\begin{split}
	\sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right)} \le O\left(T^{\frac{2}{3}}\left( \ln{T} \right)^{\frac{1}{3}}\right).
	\end{split}
	\end{equation*}
\end{thm}

\iffalse
\begin{thm}
\label{thm:policy_gradient_main_result}
    Given policy neural networks as shown in \cref{fig:nn_policy_value}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, $\beta = \frac{ \ln{\left(\frac{h}{3}\right) + \ln{\ln{t}} } }{ 3 \ln{t}}$, the expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right)} \le 4 \cdot T^{\frac{2}{3}} \cdot \left( h \ln{T} \right)^{\frac{1}{3}} + \frac{5 h}{c} \cdot  T^{\frac{2}{3}} + \frac{h}{3} \cdot \ln{ \left( \frac{h}{3} \right) },
\end{split}
\end{equation*}
with probability at least,
\begin{equation*}
    1 - h \cdot \exp\left\{ - \frac{t^{\frac{1}{3}}}{2 h^2} \cdot \left( \frac{h}{3} \ln{t} \right)^{\frac{2}{3}} \right\} - 2 h \cdot t^{- \frac{1}{3}} - \exp\left\{ - \frac{T^2}{16 h} \right\},
\end{equation*}
\end{thm}
\fi
\begin{proof}[\bf Proof sketch]
PGE divides the sampling policy $\tilde{\rvpi}_t$ into two parts, i.e., the network policy $\rvpi\left( \rmW_t \right)$ and an uniform policy. Intuitively, when the reward estimation is very inaccurate, updates can probably hurt the neural network policy.  
%Therefore, an uniform policy is used to guarantee that every action can be sampled for enough many times, which will provide accurate reward estimations for learning. 
The first step of our proof is the claim that for large enough $t$, with large probability PGE has an accurate estimate of $\rvr$, as
%Note that with enough exploration and given large enough $t$, with high probability we have 
\[
\left\| \hat{\rvr}_t - \rvr \right\|_\infty \le t^{\beta - \frac{1}{3}} \le \left( h \ln{t} \right) ^\frac{1}{3} \cdot t^{- \frac{1}{3}}.
\]
	
Further note that the expected regret can be decomposed into two component, as follows.
 \begin{equation}
 \label{eq:total_regret_decomposition}
 \begin{split}
 \sum\limits_{t=0}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right) } & \le 1 + \sum\limits_{t=1}^{T-1}{ \left( {\rvpi^*}^\top \rvr - \sE \left[ r\left(A_t\right) \right] \right) }\\
 &\le 2 \cdot T^{\frac{2}{3}} \cdot \left( h \ln{T} \right)^{\frac{1}{3}} + \sum\limits_{t=1}^{T-1}{ \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr }.
 \end{split}
 \end{equation}
It remains to bound $ \sum\limits_{t=1}^{T-1}{ \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr }$. 
Denote $\rvpi_t^* \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}_t\right\}}$. This term can then be decomposed as follows.
\begin{equation}
\label{eq:playing_learning_phase_regret_decomposition}
\begin{split}
\left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \rvr &= \left( {\rvpi^*} - {\rvpi_t^*} \right)^\top \hat{\rvr}_t + \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t + \left( {\rvpi^*} - \rvpi\left( \rmW_t \right) \right)^\top \left( \rvr - \hat{\rvr}_t \right) \\
&\le \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t  + \left\| \rvpi^* - \rvpi\left( \rmW_t \right) \right\|_1 \cdot \left\| \rvr - \hat{\rvr}_t \right\|_\infty \\
&\le \left( {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t + \left( h \ln{t} \right)^\frac{1}{3}  \cdot t^{- \frac{1}{3}},
\end{split}
\end{equation}
where the first inequality is by the definition of $\rvpi_t^*$ and  H{\"o}lder's inequality. 

The key step of proof is to prove that if $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, then the dynamic regret satisfies,
\begin{equation}
\label{step:proof1step1}
\begin{split}
\sum\limits_{t=1}^{T-1}{ \left(  {\rvpi_t^*} - \rvpi\left( \rmW_t \right) \right)^\top \hat{\rvr}_t } \le \frac{5 h}{c} \cdot  T^{\frac{2}{3}},
\end{split}
\end{equation}
with probability at least $1 - \exp\left\{ - \frac{T^2}{16 h} \right\} \ge 1 - \frac{16 h}{T^2}$.
The final expected regret bound follows by combining all the terms above. 	
\end{proof}

\begin{remk}
	The proof of \cref{step:proof1step1} is postponed in appendix, which contributes to the main component of the proof for \cref{thm:policy_gradient_main_result}.
	To prove \cref{step:proof1step1}, we borrow some technical ideas from \cite{li2018learning,allen2018convergenceB},  where optimization theory of over-parameterized neural networks is developed for the supervized learning setting. To adapt these techniques for the reinforcement learning setting, many challenges still remain. One particular challenge is that the learning objective depends on $\hat{\rvr}$ that is dynamic. Another challenge is that all the actions need sufficient exploration, so that the learnig signal from $\hat{\rvr}$ is aligned with the true expected reward. Again, to overcome these two challenge, \cref{alg:policy_gradient_uniform_exploration} needs to assign a large portion of budget for exploration.
\end{remk}

\begin{remk}
The regret rate of $\tilde{O}(T^{2/3})$ is unsatisfactory. 
One may recall that an optimal rate of $O(\log(T))$ can be achieved in stochastic multi-armed bandit \citep{bubeck2012regret}. 
One key step in our analysis that 
What prevents us to achieve a better rate is that the current proof requires a small estimation error $\left\| \rvr - \hat{\rvr}_t \right\|_\infty$.
However, an accurate estimate of $r$ may not be necessary for the agent to achieve a small regret. 
In fact, identifying the best action without an accurate estimation of $r$ is the essence of the UCB algorithm and many other stochastic bandit algorithms. 
As shown in \cref{sec:logit_learning}, our next algorithm manages to avoid such requirement, and thus achieves a nearly optimal regret rate of $O((\ln T)^2)$.
\end{remk}

\iffalse
\subsection{Exploration of the Optimal Action}
\label{subsec:exploration_in_policy_learning}

Consider the logit derivative of the true mean reward,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi( \rvo)^\top \rvr}{d \rvo} = \left[ \Delta(\rvpi) - \rvpi \rvpi^\top \right] \rvr,
\end{split}
\end{equation}
where $\Delta(\rvpi) \in \sR^{h \times h}$ is a diagonal matrix with $\rvpi$ as its diagonal. Suppose the $k$th action is worth learning, i.e, $r(k) - \rvpi^\top \rvr > 0$ is large, meaning this action has mean reward $r(k)$ much larger than the expected mean reward of the current policy, so the agent should increase its action logit. But if $\pi(k)$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi(k)$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi(k)$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{t}(k) > c > 0$ should hold for some constant $c$. In particular, to learn an optimal policy, $\pi_{t}(k^*) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $r(k^*) = \max\limits_{k \in \left[ h \right]}\left\{ r(k) \right\}$.

Consider policy update using \cref{eq:logit_derivative}. $\forall \eta > 0$, $\forall k \in [h]$,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o_{t+1}(k) - o_{t}(k) = \eta \cdot \pi_{t}(k) \cdot \left( r(k) - \rvpi(\rvo_{t})^\top \rvr \right),
\end{split}
\end{equation*}
which mean as long as $\pi_{t}(k) > 0$, for any valuable action $k$ with $r(k) -  \rvpi(\rvo_{t})^\top \rvr > 0$, the action logit will increase. While for any bad actions with $r(k) -  \rvpi(\rvo_{t})^\top \rvr < 0$, their logits will decrease.

Consider the uniform policy $\rvpi( \rvo_0)$, with $\pi_{0}(k^*) = \frac{1}{h} \in \Omega(1)$. Also note $r(k^*) - \rvpi( \rvo_0)^\top \rvr$ is larger than any other action $k$, because $k^*$ is the the optimal action. Therefore, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
Policy update using \cref{eq:logit_derivative} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{t+1}(k^*) \ge \pi_{t}(k^*) \in \Omega(1).
\end{equation*}
\end{lem}

\cref{alg:policy_gradient_uniform_exploration} enjoys similar results with \cref{lem:optimal_probability_increse_logit_space}. First, with enough exploration, $\hat{\rvr}_t$ is close to $\rvr$, thus $\rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t$ is close to $\rvpi\left(\rmW_t\right)^\top \rvr$. Second, $\pi_{0}(\hat{k}_t^*) \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is around the initialization by \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, which makes the the optimal action logit always large comparing with the suboptimal actions.
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
\cref{alg:policy_gradient_uniform_exploration} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{t}(\hat{k}_t^*) \in \Omega(1).
\end{equation*}
\end{lem}
\cref{lem:optimal_probability_increse_parameter_space} indicates replacing $c$ in \cref{thm:policy_gradient_main_result} will not incur any additional regret dependent on $T$.
\fi