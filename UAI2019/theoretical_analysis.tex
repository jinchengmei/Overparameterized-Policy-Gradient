\section{THEORETICAL ANALYSES}
\label{sec:theoretical_analyses}

In this section, we first prove the $T^{\frac{2}{3}}\ln{T}$ finite time regret of \cref{alg:policy_gradient_uniform_exploration}, and then we prove the optimal finite time regret of \cref{alg:logit_learning_eps_greedy_exploration}.

\subsection{POLICY GRADIENT}
\label{subsec:theoretical_analyses_policy_gradient}

We first present the main results as shown in \cref{thm:policy_gradient_main_result}, and then discuss the detailed proof ideas and intuitions.

\subsubsection{Main Results}
\label{subsubsec:main_results_policy_gradient}

\begin{thm}
\label{thm:policy_gradient_main_result}
    Given policy neural networks as shown in \cref{fig:nn_policy}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, the expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
\begin{equation*}
\small
\begin{split}
    \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{A_t} \right] } \le \frac{8 \sqrt{h}}{c} \cdot T^{\frac{2}{3}} + 3 \ln{h} \cdot T^{\frac{2}{3}} \left(\ln{T}\right)^{\frac{1}{3}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
According to \cref{alg:policy_gradient_uniform_exploration}, the uniform policy and $\rvpi\left( \rmW(t) \right)$ are used to sample actions in the two phases. Therefore the regret is divided into two parts.
\begin{equation}
\small
\label{eq:total_regret_decomposition}
\begin{split}
    R_T &\triangleq \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{ A_t} \right] } \\
    &= \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{ {\rvpi^*}^\top \rvr} - \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{ \expectation\limits_{A_t \sim \gU\left[h\right]}{ \left[ r_{A_t} \right] }} \\
    &\quad + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ {\rvpi^*}^\top \rvr }  - \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi\left( \rmW(t) \right)^\top \rvr } \\
    &\le T^{\frac{2}{3} + \beta} + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi\left( \rmW(t) \right)^\top \rvr }.
\end{split}
\end{equation}
Denote $\rvpi^*\left(t\right) \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}\left(t\right)\right\}}$. The last two terms can be decomposed as follows, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation}
\small
\label{eq:playing_learning_phase_regret_decomposition}
\begin{split}
    \left( {\rvpi^*} - \rvpi\left( \rmW(t) \right) \right)^\top \rvr &= \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad - \left( {\rvpi^*\left(t\right)} - {\rvpi^*} \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + \left( \rvpi\left( \rmW(t) \right) - {\rvpi^*}\right)^\top \left( \hat{\rvr}\left(t\right) - \rvr \right) \\
    &\le \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + \left\| \rvpi\left( \rmW(t) \right) - \rvpi^* \right\|_1 \cdot \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty  \\
    &\le \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + 4 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 2 T^{\beta - \frac{1}{3}},
\end{split}
\end{equation}
by the definition of $\rvpi^*\left(t\right)$, H{\"o}lder's inequality, and \cref{thm:reward_estimation_hoeffding}. Summing up \cref{eq:playing_learning_phase_regret_decomposition} from $t = T^{\frac{2}{3} + \beta}$ to $T - 1$, and
combining \cref{eq:total_regret_decomposition} and \cref{thm:dynamic_regret_sublinear},
\begin{equation*}
\begin{split}
    R_T \le  4 T \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 3 T^{\frac{2}{3} + \beta} + \frac{2 \sqrt{h}}{ c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Taking $\beta = \frac{ \ln{\left(\frac{h}{6}\right) + \ln{\ln{T}} } }{ 3 \ln{T}} $ completes the proof.
\end{proof}

\cref{alg:policy_gradient_uniform_exploration} divides $T$ steps into two parts. In the first exploring phase, the agent uniformly samples actions without learning the neural network policy. Intuitively, at the beginning, when the loss estimation is very inaccurate, early updating can probably hurt the neural network policy.  While in the second playing-learning phase, since the loss estimation is good, the neural network policy will keep reducing its dynamic expected loss, which is highly related with its true expected loss, i.e., the expected regret of the playing-learning phase.

\subsubsection{Exploring Phase}
\label{subsubsec:exploring_phase}

The exploring phase of \cref{alg:policy_gradient_uniform_exploration} provides us good estimations of the true mean loss/reward as follows.
\begin{thm}
\label{thm:reward_estimation_hoeffding}
    In \cref{alg:policy_gradient_uniform_exploration}, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation*}
    \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + T^{\beta - \frac{1}{3}}.
\end{equation*}
\end{thm}
\begin{proof}
    At step $t \ge T^{\frac{2}{3} + \beta}$, the $k$th action is sampled $n_{k}\left(t\right) \ge \frac{T^{\frac{2}{3} + \beta} }{h}$ times because of the exploring phase, $\forall k \in [h]$. By Hoeffding's inequality, $\forall k \in [h]$,
\begin{equation}
\label{eq:loss_estimation_hoeffding}
\begin{split}
    &\pr\left\{ \left| \hat{r}_{ k}\left(t\right) - r_{k} \right| > T^{\beta - \frac{1}{3}} \right\} \le 2 \exp\left\{ - 2 n_{ k}\left(t\right) \cdot T^{2\beta - \frac{2}{3}} \right\} \\
    &\le 2 \exp\left\{ -  \frac{2 T^{\frac{2}{3} + \beta}}{h} \cdot T^{2\beta - \frac{2}{3}} \right\} = 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\}.
\end{split}
\end{equation}
Therefore,
\begin{equation*}
\begin{split}
    \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} \cdot 1 + 1 \cdot T^{\beta - \frac{1}{3}},
\end{split}
\end{equation*}
since $\pr\left\{ \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le T^{\beta - \frac{1}{3}} \right\} \le 1$, $\left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 1$, and according to \cref{eq:loss_estimation_hoeffding}, 
\end{proof}

\subsubsection{Playing-Learning Phase}
\label{subsubsec:playing_learning_phase}

The good estimation of the true mean loss $\hat{\rvr}\left(t\right)$ obtained at the end of the exploring phase will be used to train the neural network policy $\rvpi\left( \rmW(t) \right)$. However, the recently proposed optimization theory of over-parameterized neural networks is developed for supervized learning settings \citep{li2018learning,allen2018convergenceB}, where the learning objective functions are fixed. There are two main differences between our results and existing work, (a) in \cref{alg:policy_gradient_uniform_exploration}, the objectives are dynamic with respect to step $t$; (b) the optimal action need enough exploration during learning (explained later on in \cref{subsubsec:exploration_in_policy_learning}). We first prove the cumulative dynamic expected loss of $\rvpi\left( \rmW(t) \right)$ is sublinear. Then we show some intuitions with lemmas, the proofs of which can be found in the appendix.
\begin{thm}
\label{thm:dynamic_regret_sublinear}
    Denote $\rvpi^*\left(t\right) \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}\left(t\right)\right\}}$. If $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, then the dynamic regret in the playing-learning phase satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{ \left(  {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) } \le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    Denote $\delta_t \triangleq \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right)$.
\begin{equation}
\label{eq:dynamic_regret_decomposition}
\begin{split}
    \delta_t &= {\rvpi^*\left(t\right)}^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t-1\right)\right) \\
    &\quad + \left( {\rvpi^*\left(t\right)} - {\rvpi^*\left(t-1\right)} \right)^\top \hat{\rvr} \left(t-1\right) \\
    &\quad + \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad + \left(  \rvpi\left( \rmW(t-1) \right) - \rvpi\left( \rmW(t) \right) 
    \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad + \rvpi\left( \rmW(t) \right)^\top \left( \hat{\rvr}\left(t-1\right) - \hat{\rvr}\left(t\right) \right).
\end{split}
\end{equation}
We upper bound each term in the right hand side. Firstly,
\begin{equation*}
\begin{split}
    &{\rvpi^*\left(t\right)}^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t-1\right)\right) \\
    &\quad = \frac{{\pi_{ A_{t-1}}^*\left(t\right)}}{n_{ A_{t-1}}\left(t\right)} \left[ R_{ A_{t-1}}\left( n_{ A_{t-1}}\left(t\right) -1 \right) - \hat{r}_{ A_{t-1}}\left(t-1\right) \right] \\
    &\quad \le \frac{1}{n_{A_{t-1}}\left(t\right)} \le \frac{h}{T^{\frac{2}{3} + \beta}},
\end{split}
\end{equation*}
by $n_{A_{t-1}}\left(t\right) = n_{ A_{t-1}}\left(t-1\right) + 1$, and $n_{ A_{t-1}}\left(t\right) \ge \frac{T^{\frac{2}{3} + \beta}}{h}$, $\forall t \ge T^{\frac{2}{3} + \beta}$. By the definition of ${\rvpi^*\left(t-1\right)}$,
\begin{equation*}
    \left( {\rvpi^*\left(t\right)} - {\rvpi^*\left(t-1\right)} \right)^\top \hat{\rvr}\left(t-1\right) \le 0.
\end{equation*}
Note that according to the definition of $\delta_t$,
\begin{equation*}
    \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right) = \delta_{t-1}.
\end{equation*}
By \cref{lem:empirically_expected_reward_parameter_smoothness}, $\eta = \frac{1}{2 h m}$, and \cref{lem:gradient_lower_bound},
\begin{equation*}
\begin{split}
    &\left( \rvpi\left( \rmW(t-1) \right) - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad \le - \frac{1}{4 h m} \left\| \frac{d \rvpi\left( \rmW(t-1) \right)^\top \hat{\rvr}\left(t-1\right)}{d \rmW(t-1)} \right\|_F^2 \\
    &\quad = - \frac{1}{4 h m} \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi\left( \rmW(t-1) \right)^\top \hat{\rvr}\left(t-1\right)}{d \rvw_r(t-1)} \right\|_2^2 } \\
    &\quad \le - \frac{c^2}{4 h} \left[ \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right)  \right]^2 \\
    &\quad = - \frac{c^2}{4 h} \cdot \delta_{t-1}^2.
\end{split}
\end{equation*}
Using similar arguments,
\begin{equation*}
    \rvpi\left( \rmW(t) \right)^\top \left( \hat{\rvr}\left(t-1\right) - \hat{\rvr}\left(t\right)  \right) \le \frac{h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Plugging the above upper bounds into \cref{eq:dynamic_regret_decomposition},
\begin{equation*}
    \delta_t \le \delta_{t-1} - \frac{c^2}{4 h} \cdot \delta_{t-1}^2 + \frac{2h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Rearranging and summing up from $T^{\frac{2}{3} + \beta} + 1$ to $T$,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2} &\le \frac{4 h}{ c^2} \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T} { \left[ \delta_{t-1} - \delta_t + \frac{2h}{T^{\frac{2}{3} + \beta}} \right] } \\
    &\le \frac{4 h}{ c^2} \cdot T^{\frac{1}{3} - \beta}.
\end{split}
\end{equation*}
By the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{\delta_{t}} &\le \sqrt{\left(T  - T^{\frac{2}{3}+ \beta} \right) \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2}} \\
    &\le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Let $\tau = \sigma$ in \cref{lem:gradient_coupling}. Let $\sigma = \frac{1}{\sqrt{m}}$ and $T \le \frac{\tau}{2 \eta}$ such that \cref{lem:gradient_coupling} can be used during all the playing-learning phase, we have $m \ge \frac{T^2}{h^2}$.
\end{proof}



\cref{thm:dynamic_regret_sublinear} relies on two arguments. First, the dynamic expected loss is smooth in the logit space, and small policy gradient updates preserve the signs of ReLU outputs, therefore highly correlates the logit derivative and the policy gradient. Second, by the over-parameterization theory, gradient norm is lower bounded by expected loss around initialization, which means there is no bad local minima near the randomly initialized neural network policy $\rvpi\left( \rmW(0) \right)$.

\begin{lem}
\label{lem:logit_smoothness}
Let $\rvpi\left( \rvo^\prime \right)$ and $\rvpi\left( \rvo \right)$ be the softmax policies of any logit vectors $\rvo^\prime, \rvo \in \sR^h$, respectively. $\forall \rvr \in \left[ 0, 1\right]^h$,
\begin{equation*}
    \rvpi\left( \rvo^\prime \right)^\top \rvr \le \rvpi\left( \rvo \right)^\top \rvr + \left\langle \frac{d \rvpi\left( \rvo \right)^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle + \left\| \rvo^\prime - \rvo \right\|_2^2.
\end{equation*}
\end{lem}
Let $\rvo(t+1) = \rvo(t) + \eta \cdot \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)}$, and let $\rvr = - \hat{\rvr}\left(t\right)$, $\eta = \frac{1}{2}$ in \cref{lem:logit_smoothness},
\begin{equation*}
\begin{split}
\small
    \rvpi\left( \rvo(t+1) \right)^\top \hat{\rvr}\left(t\right) - \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right) \ge\frac{1}{4} \left\| \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)} \right\|_2^2.
\end{split}
\end{equation*}
Note the logit derivative norm is lower bounded by loss,
\begin{equation}
\label{eq:logit_derivative_lower_bound}
\begin{split}
    \left\| \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)} \right\|_2 \ge \pi_{\hat{k}^*\left(t\right)} \cdot \left( \hat{r}_{\hat{k}^*\left(t\right)} - \rvpi\left( \rvo\left(t\right) \right)^\top \hat{\rvr}\left(t\right) \right),
\end{split}
\end{equation}
where $\hat{k}^*\left(t\right) \triangleq \argmax\limits_{ k \in [h]}{\left\{ \hat{r}\left(t\right) \right\}}$. Therefore, by \cref{lem:logit_smoothness} and \cref{eq:logit_derivative_lower_bound}, we can derive similar relationship for $\delta_t$ and sublinear regret for update in the logit space. However, in practice, $\rvpi$ is updated in the parameter space. When the parameters are updated from $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right]$ to ${\rmW^\prime}^\top \triangleq \left[ \rvw_1^\prime, \rvw_2^\prime, \dots, \rvw_m^\prime \right]$,
\begin{equation*}
\begin{split}
    \rvo^\prime - \rvo = \rmA \left[ \sigma \left( \rmW^\prime \rvs \right) - \sigma \left( \rmW \rvs \right) \right].
\end{split}
\end{equation*}
The difficulty is that after updating the parameters, possibly many signs in the ReLU components also change. This can be circumvented by restraining the updates around the initialization \citep{li2018learning}.

\begin{lem}
\label{lem:gradient_coupling}
	Define the pseudo policy gradient at step $t$ as,
\begin{equation*}
\begin{split}
	\frac{d \tilde{\ell}(t)}{d \rmW(t)} \triangleq \tilde{\rmD} \rmA^\top \rmH\left( \rvpi\left(\rmW(t)\right) \right) \hat{\rvr}\left(t\right) \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD} \in \sR^{h \times h}$ is a diagonal matrix, and  $\tilde{\rmD}_{r,r} \triangleq \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\rmH\left( \rvpi\left(\rmW(t)\right) \right)$ is,
\begin{equation*}
    \rmH\left( \rvpi\left(\rmW(t)\right) \right) \triangleq \Delta\left( \rvpi\left(\rmW(t)\right) \right) - \rvpi\left(\rmW(t)\right) \rvpi\left(\rmW(t)\right)^\top.
\end{equation*}
The true policy gradient is,
\begin{equation*}
\begin{split}
    \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)} \triangleq  \rmD(t) \rmA^\top \rmH\left( \rvpi\left(\rmW(t)\right) \right) \hat{\rvr}\left(t\right) \rvs^\top.
\end{split}
\end{equation*}
where $\rmD_{r,r}(t) \triangleq \sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\forall \tau > 0$, $\forall r \in [m]$, with probability at least $1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$, $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
	\frac{d\tilde{\ell}(t)}{d \rvw_r(t)} = \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW(t)$.
\end{lem}

\cref{lem:gradient_coupling} implies that for bounded numbers of policy gradient updates, the signs of the ReLUs will not change, i.e., $\sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\} = \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$. Combine \cref{lem:logit_smoothness} with \cref{lem:gradient_coupling}, we have the smoothness property of the surrogate expected loss in the parameter space.
\begin{lem}
\label{lem:empirically_expected_reward_parameter_smoothness}
    $\rmW(t+1) = \rmW(t) + \eta \cdot \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)}$. $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
    &\rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right) - \rvpi\left( \rmW(t+1) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad \le - \left( \eta - h m \eta^2 \right) \cdot \left\| \frac{d \rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}

Now by the key insight of the recent progresses of the over-parameterized neural network optimization theory, with constants probability, the pseudo gradient norm is lower bounded by the objective \citep{li2018learning}. However, unlike the supervised learning, RL has exploration issue, as shown in \cref{subsubsec:exploration_in_policy_learning}. Our result contains an exploration related term, which is consistent with \cref{eq:logit_derivative_lower_bound}, making guarantees for exploring the optimal action necessary during learning.

\begin{lem}
\label{lem:gradient_lower_bound}
	Denote $\hat{k}^*(t) \triangleq \argmax\limits_{k \in [h]}\left\{ \hat{r}_{k}\left(t\right) \right\}$, i.e., the optimal action using the estimated reward $ \hat{\rvr}\left(t\right)$. If $\pi_{\hat{k}^*(t)}\left(\rmW(t)\right) > c > 0$, with probability $\frac{3}{64} \in \Omega\left( 1 \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}\left(t\right)}{d \rvw_r(t)} \right\|_2 \ge c \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_k\left(t\right) \right\} - \rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right) \right) .
\end{split}
\end{equation*}
\end{lem}

\cref{lem:gradient_lower_bound} generalizes the over-parameterized neural network optimization theory into the RL settings. By \cref{lem:gradient_lower_bound}, whenever the policy empirically expected reward $\rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)$ small comparing with the largest possible empirical reward $\max\limits_{k \in \left[h\right]}\left\{ \hat{r}_k\left(t\right) \right\}$, with enough exploration of the suggorate optimal action ($\pi_{\hat{k}^*(t)}\left(\rmW(t)\right) > c > 0$), with constant probability, the pseudo policy gradient norm will also be large. Therefore, combining \cref{lem:gradient_lower_bound} with \cref{lem:gradient_coupling}, the true policy gradient norm is also large, which is necessary for using \cref{lem:empirically_expected_reward_parameter_smoothness}. Applying all the stated lemmas, the policy surrogate expected loss converges as shown in \cref{thm:dynamic_regret_sublinear}.

\subsubsection{Exploration of the Optimal Action}
\label{subsubsec:exploration_in_policy_learning}

In \cref{subsec:policy_gradient}, it is claimed that the vanilla policy gradient will suffer the lack of exploration issue. Consider the logit derivative of the true mean reward,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi\left( \rvo \right)^\top \rvr}{d \rvo} = \left[ \Delta\left( \rvpi \right) - \rvpi \rvpi^\top \right] \rvr,
\end{split}
\end{equation}
where $\Delta\left( \rvpi \right) \in \sR^{h \times h}$ is a diagonal matrix with $\Delta\left( \rvpi \right)_{k,k} = \pi_{k}$, $\forall k \in [h]$. Suppose the $k$th action is worth learning, i.e, $r_k - \rvpi^\top \rvr > 0$ is large, meaning this action has mean reward $r_k$ much larger than the expected mean reward of the current policy, so the agent should increase its action logit. But if $\pi_{k}$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi_{k}$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi_{k}$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{k}\left(t\right) > c > 0$ should hold for some constant $c$. In particular, to learn an optimal policy, $\pi_{k^*}(t) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $r_{k^*} = \max\limits_{k \in \left[ h \right]}\left\{ r_k \right\}$.

Consider policy update using \cref{eq:logit_derivative}. $\forall \eta > 0$, $\forall k \in [h]$,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o_{k}\left( t+1 \right) - o_{k}\left( t \right) = \eta \cdot \pi_{k}\left( \rvo(t)\right) \cdot \left( r_{k} - \rvpi\left( \rvo(t) \right)^\top \rvr \right),
\end{split}
\end{equation*}
which mean as long as $\pi_{k}\left( \rvo(t)\right) > 0$, for any valuable action $k$ with $r_k -  \rvpi\left( \rvo(t) \right)^\top \rvr > 0$, the action logit will increase. While for any bad actions with $r_k -  \rvpi\left( \rvo(t) \right)^\top \rvr < 0$, their logits will decrease.

Now consider the randomly initialized policy $\rvpi\left( \rvo(0) \right)$, which is (very close to) the uniform policy, with $\pi_{k^*}\left( \rvo(0)\right) \approx \frac{1}{h} \in \Omega(1)$. Also note $r_{k^*} - \rvpi\left( \rvo(0) \right)^\top \rvr$ is larger than any other action $k$, because $k^*$ is the the optimal action. Therefore, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
Policy update using \cref{eq:logit_derivative} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{k^*}\left( \rvo(t+1)\right) \ge \pi_{k^*}\left( \rvo(t)\right) \in \Omega(1).
\end{equation*}
\end{lem}

\cref{alg:policy_gradient_uniform_exploration} enjoys similar results with \cref{lem:optimal_probability_increse_logit_space}. First, with enough exploration, $\hat{\rvr}\left(t\right)$ is close to $\rvr$, thus $\rvpi\left(t\right)^\top \hat{\rvr}\left(t\right)$ is close to $\rvpi\left(t\right)^\top \rvr$. Second, $\pi_{\hat{k}^*\left(t\right)}\left( \rmW(0)\right) \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is around the initialization by \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, which makes the the optimal action logit always large comparing with the suboptimal actions.
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
\cref{alg:policy_gradient_uniform_exploration} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{\hat{k}^*\left(t\right)}\left( \rmW(t)\right) \in \Omega(1).
\end{equation*}
\end{lem}
By \cref{lem:optimal_probability_increse_parameter_space}, we can replace the $c$ in \cref{thm:policy_gradient_main_result} without incurring any additional regret dependent on $T$.

\subsection{LOGIT LEARNING}
\label{subsec:theoretical_analyses_logit_learning}

It seems difficult to improve \cref{thm:policy_gradient_main_result}, as all the three parts of the regret are almost balanced, i.e., uniform exploration ($ T^{\frac{2}{3} + \beta}$, \cref{eq:total_regret_decomposition}), estimation error ($ T^{\frac{2}{3} + \beta} $, \cref{thm:reward_estimation_hoeffding}), and the cumulative expected loss of neural network policies ($ T^{\frac{2}{3} - \frac{\beta}{2}} $, \cref{thm:dynamic_regret_sublinear}). \cref{alg:policy_gradient_uniform_exploration} learns policies in the primal policy space by maximizing $\rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)$. And it is not necessary that $\rvo\left( \rmW(t)\right)$ is close to $\hat{\rvr}\left(t\right)$, due to the homogeneity invariant property of the softmax. To show the convergence of $\rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)$, every action needs to be pulled many times, which leads to the undesirable results.

We propose \cref{alg:logit_learning_eps_greedy_exploration}, i.e., learning in the dual value space by gradient updates over $\frac{1}{2} \left\| \rvo\left( \rmW\left(t\right)\right) - \hat{\rvr}\left(t\right) \right\|_2^2$. Although the logit learning is not as practical as the policy gradient, the results are better, as shown in \cref{thm:logit_learning_main_result}.

\begin{thm}
\label{thm:logit_learning_main_result}
    Given value neural networks as shown in \cref{fig:nn_value}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{h m}$, $\alpha > 3$, $\beta > 256$, the expected regret of \cref{alg:logit_learning_eps_greedy_exploration} satisfies,
\begin{equation*}
\small
\begin{split}
    \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{A_t} \right] } \le \frac{8 \sqrt{h}}{c} \cdot T^{\frac{2}{3}} + 3 \ln{h} \cdot T^{\frac{2}{3}} \left(\ln{T}\right)^{\frac{1}{3}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    By \citep{seldin2017improved},
    $\forall t \ge t_1 \triangleq \min{\left\{ t : t \ge \frac{4 h \beta \left(\ln{t}\right)^2}{\Delta_k^4 \ln{h}} \right\}}$,
\begin{equation*}
\small
\begin{split}
    \pr{\left\{ \hat{\Delta}_k\left(t\right) \ge \frac{3}{2} \Delta_k \right\}} \le \left( \frac{\ln{t}}{t \Delta_k^2} \right)^{\alpha - 2} + \frac{2}{h t^{\alpha - 1}} + 2 \left(\frac{1}{t}\right)^{\frac{\beta}{8}}.
\end{split}
\end{equation*}
With high probability, $\xi_k\left(t\right) \ge \frac{4 \beta \ln{t}}{9 t \Delta_k^2}$, which implies that $\forall t > t_3$,  each arm has been pulled at least $\frac{25\ln{t}}{\Delta^2}$ times. By Hoeffding's inequality,
\begin{equation*}
\begin{split}
    \pr{\left\{ \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \ge \frac{\Delta}{5} \right\}} \le \frac{2}{t^2}.
\end{split}
\end{equation*}
According to \cref{lem:logit_l2_loss_parameter_smoothness}, $\forall t \le \frac{\tau}{2 \eta}$,
\begin{equation*}
\begin{split}
    &\frac{1}{2} \left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2^2 \\
    &\quad = \frac{1}{2} \left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t\right)\right\|_2^2 \\
    &\qquad + \left( \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t\right) \right)^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t+1\right) \right) \\
    &\qquad + \frac{1}{2} \left\| \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t+1\right) \right\|_2^2 \\
    &\quad \le \left( 1 - \frac{1}{2 h m } \right) \cdot \frac{1}{2} \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2^2 \\
    &\qquad + \sqrt{1 - \frac{1}{2 h m } } \cdot \frac{\Delta^2 \sqrt{h}}{\ln{t}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \\
    &\qquad + \frac{\Delta^4 h}{2 \left( \ln{t} \right)^2 }
\end{split}
\end{equation*}
If $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \le \frac{\Delta}{5}$, $\forall \ln{t} \ge \frac{5 \Delta \sqrt{h}}{1 - \sqrt{1 - \frac{1}{2 h m}}}$,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2 \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \frac{\Delta^2 \sqrt{h}}{\ln{t}} \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \frac{\Delta}{5} + \frac{\Delta^2 \sqrt{h}}{\ln{t}} \\
    &\quad \le \frac{\Delta}{5}.
\end{split}
\end{equation*}
If $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 > \frac{\Delta}{5}$, $\forall \ln{t} \ge \frac{10 \Delta \sqrt{h}}{1 - \sqrt{1 - \frac{1}{2 h m}}}$,
\begin{equation*}
\begin{split}
    \frac{\Delta^2 \sqrt{h}}{\ln{t}} &\le \frac{1 - \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \frac{\Delta}{5} \\
    &< \frac{1 - \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2,
\end{split}
\end{equation*}
which implies,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2 \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \frac{\Delta^2 \sqrt{h}}{\ln{t}} \\
    &\quad < \frac{1 + \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2.
\end{split}
\end{equation*}
Since $\frac{1 + \sqrt{1 - \frac{1}{2 h m}}}{2} \in \left(0, 1\right)$, after constant numbers of iterations, we have $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \le \frac{\Delta}{5}$ always holds. By the triangle inequality,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t) \right) - \rvr \right\|_\infty \\
    &\quad \le \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_\infty + \left\| \hat{\rvr}\left(t\right) - \rvr\right\|_\infty \\
    &\quad \le \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \left\| \hat{\rvr}\left(t\right) - \rvr\right\|_\infty \\
    &\quad \le \frac{2 \Delta}{5}. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:logit_l2_loss_parameter_smoothness}
$\rmW(t+1) \leftarrow \rmW(t) - \eta \cdot \frac{d \left\{ \frac{1}{2} \left\| \rvo\left( \rmW\left(t\right)\right) - \hat{\rvr}\left(t\right) \right\|_2^2 \right\}}{d \rmW(t)}$, where $\eta = \frac{1}{h m}$. $\forall t \le \frac{\tau}{2 \eta}$,
\begin{equation*}
\begin{split}
    &\frac{1}{2} \left\| \rvo\left( \rmW\left(t+1\right) \right) - \hat{\rvr}\left(t\right) \right\|_2^2 \\
    &\quad \le \left( 1 - \frac{1}{2 h m} \right) \cdot \frac{1}{2} \left\| \rvo\left( \rmW\left(t\right) \right) - \hat{\rvr}\left(t\right) \right\|_2^2.
\end{split}
\end{equation*}
\end{lem}
