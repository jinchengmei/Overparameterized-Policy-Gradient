\section{THEORETICAL ANALYSES}
\label{sec:theoretical_analyses}

In this section, we first prove the $T^{\frac{2}{3}}\log{T}$ finite time regret of \cref{alg:policy_gradient_uniform_exploration}, and then we prove the optimal finite time regret of \cref{alg:logit_learning_eps_greedy_exploration}.

\subsection{POLICY GRADIENT}
\label{subsec:theoretical_analyses_policy_gradient}

We first present the main results as shown in \cref{thm:policy_gradient_main_result}, and then discuss the detailed proof ideas and intuitions.

\subsubsection{Main Results}
\label{subsubsec:main_results_policy_gradient}

\begin{thm}
\label{thm:policy_gradient_main_result}
    Given policy neural networks as shown in \cref{fig:nn_policy}, with number of parameters $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, the expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
\begin{equation*}
\small
\begin{split}
    \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{A_t} \right] } \le \frac{8 \sqrt{h}}{c} \cdot T^{\frac{2}{3}} + 3 \log{h} \cdot T^{\frac{2}{3}} \left(\log{T}\right)^{\frac{1}{3}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
According to \cref{alg:policy_gradient_uniform_exploration}, the uniform policy and $\rvpi\left( \rmW(t) \right)$ are used to sample actions in the two phases. Therefore the regret is divided into two parts.
\begin{equation}
\small
\label{eq:total_regret_decomposition}
\begin{split}
    R_T &\triangleq \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{ A_t} \right] } \\
    &= \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{ {\rvpi^*}^\top \rvr} - \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta} - 1}{ \expectation\limits_{A_t \sim \gU\left[h\right]}{ \left[ r_{A_t} \right] }} \\
    &\quad + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ {\rvpi^*}^\top \rvr }  - \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi\left( \rmW(t) \right)^\top \rvr } \\
    &\le T^{\frac{2}{3} + \beta} + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi\left( \rmW(t) \right)^\top \rvr }.
\end{split}
\end{equation}
Denote $\rvpi^*\left(t\right) \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}\left(t\right)\right\}}$. The last two terms can be decomposed as follows, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation}
\small
\label{eq:playing_learning_phase_regret_decomposition}
\begin{split}
    \left( {\rvpi^*} - \rvpi\left( \rmW(t) \right) \right)^\top \rvr &= \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad - \left( {\rvpi^*\left(t\right)} - {\rvpi^*} \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + \left( \rvpi\left( \rmW(t) \right) - {\rvpi^*}\right)^\top \left( \hat{\rvr}\left(t\right) - \rvr \right) \\
    &\le \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + \left\| \rvpi\left( \rmW(t) \right) - \rvpi^* \right\|_1 \cdot \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty  \\
    &\le \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) \\
    &\quad + 4 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 2 T^{\beta - \frac{1}{3}},
\end{split}
\end{equation}
by the definition of $\rvpi^*\left(t\right)$, H{\"o}lder's inequality, and \cref{thm:loss_estimation_hoeffding}. Summing up \cref{eq:playing_learning_phase_regret_decomposition} from $t = T^{\frac{2}{3} + \beta}$ to $T - 1$, and
combining \cref{eq:total_regret_decomposition} and \cref{thm:dynamic_regret_sublinear},
\begin{equation*}
\begin{split}
    R_T \le  4 T \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + 3 T^{\frac{2}{3} + \beta} + \frac{2 \sqrt{h}}{ c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Taking $\beta = \frac{ \log{\left(\frac{h}{6}\right) + \log{\log{T}} } }{ 3 \log{T}} $ completes the proof.
\end{proof}

\cref{alg:policy_gradient_uniform_exploration} divides $T$ steps into two parts. In the first exploring phase, the agent uniformly samples actions without learning the neural network policy. Intuitively, at the beginning, when the loss estimation is very inaccurate, early updating can probably hurt the neural network policy.  While in the second playing-learning phase, since the loss estimation is good, the neural network policy will keep reducing its dynamic expected loss, which is highly related with its true expected loss, i.e., the expected regret of the playing-learning phase.

\subsubsection{Exploring Phase}
\label{subsubsec:exploring_phase}

The exploring phase of \cref{alg:policy_gradient_uniform_exploration} provides us good estimations of the true mean loss/reward as follows.
\begin{thm}
\label{thm:loss_estimation_hoeffding}
    In \cref{alg:policy_gradient_uniform_exploration}, $\forall t \ge T^{\frac{2}{3} + \beta}$,
\begin{equation*}
    \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + T^{\beta - \frac{1}{3}}.
\end{equation*}
\end{thm}
\begin{proof}
    At step $t \ge T^{\frac{2}{3} + \beta}$, the $k$th action is sampled $n_{k}\left(t\right) \ge \frac{T^{\frac{2}{3} + \beta} }{h}$ times because of the exploring phase, $\forall k \in [h]$. By Hoeffding's inequality, $\forall k \in [h]$,
\begin{equation}
\label{eq:loss_estimation_hoeffding}
\begin{split}
    &\pr\left\{ \left| \hat{r}_{ k}\left(t\right) - r_{k} \right| > T^{\beta - \frac{1}{3}} \right\} \le 2 \exp\left\{ - 2 n_{ k}\left(t\right) \cdot T^{2\beta - \frac{2}{3}} \right\} \\
    &\le 2 \exp\left\{ -  \frac{2 T^{\frac{2}{3} + \beta}}{h} \cdot T^{2\beta - \frac{2}{3}} \right\} = 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\}.
\end{split}
\end{equation}
Therefore,
\begin{equation*}
\begin{split}
    \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} \cdot 1 + 1 \cdot T^{\beta - \frac{1}{3}},
\end{split}
\end{equation*}
since $\pr\left\{ \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le T^{\beta - \frac{1}{3}} \right\} \le 1$, $\left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \le 1$, and according to \cref{eq:loss_estimation_hoeffding}, 
\end{proof}

\subsubsection{Playing-Learning Phase}
\label{subsubsec:playing_learning_phase}

The good estimation of the true mean loss $\hat{\rvr}\left(t\right)$ obtained at the end of the exploring phase will be used to train the neural network policy $\rvpi\left( \rmW(t) \right)$. However, the recently proposed optimization theory of over-parameterized neural networks is developed for supervized learning settings \citep{li2018learning,allen2018convergenceB}, where the learning objective functions are fixed. There are two main differences between our results and existing work, (a) in \cref{alg:policy_gradient_uniform_exploration}, the objectives are dynamic with respect to step $t$; (b) the optimal action need enough exploration during learning (explained later on in \cref{subsubsec:exploration_in_policy_learning}). We first prove the cumulative dynamic expected loss of $\rvpi\left( \rmW(t) \right)$ is sublinear. Then we show some intuitions with lemmas, the proofs of which can be found in the appendix.
\begin{thm}
\label{thm:dynamic_regret_sublinear}
    Denote $\rvpi^*\left(t\right) \triangleq \argmax\limits_{\rvpi \in \Delta^{h-1}}{\left\{ \rvpi^\top \hat{\rvr}\left(t\right)\right\}}$. If $m \ge \frac{T^2}{h^2}$, $\eta = \frac{1}{2 h m}$, then the dynamic regret in the playing-learning phase satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{ \left(  {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right) } \le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    Denote $\delta_t \triangleq \left( {\rvpi^*\left(t\right)} - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t\right)$.
\begin{equation}
\label{eq:dynamic_regret_decomposition}
\begin{split}
    \delta_t &= {\rvpi^*\left(t\right)}^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t-1\right)\right) \\
    &\quad + \left( {\rvpi^*\left(t\right)} - {\rvpi^*\left(t-1\right)} \right)^\top \hat{\rvr} \left(t-1\right) \\
    &\quad + \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad + \left(  \rvpi\left( \rmW(t-1) \right) - \rvpi\left( \rmW(t) \right) 
    \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad + \rvpi\left( \rmW(t) \right)^\top \left( \hat{\rvr}\left(t-1\right) - \hat{\rvr}\left(t\right) \right).
\end{split}
\end{equation}
We upper bound each term in the right hand side. Firstly,
\begin{equation*}
\begin{split}
    &{\rvpi^*\left(t\right)}^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t-1\right)\right) \\
    &\quad = \frac{{\pi_{ A_{t-1}}^*\left(t\right)}}{n_{ A_{t-1}}\left(t\right)} \left[ R_{ A_{t-1}}\left( n_{ A_{t-1}}\left(t\right) -1 \right) - \hat{r}_{ A_{t-1}}\left(t-1\right) \right] \\
    &\quad \le \frac{1}{n_{A_{t-1}}\left(t\right)} \le \frac{h}{T^{\frac{2}{3} + \beta}},
\end{split}
\end{equation*}
by $n_{A_{t-1}}\left(t\right) = n_{ A_{t-1}}\left(t-1\right) + 1$, and $n_{ A_{t-1}}\left(t\right) \ge \frac{T^{\frac{2}{3} + \beta}}{h}$, $\forall t \ge T^{\frac{2}{3} + \beta}$. By the definition of ${\rvpi^*\left(t-1\right)}$,
\begin{equation*}
    \left( {\rvpi^*\left(t\right)} - {\rvpi^*\left(t-1\right)} \right)^\top \hat{\rvr}\left(t-1\right) \le 0.
\end{equation*}
Note that according to the definition of $\delta_t$,
\begin{equation*}
    \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right) = \delta_{t-1}.
\end{equation*}
By \cref{lem:parameter_smoothness}, $\eta = \frac{1}{2 h m}$, and \cref{lem:gradient_lower_bound},
\begin{equation*}
\begin{split}
    &\left( \rvpi\left( \rmW(t-1) \right) - \rvpi\left( \rmW(t) \right) \right)^\top \hat{\rvr}\left(t-1\right) \\
    &\quad \le - \frac{1}{4 h m} \left\| \frac{d \rvpi\left( \rmW(t-1) \right)^\top \hat{\rvr}\left(t-1\right)}{d \rmW(t-1)} \right\|_F^2 \\
    &\quad = - \frac{1}{4 h m} \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi\left( \rmW(t-1) \right)^\top \hat{\rvr}\left(t-1\right)}{d \rvw_r(t-1)} \right\|_2^2 } \\
    &\quad \le - \frac{c^2}{4 h} \left[ \left( {\rvpi^*\left(t-1\right)} - \rvpi\left( \rmW(t-1) \right) \right)^\top \hat{\rvr}\left(t-1\right)  \right]^2 \\
    &\quad = - \frac{c^2}{4 h} \cdot \delta_{t-1}^2.
\end{split}
\end{equation*}
Using similar arguments,
\begin{equation*}
    \rvpi\left( \rmW(t) \right)^\top \left( \hat{\rvr}\left(t-1\right) - \hat{\rvr}\left(t\right)  \right) \le \frac{h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Plugging the above upper bounds into \cref{eq:dynamic_regret_decomposition},
\begin{equation*}
    \delta_t \le \delta_{t-1} - \frac{c^2}{4 h} \cdot \delta_{t-1}^2 + \frac{2h}{T^{\frac{2}{3} + \beta}}.
\end{equation*}
Rearranging and summing up from $T^{\frac{2}{3} + \beta} + 1$ to $T$,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2} &\le \frac{4 h}{ c^2} \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T} { \left[ \delta_{t-1} - \delta_t + \frac{2h}{T^{\frac{2}{3} + \beta}} \right] } \\
    &\le \frac{4 h}{ c^2} \cdot T^{\frac{1}{3} - \beta}.
\end{split}
\end{equation*}
By the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \sum\limits_{t=T^{\frac{2}{3}+ \beta}}^{T-1}{\delta_{t}} &\le \sqrt{\left(T  - T^{\frac{2}{3}+ \beta} \right) \sum\limits_{t=T^{\frac{2}{3}+ \beta}+1}^{T}{\delta_{t-1}^2}} \\
    &\le \frac{2 \sqrt{h}}{c} \cdot T^{\frac{2}{3} - \frac{\beta}{2}}.
\end{split}
\end{equation*}
Let $\tau = \sigma$ in \cref{lem:gradient_coupling}. Let $\sigma = \frac{1}{\sqrt{m}}$ and $T \le \frac{\tau}{2 \eta}$ such that \cref{lem:gradient_coupling} can be used during all the playing-learning phase, we have $m \ge \frac{T^2}{h^2}$.
\end{proof}



\cref{thm:dynamic_regret_sublinear} relies on two arguments. First, the dynamic expected loss is smooth in the logit space, and small policy gradient updates preserve the signs of ReLU outputs, therefore highly correlates the logit derivative and the policy gradient. Second, by the over-parameterization theory, gradient norm is lower bounded by expected loss around initialization, which means there is no bad local minima near the randomly initialized neural network policy $\rvpi\left( \rmW(0) \right)$.

\begin{lem}
\label{lem:logit_smoothness}
Let $\rvpi\left( \rvo^\prime \right)$ and $\rvpi\left( \rvo \right)$ be the softmax policies of any logit vectors $\rvo^\prime, \rvo \in \sR^h$, respectively. $\forall \rvr \in \left[ 0, 1\right]^h$,
\begin{equation*}
    \rvpi\left( \rvo^\prime \right)^\top \rvr \le \rvpi\left( \rvo \right)^\top \rvr + \left\langle \frac{d \rvpi\left( \rvo \right)^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle + \left\| \rvo^\prime - \rvo \right\|_2^2.
\end{equation*}
\end{lem}
Let $\rvo(t+1) = \rvo(t) + \eta \cdot \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)}$, and let $\rvr = - \hat{\rvr}\left(t\right)$, $\eta = \frac{1}{2}$ in \cref{lem:logit_smoothness},
\begin{equation*}
\begin{split}
\small
    \rvpi\left( \rvo(t+1) \right)^\top \hat{\rvr}\left(t\right) - \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right) \ge\frac{1}{4} \left\| \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)} \right\|_2^2.
\end{split}
\end{equation*}
Note the logit derivative norm is lower bounded by loss,
\begin{equation}
\label{eq:logit_derivative_lower_bound}
\begin{split}
    \left\| \frac{d \rvpi\left( \rvo(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rvo(t)} \right\|_2 \ge \pi_{\hat{k}^*\left(t\right)} \cdot \left( \hat{r}_{\hat{k}^*\left(t\right)} - \rvpi\left( \rvo\left(t\right) \right)^\top \hat{\rvr}\left(t\right) \right),
\end{split}
\end{equation}
where $\hat{k}^*\left(t\right) \triangleq \argmax\limits_{ k \in [h]}{\left\{ \hat{r}\left(t\right) \right\}}$. Therefore, by \cref{lem:logit_smoothness} and \cref{eq:logit_derivative_lower_bound}, we can derive similar relationship for $\delta_t$ and sublinear regret for update in the logit space. However, in practice, $\rvpi$ is updated in the parameter space. When the parameters are updated from $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right]$ to ${\rmW^\prime}^\top \triangleq \left[ \rvw_1^\prime, \rvw_2^\prime, \dots, \rvw_m^\prime \right]$,
\begin{equation*}
\begin{split}
    \rvo^\prime - \rvo = \rmA \left[ \sigma \left( \rmW^\prime \rvs \right) - \sigma \left( \rmW \rvs \right) \right].
\end{split}
\end{equation*}
The difficulty is that after updating the parameters, possibly many signs in the ReLU components also change. This can be circumvented by restraining the updates around the initialization \citep{li2018learning}.

\begin{lem}
\label{lem:gradient_coupling}
	Define the pseudo policy gradient at step $t$ as,
\begin{equation*}
\begin{split}
	\frac{d \tilde{\ell}(t)}{d \rmW(t)} \triangleq \tilde{\rmD} \rmA^\top \rmH\left( \rvpi\left(\rmW(t)\right) \right) \hat{\rvr}\left(t\right) \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD} \in \sR^{h \times h}$ is a diagonal matrix, and  $\tilde{\rmD}_{r,r} \triangleq \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\rmH\left( \rvpi\left(\rmW(t)\right) \right)$ is,
\begin{equation*}
    \rmH\left( \rvpi\left(\rmW(t)\right) \right) \triangleq \Delta\left( \rvpi\left(\rmW(t)\right) \right) - \rvpi\left(\rmW(t)\right) \rvpi\left(\rmW(t)\right)^\top.
\end{equation*}
The true policy gradient is,
\begin{equation*}
\begin{split}
    \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)} \triangleq  \rmD(t) \rmA^\top \rmH\left( \rvpi\left(\rmW(t)\right) \right) \hat{\rvr}\left(t\right) \rvs^\top.
\end{split}
\end{equation*}
where $\rmD_{r,r}(t) \triangleq \sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\}$, $\forall r \in [m]$. $\forall \tau > 0$, $\forall r \in [m]$, with probability at least $1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$, $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
	\frac{d\tilde{\ell}(t)}{d \rvw_r(t)} = \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW(t)$.
\end{lem}

\cref{lem:gradient_coupling} implies that for bounded numbers of policy gradient updates, the signs of the ReLUs will not change, i.e., $\sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\} = \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$. Combine \cref{lem:logit_smoothness} with \cref{lem:gradient_coupling}, we have the smoothness property of the surrogate expected loss in the parameter space.
\begin{lem}
\label{lem:parameter_smoothness}
    $\rmW(t+1) = \rmW(t) + \eta \cdot \frac{d \rvpi\left(\rmW(t)\right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)}$. $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
    \rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right) &\le \rvpi\left( \rmW(t+1) \right)^\top \hat{\rvr}\left(t\right) \\
    &\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! - \left( \eta - h m \eta^2 \right) \cdot \left\| \frac{d \rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)}{d \rmW(t)} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}

Now by the key insight of the recent progresses of the over-parameterized neural network optimization theory, with constants probability, the pseudo gradient norm is lower bounded by the objective \citep{li2018learning}. However, unlike the supervised learning, RL has exploration issue, as shown in \cref{subsubsec:exploration_in_policy_learning}. Our result contains an exploration related term, which is consistent with \cref{eq:logit_derivative_lower_bound}, making guarantees for exploring the optimal action necessary during learning.

\begin{lem}
\label{lem:gradient_lower_bound}
	Denote $\hat{k}^*(t) \triangleq \argmax\limits_{k \in [h]}\left\{ \hat{r}_{k}\left(t\right) \right\}$, i.e., the optimal action using the estimated reward $ \hat{\rvr}\left(t\right)$. If $\pi_{\hat{k}^*(t)}\left(\rmW(t)\right) > c_t > 0$, with probability $\frac{3}{64} \in \Omega\left( 1 \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}\left(t\right)}{d \rvw_r(t)} \right\|_2 \ge c_t \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_k\left(t\right) \right\} - \rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right) \right) .
\end{split}
\end{equation*}
\end{lem}

\cref{lem:gradient_lower_bound} generalizes the over-parameterized neural network optimization theory into the RL settings. By \cref{lem:gradient_lower_bound}, whenever the policy empirically expected reward $\rvpi\left( \rmW(t) \right)^\top \hat{\rvr}\left(t\right)$ small comparing with the largest possible empirical reward $\max\limits_{k \in \left[h\right]}\left\{ \hat{r}_k\left(t\right) \right\}$, with enough exploration of the suggorate optimal action ($\pi_{\hat{k}^*(t)}\left(\rmW(t)\right) > c_t > 0$), with constant probability, the pseudo policy gradient norm will also be large. Therefore, combining \cref{lem:gradient_lower_bound} with \cref{lem:gradient_coupling}, the true policy gradient norm is also large, which is necessary for using \cref{lem:parameter_smoothness}. Applying all the stated lemmas, the policy surrogate expected loss converges as shown in \cref{thm:dynamic_regret_sublinear}.

\subsubsection{Exploration in Policy Learning}
\label{subsubsec:exploration_in_policy_learning}

In \cref{subsec:policy_gradient}, it is claimed that the vanilla policy gradient method will suffer the lack of exploration issue. Now we provide some intuitions. Consider the derivative of the true expected loss with respect to the logits,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo} = \left[ \Delta\left( \rvpi_i \right) - \rvpi_i \rvpi_i^\top \right] \rvtilder_i,
\end{split}
\end{equation}
where $\Delta\left( \rvpi_i \right)$ is a diagonal matrix with $\Delta\left( \rvpi_i \right)_{k,k} = \pi_{i,k}$, $\forall k \in [h]$. For the $k$th action, the derivative value is $\pi_{i,k} \cdot \left( \tilde{r}_{i,k} - \rvpi_i^\top \rvtilder_i \right)$. Suppose the $k$th action is worth learning, i.e, $\rvpi_i^\top \rvtilder_i - \tilde{r}_{i,k} > 0$ is large, meaning this action will occur loss $\tilde{r}_{i,k}$ much smaller than the expected loss of the current policy, so the agent should increase its action logit. But if $\pi_{i,k}$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi_{i,k}$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi_{i,k}$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{i,k}(t) > c_t > 0$ should hold for each time step $t$, where $c_t$ is a constant (cannot be something like $\frac{1}{t^\alpha}$, $\alpha> 0$). In particular, to learn an optimal policy, $\pi_{i,k_i^*}(t) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $\tilde{r}_{i,k_i^*} = 0$, and $c \triangleq \min\limits_{t \ge 0}{\left\{  c_t \right\}}$.

Consider policy update using the logit derivatives \cref{eq:logit_derivative} with the true loss as the objective. With learning rate $\eta > 0$, for each action $k \in [h]$, the logit increment between two consecutive time steps is,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o\left( t+1 \right)_{k} - o\left( t \right)_{k} = \eta \cdot \pi\left( \rvo(t)\right)_{i,k} \cdot \left( \rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} \right),
\end{split}
\end{equation*}
which mean as long as $\pi\left( \rvo(t)\right)_{i,k} > 0$, for any valuable action $k$ with its true action loss smaller than the current policy expected loss, i.e., $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} > 0$, at the next time step $t+1$, the policy logit for this action will increase. While for any bad actions with $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} < 0$, their logits will decrease.

Now consider the initialized policy $\rvpi_i\left( \rvo(0) \right)$, which is (very close to) the uniform policy, with $\pi\left( \rvo(0)\right)_{i,k_i^*} \approx \frac{1}{h} \in \Omega(1)$. Also note that $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k_i^*}$ is larger than any other action $k$, which is consistent with intuition because the optimal action is the most valuable for learning. With the same learning rate, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
$\forall t \ge 0$,
\begin{equation*}
    \pi\left( \rvo(t+1)\right)_{i,k_i^*} \ge \pi\left( \rvo(t)\right)_{i,k_i^*} \in \Omega(1).
\end{equation*}
\end{lem}

With this result in mind, \cref{alg:policy_gradient_uniform_exploration} actually does similar things. First, after the exploring phase, it obtains a good estimation $\hat{\rvtilder}_i$ of the true loss $\rvtilder_i$, thus the surrogate expected loss will be closed to the true expected loss. Second, the initialized policy $\rvpi_i\left( \rmW(0) \right)$ is very closed to the uniform policy with $\pi\left( \rmW(0)\right)_{i, \hat{k}_i^*} \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is restrained around initialization \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, after each policy gradient update, the optimal action logit will increase more than any other  suboptimal actions, which implies similar results with \cref{lem:optimal_probability_increse_logit_space} as follows,
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
$\forall t \ge 0$,
\begin{equation*}
    \pi\left( \rmW(t+1)\right)_{i,\hat{k}_i^*} \ge \pi\left( \rmW(t)\right)_{i,\hat{k}_i^*} \in \Omega(1).
\end{equation*}
\end{lem}
By \cref{lem:optimal_probability_increse_parameter_space}, we can safely replace the $c$ and $c_t$ values in the main result \cref{thm:policy_gradient_main_result} and all the other intermediate lemmas and theorems, without incurring any additional regret dependent on $T$.

\subsection{LOGIT LEARNING}
\label{subsec:theoretical_analyses_logit_learning}

We first present the main results as shown in \cref{thm:logit_learning_main_result}, and then discuss the detailed proof ideas and intuitions.

\begin{thm}
\label{thm:logit_learning_main_result}
    Given value neural networks as shown in \cref{fig:nn_value}, with number of parameters $m \in \Theta\left( \frac{1}{c^4 \varepsilon^2} \right)$, $\eta = \frac{1}{2 h m}$, the expected regret of \cref{alg:logit_learning_eps_greedy_exploration} satisfies,
\begin{equation*}
\small
\begin{split}
    \sum\limits_{t=0}^{T-1}{ {\rvpi^*}^\top \rvr } - \sum\limits_{t=0}^{T-1}{ \sE \left[ r_{A_t} \right] } \le \frac{8 \sqrt{h}}{c} \cdot T^{\frac{2}{3}} + 3 \log{h} \cdot T^{\frac{2}{3}} \left(\log{T}\right)^{\frac{1}{3}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    Since $\xi_t = \frac{\log{t}}{t \hat{\Delta}^2\left(t\right)} \ge \frac{\log{t}}{t}$, $\forall t > t_1$, each arm has been pulled at least $\log{t}$ times. Therefore $\forall t > t_2$,
\begin{equation*}
\begin{split}
    \pr{\left\{ \hat{\Delta} \ge \frac{3}{2} \Delta \right\}} \le 
\end{split}
\end{equation*}
So with probability at least $1 - $, $\xi_t \ge \frac{4 \log{t}}{t \Delta^2}$, which implies that $\forall t > t_3$,  each arm has been pulled at least $\frac{25\log{t}}{\Delta^2}$ times. By Hoeffding's inequality,
\begin{equation*}
\begin{split}
    \pr{\left\{ \left\| \hat{\rvr}\left(t\right) - \rvr \right\|_\infty \ge \frac{\Delta}{5} \right\}} \le \frac{2}{t^2}.
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
    &\frac{1}{2} \left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2^2 \\
    &\quad = \frac{1}{2} \left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t\right)\right\|_2^2 \\
    &\qquad + \left( \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t\right) \right)^\top \left( \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t+1\right) \right) \\
    &\qquad + \frac{1}{2} \left\| \hat{\rvr}\left(t\right) - \hat{\rvr}\left(t+1\right) \right\|_2^2 \\
    &\quad \le \left( 1 - \frac{1}{2 h m } \right) \cdot \frac{1}{2} \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2^2 \\
    &\qquad + \sqrt{1 - \frac{1}{2 h m } } \cdot \frac{\Delta^2 \sqrt{h}}{\log{t}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \\
    &\qquad + \frac{\Delta^4 h}{2 \left( \log{t} \right)^2 }
\end{split}
\end{equation*}
If $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \le \frac{\Delta}{5}$, $\forall \log{t} \ge \frac{5 \Delta \sqrt{h}}{1 - \sqrt{1 - \frac{1}{2 h m}}}$,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2 \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \frac{\Delta^2 \sqrt{h}}{\log{t}} \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \frac{\Delta}{5} + \frac{\Delta^2 \sqrt{h}}{\log{t}} \\
    &\quad \le \frac{\Delta}{5}.
\end{split}
\end{equation*}
If $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 > \frac{\Delta}{5}$, $\forall \log{t} \ge \frac{10 \Delta \sqrt{h}}{1 - \sqrt{1 - \frac{1}{2 h m}}}$,
\begin{equation*}
\begin{split}
    \frac{\Delta^2 \sqrt{h}}{\log{t}} &\le \frac{1 - \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \frac{\Delta}{5} \\
    &< \frac{1 - \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2,
\end{split}
\end{equation*}
which implies,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t+1) \right) - \hat{\rvr}\left(t+1\right)\right\|_2 \\
    &\quad \le \sqrt{1 - \frac{1}{2 h m}} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \frac{\Delta^2 \sqrt{h}}{\log{t}} \\
    &\quad < \frac{1 + \sqrt{1 - \frac{1}{2 h m}}}{2} \cdot \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2.
\end{split}
\end{equation*}
Since $\frac{1 + \sqrt{1 - \frac{1}{2 h m}}}{2} \in \left(0, 1\right)$, after constant numbers of iterations, we have $\left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 \le \frac{\Delta}{5}$ always holds. By the triangle inequality,
\begin{equation*}
\begin{split}
    &\left\| \rvo\left( \rmW(t) \right) - \rvr \right\|_\infty \\
    &\quad \le \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_\infty + \left\| \hat{\rvr}\left(t\right) - \rvr\right\|_\infty \\
    &\quad \le \left\| \rvo\left( \rmW(t) \right) - \hat{\rvr}\left(t\right)\right\|_2 + \left\| \hat{\rvr}\left(t\right) - \rvr\right\|_\infty \\
    &\quad \le \frac{2 \Delta}{5}.
\end{split}
\end{equation*}
\end{proof}