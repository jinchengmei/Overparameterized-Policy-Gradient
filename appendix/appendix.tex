\documentclass[10pt]{article}
\usepackage[usenames]{color} %used for font color
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage[utf8]{inputenc} %useful to type directly diacritic characters
\usepackage[capitalize]{cleveref}
\crefname{prop}{Proposition}{Propositions}
\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\crefname{algorithm}{Algorithm}{Algorithms}

\def\rva{{\mathbf{a}}}
\def\rvo{{\mathbf{o}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvu{{\mathbf{u}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvg{{\mathbf{g}}}
\def\rvo{{\mathbf{o}}}
\def\rvone{{\mathbf{1}}}
\def\rvzero{{\mathbf{0}}}
\def\rvtilder{{\tilde{\mathbf{r}}}}
\def\rvhat{{\hat{\mathbf{r}}}}

\def\rvp{{\mathbf{p}}}

\def\pr{{\text{Pr}}}
\def\r{{\text{R}}}

\def\regret{{\text{Regret}}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{defi}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{remk}{Remark}


\def\rvpi{{\boldsymbol{\pi}}}

\def\rmA{{\mathbf{A}}}
\def\rmD{{\mathbf{D}}}
\def\rmI{{\mathbf{I}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}

\def\sE{{\mathbb{E}}}
\def\sR{{\mathbb{R}}}
\def\sI{{\mathbb{I}}}

\def\gH{{\mathcal{H}}}
\def\gN{{\mathcal{N}}}
\def\gE{{\mathcal{E}}}
\def\gU{{\mathcal{U}}}


\title{Appendix of ``On Optimal Stochastic Bandit Algorithms and Provable Policy Gradient Methods with Neural Networks''}
\author{}
\date{}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\probability}{Pr}
\DeclareMathOperator*{\expectation}{\sE}
\DeclareMathOperator*{\trace}{Tr}


\begin{document}

\maketitle

\section{Proofs}

Given the true mean reward $\rvr \in \left[ 0, 1 \right]^h$, define $\rvtilder \triangleq \max\limits_{k^\prime \in [h]}\left\{ r_{k^\prime } \right\} \cdot \rvone - \rvr$. Note that,
\begin{equation*}
\begin{split}
    \rvpi^\top \rvtilder = \max\limits_{k^\prime \in [h]}\left\{ r_{k^\prime }\right\} - 
\end{split}
\end{equation*}

\begin{lem}
    Let $\rvo \in \sR^h$ be any logit vector, and $\rvpi_i(\rvo)$ be the softmax policy of $\rvo$ at state $\rvs_i$. Denote $\gH\left( \rvpi_i \left(\rvo \right) \right) \triangleq \Delta \left( \rvpi_i \left(\rvo \right) \right) - \rvpi_i \left(\rvo \right) \rvpi_i \left(\rvo \right)^\top$.
\begin{equation*}
    \left\| \frac{d \gH\left( \rvpi_i \left(\rvo \right) \right) \rvtilder_i}{d \rvo } \right\|_2 \le 2.
\end{equation*}
\end{lem}
\begin{proof}
    Denote $\rmA \triangleq \frac{d \gH\left( \rvpi_i \left(\rvo \right) \right) \rvtilder_i}{d \rvo } \in \sR^{h \times h}$. $\forall s, t \in [h]$,
\begin{equation*}
\begin{split}
    A_{s, t} &= \frac{d \pi_{i,s} \left( \tilde{r}_{i,s} - \rvpi_i^\top \rvtilder_i \right) }{d o_{t}} \\
    &= \frac{d \pi_{i,s} }{d o_{t}} \left( \tilde{r}_{i,s} - \rvpi_i^\top \rvtilder_i \right) + \pi_{i,s} \frac{d \left( \tilde{r}_{i,s} - \rvpi_i^\top \rvtilder_i \right) }{d o_{t}} \\
    &=\left ( \sI\left\{ s = t\right\} \pi_{i,t} -  \pi_{i,s } \pi_{i,t} \right) \left( \tilde{r}_{i,s} - \rvpi_i^\top \rvtilder_i \right) - \pi_{i,s} \left( \pi_{i,t} \tilde{r}_{i,t} - \pi_{i,t} \rvpi_i^\top \rvtilder_i \right).
\end{split}
\end{equation*}
For any vector $\rvx \in \sR^h$, 
\begin{equation*}
\begin{split}
    \rvx^\top \rmA \rvx &= \sum\limits_{s=1}^{h}{ \sum\limits_{t=1}^{h}{ A_{s,t} x_s x_t} } \\
    &= \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \right] \cdot \expectation\limits_{\rvpi_i}\left[ \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \right] \cdot \expectation\limits_{\rvpi_i}\left[ \rvx \right] + \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \right] \cdot \left( \expectation\limits_{\rvpi_i}\left[ \rvx \right] \right)^2 \\
    &\qquad - \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \right] \cdot \expectation\limits_{\rvpi_i}\left[ \rvx \right] + \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \right] \cdot \left( \expectation\limits_{\rvpi_i}\left[ \rvx \right] \right)^2 \\
    &\le \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \right] \cdot \expectation\limits_{\rvpi_i}\left[ \rvx \right] - \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \cdot \rvx \right] \cdot \expectation\limits_{\rvpi_i}\left[ \rvx \right] + \expectation\limits_{\rvpi_i}\left[ \rvtilder_i \right] \cdot \left( \expectation\limits_{\rvpi_i}\left[ \rvx \right] \right)^2 \\
    &\le \left\| \rvpi_i \right\|_2 \cdot \left\| \rvtilder_i \cdot \rvx \cdot \rvx \right\|_2 + 2 \cdot \left\| \rvpi_i \right\|_2 \cdot \left\| \rvtilder_i \cdot \rvx \right\|_2 \cdot \left\| \rvpi_i \right\|_2 \cdot \left\| \rvx \right\|_2 + \left\| \rvpi_i \right\|_2^2 \cdot \left\| \rvtilder_i \cdot \rvx \right\|_2^2 \\
    &\le \left\| \rvtilder_i \cdot \rvx \cdot \rvx \right\|_2 + 2 \cdot \left\| \rvtilder_i \cdot \rvx \right\|_2 \cdot \left\| \rvx \right\|_2 + \left\| \rvtilder_i \cdot \rvx \right\|_2^2 \\
    &\le \left\| \rvx \cdot \rvx \right\|_2 + 2 \cdot \left\| \rvx \right\|_2^2 + \left\| \rvx \right\|_2^2 \\
    &\le 4 \cdot \left\| \rvx \right\|_2^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}[Lemma 1 in the paper]
\label{lem:logit_smoothness}
\begin{equation*}
    \rvpi_i\left( \rvo^\prime \right)^\top \rvtilder_i \le \rvpi_i\left( \rvo \right)^\top \rvtilder_i + \left\langle \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle + 2 \left\| \rvo^\prime - \rvo \right\|_2^2.
\end{equation*}
\end{lem}
\begin{proof}
Denote $\bar{\rvo}_{\xi} = \rvo + \xi \left( \rvo^\prime - \rvo \right)$.
\begin{equation*}
\begin{split}
    &\left| \rvpi_i\left( \rvo^\prime \right)^\top \rvtilder_i - \rvpi_i\left( \rvo \right)^\top \rvtilder_i - \left\langle \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &= \left| \int_0^1{ \frac{d \rvpi_i\left( \rvo + \xi \left( \rvo^\prime - \rvo \right) \right)^\top \rvtilder_i}{d \xi} d\xi} - \left\langle \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &= \left| \int_0^1{ \left\langle \frac{d \rvpi_i\left( \rvo + \xi \left( \rvo^\prime - \rvo \right) \right)^\top \rvtilder_i}{d \left( \rvo + \xi \left( \rvo^\prime - \rvo \right) \right)}, \rvo^\prime - \rvo \right\rangle d\xi} - \left\langle \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &= \left| \int_0^1{ \left\langle \frac{d \rvpi_i\left( \bar{\rvo}_{\xi} \right)^\top \rvtilder_i}{d \bar{\rvo}_{\xi}} - \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle d\xi} \right| \\
    &\le \int_0^1{ \left| \left\langle \frac{d \rvpi_i\left( \bar{\rvo}_{\xi} \right)^\top \rvtilder_i}{d \bar{\rvo}_{\xi}} - \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| d\xi} \\
    &\le \int_0^1{ \left\| \frac{d \rvpi_i\left( \bar{\rvo}_{\xi} \right)^\top \rvtilder_i}{d \bar{\rvo}_{\xi}} - \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo} \right\|_2 \cdot \left\| \rvo^\prime - \rvo \right\|_2 d\xi} \\
    &= \int_0^1{ \left\| \gH\left( \rvpi_i\left( \bar{\rvo}_{\xi} \right)\right) \rvtilder_i - \gH\left( \rvpi_i\left( \rvo \right)\right) \rvtilder_i \right\|_2 \cdot \left\| \rvo^\prime - \rvo \right\|_2 d\xi} \\
    &= \int_0^1{ \left\| \int_0^1{\left\langle \frac{d \gH \left( \rvpi_i\left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right) \right) \rvtilder_i }{d \left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right)}, \bar{\rvo}_{\xi} - \rvo \right\rangle d\mu} \right\|_2 \cdot \left\| \rvo^\prime - \rvo \right\|_2 d\xi} \\
    &\le \int_0^1{  \int_0^1{ \left\| \left\langle \frac{d \gH \left( \rvpi_i\left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right) \right) \rvtilder_i }{d \left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right)}, \bar{\rvo}_{\xi} - \rvo \right\rangle \right\|_2 d\mu} \cdot \left\| \rvo^\prime - \rvo \right\|_2 d\xi} \\
    &\le \int_0^1{  \int_0^1{ \left\| \frac{d \gH \left( \rvpi_i\left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right) \right) \rvtilder_i }{d \left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right)} \right\|_2 \cdot \left\| \bar{\rvo}_{\xi} - \rvo \right\|_2 d\mu} \cdot \left\| \rvo^\prime - \rvo \right\|_2 d\xi} \\
    &= \int_0^1{  \int_0^1{ \left\| \frac{d \gH \left( \rvpi_i\left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right) \right) \rvtilder_i }{d \left( \rvo + \mu\left( \bar{\rvo}_{\xi} - \rvo \right) \right)} \right\|_2 d\mu} \cdot \xi \cdot \left\| \rvo^\prime - \rvo \right\|_2^2 d\xi} \\
    &\le \int_0^1{  \int_0^1{ 4 d\mu} \cdot \xi \cdot \left\| \rvo^\prime - \rvo \right\|_2^2 d\xi} \\
    &= 2 \cdot \left\| \rvo^\prime - \rvo \right\|_2^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}[Lemma 2 in the paper]
\label{lem:gradient_coupling}
	Define the pseudo policy gradient as,
\begin{equation*}
\begin{split}
\small
	\frac{d \tilde{\ell}(t)}{d \rmW(t)} \triangleq \tilde{\rmD} \rmA^\top \left[ \Delta\left( \rvpi_i\left(\rmW(t)\right) \right) - \rvpi_i\left(\rmW(t)\right) \rvpi_i\left(\rmW(t)\right)^\top \right] \hat{\rvtilder}_i \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD}_{k,k} \triangleq \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\}$, $\forall k \in [m]$, is a diagonal matrix. Note the true policy gradient is, $\frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \triangleq $
\begin{equation*}
\begin{split}
\small
    \rmD(t) \rmA^\top \left[ \Delta\left( \rvpi_i\left(\rmW(t)\right) \right) - \rvpi_i\left(\rmW(t)\right) \rvpi_i\left(\rmW(t)\right)^\top \right] \hat{\rvtilder}_i \rvs^\top.
\end{split}
\end{equation*}
where $\rmD(t)_{k,k} \triangleq \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\}$, $\forall k \in [m]$. For any $\tau > 0$, with probability at least $1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi}\sigma}$, $\forall t \in O\left(\frac{\tau}{\eta}\right)$, $\forall r \in [m]$,
\begin{equation*}
	\frac{d\tilde{\ell}(t)}{d \rvw_r(t)} = \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW(t)$.
\end{lem}
\begin{proof}
The $r$th row of the pseudo policy gradient as,
\begin{equation*}
	\frac{d \tilde{\ell}}{d \rvw_r(t)} \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r}  \cdot v_{k^\prime,k,i}(t) } \right) \cdot \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
While the $r$th row of the true policy gradient is,
\begin{equation*}
	\frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)} = \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
The true policy gradient norm is upper bounded by the surrogate expected loss,
\begin{equation*}
\begin{split}
	\left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)} \right\|_2 &\le \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left| \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \right| \cdot \left\| \rvs_i \right\|_2 }} \\
	&\le \frac{2}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t)  } \cdot \left\| \rvs_i \right\|_2  }} \\
	&\le \frac{2}{n} \cdot \sum\limits_{i=1}^{n}{ \rvpi_{i}\left(\rmW(t)\right)^\top \hat{\rvtilder}_i }.
\end{split}
\end{equation*}
After $t$ updates, the distance between $\rvw_r(t)$ and $\rvw_r(0)$ is also upper bounded,
\begin{equation*}
\begin{split}
	\left\| \rvw_r(t) - \rvw_r(0) \right\|_2 &\le \eta \cdot \sum\limits_{s=0}^{t-1}{\left\| \frac{d \rvpi_i\left( \rmW(s) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(s)} \right\|_2} \\
	&\le \frac{2\eta}{n} \cdot \sum\limits_{s=0}^{t-1}{ \sum\limits_{i=1}^{n}{ \rvpi_{i}\left(\rmW(t)\right)^\top \hat{\rvtilder}_i } } \\
	&\le 2 \eta t .
\end{split}
\end{equation*}
Since $\rvw_r(0)^\top \rvs_i \sim \gN(0, \sigma^2)$, $\pr\left(\left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) \le  \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$,
\begin{equation*}
\begin{split}
	\pr\left(\forall i \in [n], \left| \rvw_r(0)^\top \rvs_i \right| > \tau\right) &= 1 - \pr\left(\exists i \in [n], \left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) \\
	&\ge 1 - \sum\limits_{i=1}^{n}{ \pr\left(\left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) } \\
	&\ge 1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi}\sigma},
\end{split}
\end{equation*}
Condition on the above event happens, let $t \le \frac{\tau}{ 2 \eta }$, $\forall i \in [n]$,
\begin{equation*}
\begin{split}
	\left| \left( \rvw_r(t) - \rvw_r(0) \right)^\top \rvs_i \right| &\le \left\| \rvw_r(t) - \rvw_r(0) \right\|_2 \cdot \left\| \rvs_i \right\|_2 \\
	&\le 2 \eta t \\
	&\le \tau < \left| \rvw_r(0)^\top \rvs_i \right|,
\end{split}
\end{equation*}
which implies if $\left| \rvw_r(0)^\top \rvs_i \right| > \tau$, $\forall i \in [n]$, then,
\begin{equation*}
\begin{split}
	\sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} &= \sI\left\{ \rvw_r(0)^\top \rvs_i  + \left( \rvw_r(t) - \rvw_r(0) \right)^\top \rvs_i > 0 \right\} \\
	&= \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\}. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:inner_product_logit_difference_logit_derivative}
    $\rmW(t+1) = \rmW(t) - \eta \cdot \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)}$. Denote $\rvo_i(t+1)$ and $\rvo_i(t)$ as the logit vectors of $\rmW(t)$ and $\rmW(t+1)$ at state $\rvs_i$, respectively. $\forall t \in O\left( \frac{\tau}{\eta}\right)$,
\begin{equation*}
\begin{split}
    \left\langle \frac{d \rvpi_i\left( \rvo_i(t) \right)^\top \hat{\rvtilder}_i}{d \rvo_i(t)}, \rvo_i(t+1) - \rvo_i(t) \right\rangle = - \eta \left\| \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
    First, the logit derivative is,
\begin{equation*}
\begin{split}
    \frac{d \rvpi_i\left( \rvo_i(t) \right)^\top \hat{\rvtilder}_i}{d \rvo_i(t)} &= \left[\Delta\left( \rvpi_i\left( \rvo_i(t) \right) \right) - \rvpi_i\left( \rvo_i(t) \right) \rvpi_i\left( \rvo_i(t) \right)^\top \right] \hat{\rvtilder}_i \\
    &= \left[\Delta\left( \rvpi_i\left( \rmW(t) \right) \right) - \rvpi_i\left( \rmW(t) \right) \rvpi_i\left( \rmW(t) \right)^\top \right] \hat{\rvtilder}_i.
\end{split}
\end{equation*}
Second, the logit difference of policy gradient update is,
\begin{equation*}
\begin{split}
    \rvo_i(t+1) - \rvo_i(t) &= \rmA \left[ \sigma\left(\rmW(t+1) \rvs_i \right) - \sigma\left( \rmW(t) \rvs_i \right)\right] \\
    &= \rmA \rmD(t) \left[ \rmW(t+1) - \rmW(t) \right] \rvs_i \\
    &= - \eta \cdot \rmA \rmD(t) \left[ \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right] \rvs_i.
\end{split}
\end{equation*}
Combining the two above results,
\begin{equation*}
\begin{split}
    &\left\langle \frac{d \rvpi_i\left( \rvo_i(t) \right)^\top \hat{\rvtilder}_i}{d \rvo_i(t)}, \rvo_i(t+1) - \rvo_i(t) \right\rangle \\
    &= - \eta \cdot \hat{\rvtilder}_i^\top \left[\Delta\left( \rvpi_i\left( \rmW(t) \right) \right) - \rvpi_i\left( \rmW(t) \right) \rvpi_i\left( \rmW(t) \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right] \rvs_i \\
    &= - \eta \cdot \trace \left\{ \hat{\rvtilder}_i^\top \left[\Delta\left( \rvpi_i\left( \rmW(t) \right) \right) - \rvpi_i\left( \rmW(t) \right) \rvpi_i\left( \rmW(t) \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right] \rvs_i \right\} \\
    &= - \eta \cdot \trace \left\{ \rvs_i \hat{\rvtilder}_i^\top \left[\Delta\left( \rvpi_i\left( \rmW(t) \right) \right) - \rvpi_i\left( \rmW(t) \right) \rvpi_i\left( \rmW(t) \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right]  \right\} \\
    &= - \eta \left\langle \rmD(t) \rmA^\top \left[\Delta\left( \rvpi_i\left( \rmW(t) \right) \right) - \rvpi_i\left( \rmW(t) \right) \rvpi_i\left( \rmW(t) \right)^\top \right] \hat{\rvtilder}_i \rvs_i^\top, \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\rangle \\
    &= - \eta \cdot \left\| \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:logit_upper_bound_parameter}
Denote $k_*^i \triangleq \argmax\limits_{k \in [h]}\left\{ \left| o_{i,k}^\prime - o_{i,k} \right| \right\}$. Let $\rvo_i$ and $\rvo_i^\prime$ be the logit vector of $\rmW$ and $\rmW^\prime$ at state $\rvs_i$, respectively, i.e.,
\begin{equation*}
\begin{split}
    &o_{i,k} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left( u_{i,r} \right)} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left(\rvw_r^\top \rvs_i \right)} \\
    &o_{i,k}^\prime = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left( u_{i,r}^\prime \right)} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left({\rvw_r^\prime}^\top \rvs_i \right)},
\end{split}
\end{equation*}
$\forall k \in [h]$. Then $\forall i \in [n]$,
\begin{equation*}
\begin{split}
    \left\| \rvo_i^\prime - \rvo_i \right\|_2^2 \le h m \left\| \rmW^\prime - \rmW \right\|_F^2.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
By the definition of the logit, the $1$-Lipschitzness of ReLU, and the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \left\| \rvo_i^\prime - \rvo_i \right\|_2^2 &\le \sum\limits_{k = 1}^{h}{ \left\| \rvo_i^\prime - \rvo_i \right\|_\infty^2} \\
    &= h \cdot \left| o_{i,k_*^i}^\prime - o_{i,k_*^i} \right|^2 \\
    &= h \cdot \left| \sum\limits_{r=1}^{m}{ a_{k_*^i,r} \cdot \left( \sigma\left({\rvw_r^\prime}^\top \rvs_i \right) - \sigma\left(\rvw_r^\top \rvs_i \right) \right)} \right|^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left| a_{k_*^i,r} \right| \cdot \left| \sigma\left({\rvw_r^\prime}^\top \rvs_i\right) - \sigma\left({\rvw_r}^\top \rvs_i\right) \right|  } \right)^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left| \left({\rvw_r^\prime} - \rvw_r \right)^\top \rvs_i\right|  } \right)^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left\| {\rvw_r^\prime} - \rvw_r \right\|_2  } \right)^2 \\
    &\le h \cdot \left( \sqrt{m} \cdot \left\| \rmW^\prime - \rmW \right\|_F \right)^2 \\
    &= h m \cdot \left\| \rmW^\prime - \rmW \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}[Lemma 3 in the paper]
\label{lem:parameter_smoothness}
    $\rmW(t+1) = \rmW(t) - \eta \cdot \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)}$, $\forall t \in O\left( \frac{\tau}{\eta}\right)$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
\small
    \rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i \le \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i - \left( \eta - 2 h m \eta^2 \right) \cdot \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}
\begin{proof}
    By \cref{lem:logit_smoothness}, \cref{lem:inner_product_logit_difference_logit_derivative}, and \cref{lem:logit_upper_bound_parameter},
\begin{equation*}
\begin{split}
    &\rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i - \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i = \rvpi_i\left( \rvo_i(t+1) \right)^\top \hat{\rvtilder}_i - \rvpi_i\left( \rvo_i(t) \right)^\top \hat{\rvtilder}_i \\
    &\le \left\langle \frac{d \rvpi_i\left( \rvo_i(t) \right)^\top \rvtilder_i}{d \rvo_i(t)}, \rvo_i(t+1) - \rvo_i(t) \right\rangle + 2 \left\| \rvo_i(t+1) - \rvo_i(t)  \right\|_2^2 \\
    &\le - \eta \cdot \left\| \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2 + 2 h m \left\| \rmW(t+1) - \rmW(t) \right\|_F^2 \\
    &= - \eta \cdot \left\| \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2 + 2 h m \eta^2 \left\| \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}[Lemma 5 in the paper]
\label{lem:gradient_lower_bound}
	Denote $i^*(t) \triangleq \argmax\limits_{i \in [n]}\left\{\rvpi_i\left( \rmW(t)\right)^\top \hat{\rvtilder}_i\right\}$, $\hat{k}_i^*(t) \triangleq \argmax\limits_{k \in [h]}\left\{ \hat{r}_{i^*(t),k} \right\}$, i.e., the optimal action at state $\rvs_{i^*(t)}$ using the estimated reward $\rvhat_i$. If $\pi\left(\rmW(t)\right)_{i^*(t), \hat{k}_i^*(t)} > c_t > 0$, then with probability $\Omega\left( \frac{\delta}{n} \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 \ge \Omega\left( \frac{\delta}{n^2} \right) \cdot c_t \cdot \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_{i}.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
	 For conciseness, we denote $i^*(t)$ as $i^*$, and $\hat{k}_i^*(t)$ as $k^*$. Rewrite $\frac{d\tilde{\ell}}{d \rvw_r(t)} = \sum\limits_{k^
	\prime=1}^{h}{ a_{k^\prime,r} \cdot \rvp_{k^\prime, r} }$, where $\rvp_{k^\prime, r} \in \sR^d$ is defined as, 
\begin{equation*}
	\rvp_{k^\prime, r} \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^\prime,k,i}(t) \cdot \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
By the randomness of $a_{k^\prime,r}$,
\begin{equation}
\label{eq:gradient_p_lowerbound}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 &\ge \left| a_{k^*,r} \right| \cdot \left\| \rvp_{k^*, r}\right\|_2 \\
	&\ge \left\| \rvp_{k^*, r}\right\|_2,
\end{split}
\end{equation}
with probability at least $\frac{1}{2} - \frac{1}{\sqrt{2\pi}}$. We can decompose $\rvw_r(0)$ as,
\begin{equation}
\label{eq:decompose_w}
\begin{split}
	\rvw_r(0) &= \rvw_r^\prime(0) + \rvw_r^{\prime\prime}(0) \\
	&\triangleq \left( \rmI - \rvs_{i^*}\rvs_{i^*}^\top \right) \rvw_r(0) +  \rvs_{i^*}\rvs_{i^*}^\top \rvw_r(0),
\end{split}
\end{equation}
where $\rvw_r^\prime(0) \perp \rvw_r^{\prime\prime}(0)$. Define $h_{k^*,r}$ as follows,
\begin{equation*}
\begin{split}
	h_{k^*,r} &\triangleq \rvw_r(0)^\top \rvp_{k^*, r} \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i ) \right] } }.
\end{split}
\end{equation*}
And decompose $h_{k^*,r}$ into two parts, i.e., $i^*$ and $[n] \setminus \left\{ i^* \right\}$,
\begin{equation}
\label{eq:decompose_h}
\begin{split}
	h_{k^*,r} &= \frac{1}{n} \cdot \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i^* ) \right] } \\
	&+ \frac{1}{n} \cdot \sum\limits_{i \not= i^*}{\sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i ) \right] } }.
\end{split}
\end{equation}
Consider the following events,
\begin{equation*}
\begin{split}
	\gE_1 &: \left| \rvw_r(0)^\top \rvs_{i^*} \right| \le \tau, \ \text{given } \tau \in \left( 0, \sigma \right], \\
	\gE_2 &: \left| \rvw_r^\prime(0)^\top \rvs_i \right| > \tau, \ \forall i \not= i^*.
\end{split}
\end{equation*}
For $\gE_1$, note $\rvw_r(0)^\top \rvs_{i^*} \sim \gN(0, \sigma^2)$,
\begin{equation*}
\begin{split}
	\pr\left(\gE_1\right) &= \frac{1}{\sqrt{2\pi}\sigma} \int_{-\tau}^{\tau}{\exp\left\{ - \frac{x^2}{2\sigma^2} \right\} dx} \\
	&\ge \frac{1}{\sqrt{2\pi}\sigma} \int_{-\tau}^{\tau}{ \left( 1  - \frac{x^2}{2\sigma^2} \right) dx} \\
	&= \frac{\sqrt{2}}{\sqrt{\pi}\sigma} \cdot \left( \tau - \frac{\tau^3}{6\sigma^2}\right) \\
	&\ge \frac{5\sqrt{2}}{6\sqrt{\pi}} \cdot \frac{\tau}{\sigma} \in \Omega\left( \frac{\tau}{\sigma} \right).
\end{split}
\end{equation*}
For $\gE_2$, according to \cref{eq:decompose_w}, $\rvw_r^\prime(0)^\top \rvs_i \sim \gN\left(0, \left(1 - \left(\rvs_{i^*}^\top \rvs_{i} \right)^2 \right)\sigma^2 \right)$, $\forall i \not= i^*$, and $\rvw_r^\prime(0)^\top \rvs_i$ is independent with $\rvw_r^{\prime\prime}(0)^\top \rvs_{i}$.
\begin{equation*}
\begin{split}
	\pr\left(\gE_2\right) &= 1 - \pr\left( \exists i \not= i^*, \ \left| \rvw_r^\prime(0)^\top \rvs_i \right| \le \tau \right) \\
	&\ge 1 - \sum\limits_{i \not= i^*}{ \pr\left(\left| \rvw_r^\prime(0)^\top \rvs_i \right| \le \tau \right) } \\
	&\ge 1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi\left( 1 - \left(\rvs_{i^*}^\top \rvs_{i} \right)^2 \right) }\sigma} \\
	&\ge 1 - \frac{2n\tau}{\sqrt{\pi}\delta\sigma} \quad \left( \left\| \rvs_{i} -  \rvs_{j} \right\|_2 \ge \delta, \ \forall i \not= j \right).
\end{split}
\end{equation*}
Let $\tau = \frac{\delta\sigma}{2n}$, then $\pr\left(\gE_2\right) \ge 1 - \frac{1}{\sqrt{\pi}}$. Therefore, $\pr\left( \gE_1 \land \gE_2 \right) \in \Omega\left( \frac{\tau}{\sigma} \right)$. Now condition on $\gE_1 \land \gE_2$ happens, we have, $\forall i \not= i^*$,
\begin{equation*}
\begin{split}
	\sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} &= \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i + \rvw_r^{\prime\prime}(0)^\top \rvs_{i} > 0 \right\} \\
	&= \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i > 0 \right\},
\end{split}
\end{equation*}
since $\left| \rvw_r^{\prime\prime}(0)^\top \rvs_{i} \right| = \left| \rvw_r(0)^\top \rvs_{i^*} \rvs_{i^*}^\top \rvs_{i} \right| \le \left| \rvw_r(0)^\top \rvs_{i^*} \right| \le \tau < \left| \rvw_r^\prime(0)^\top \rvs_i  \right|$. Fix $\rvw_r^\prime(0)^\top \rvs_i$, and randomly generate $x \triangleq \rvw_r(0)^\top \rvs_{i^*}$, rewrite \cref{eq:decompose_h},
\begin{equation}
\label{eq:h_alpha}
\begin{split}
	h_{k^*,r} &= \frac{1}{n} \cdot \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \cdot \sigma(x) \right] } \\
	&+ \frac{1}{n} \cdot \sum\limits_{i \not= i^*}{\sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot ( x \cdot \rvs_{i^*}^\top \rvs_{i} + \rvw_r^\prime(0)^\top \rvs_i ) \cdot \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i > 0 \right\}  \right] } }.
\end{split}
\end{equation}
Note the first part of $h_{k^*,r}$ is a convex function of $x$, and the second part is linear. For the first term in \cref{eq:h_alpha}, we have,
\begin{equation*}
\begin{split}
	\sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } &= \underbrace{\hat{\tilde{r}}_{i^*,k^*} \cdot \pi_{i^*,k^*}(t) \cdot v_{k^*,k^*,i^*}(t)}_{\hat{\tilde{r}}_{i^*,k^*} = \hat{r}_{i^*,k^*} -  \hat{r}_{i^*,k^*} = 0} + \sum\limits_{k\not=k^*}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \sum\limits_{k\not=k^*}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \rvpi_{i^*}\left(\rmW(t)\right)^\top \rvtilder_{i^*}.
\end{split}
\end{equation*}
By assumption, if $\pi_{i^*,k^*}(t) > c_t > 0$, then ,
\begin{equation*}
\begin{split}
	\left| \partial{h_{k^*,r}}_{\max}{(0)} - \partial{h_{k^*,r}}_{\min}{(0)} \right| &= \frac{1}{n} \cdot \left| \sum\limits_{k=1}^{h}{ \left[ \hat{\tilde{r}}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } \right| \\
	&= \frac{1}{n} \cdot \pi_{i^*,k^*}(t) \cdot \rvpi_{i^*}\left(\rmW(t)\right)^\top \rvtilder_{i^*} \\
	&\ge \frac{c_t}{n} \cdot \rvpi_{i^*}\left(\rmW(t)\right)^\top \rvtilder_{i^*}.
\end{split}
\end{equation*}
where $\partial{h_{k^*,r}}_{\max}{(0)} = \max\left\{ \partial{h_{k^*,r}}{(0)} \right\}$, $\partial{h_{k^*,r}}_{\min}{(0)} = \min\left\{ \partial{h_{k^*,r}}{(0)} \right\}$ are the maximum and minimum of $\partial{h_{k^*,r}}{(0)}$, i.e., the subdifferential of $h_{k^*,r}$ at $0$. Therefore, by \cref{lem:non_smooth_convex},
\begin{equation*}
\begin{split}
	\probability\limits_{x \sim  \gU[-\tau, \tau]}\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \ell(\rvpi(t)) \right\} > \frac{1}{8}.
\end{split}
\end{equation*}
Therefore, we have,
\begin{equation}
\label{eq:h_regret_lower_bound}
\begin{split}
	\probability\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \ell(\rvpi(t)) \right\} &\ge \probability\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \ell(\rvpi(t)) \middle| \gE_1 \land \gE_2 \right\} \cdot \probability\left\{ \gE_1 \land \gE_2 \right\} \\
	&\ge \frac{1}{8} \cdot \frac{5\sqrt{2}}{6\sqrt{\pi}} \cdot \frac{\tau}{\sigma} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \\
	&= \frac{5\sqrt{2}}{48\sqrt{\pi}} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \cdot \frac{\tau}{\sigma}.
\end{split}
\end{equation}
Finally, $h_{k^*,r} \sim \gN\left( 0, \left\| \rvp_{k^*, r} \right\|_2^2 \cdot \sigma^2 \right)$, with probability at least $1 - \frac{1}{e^2}$,
\begin{equation}
\label{eq:p_h_lower_bound}
\begin{split}
	h_{k^*,r} < 2 \cdot \left\| \rvp_{k^*, r} \right\|_2 \cdot \sigma.
\end{split}
\end{equation}
Combining \cref{eq:gradient_p_lowerbound}, \cref{eq:h_regret_lower_bound} and \cref{eq:p_h_lower_bound}, we have,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 &\ge \left\| \rvp_{k^*, r}\right\|_2 \\
	&> \frac{h_{k^*,r}}{2\sigma} \\
	&\ge \frac{c\tau}{16\sigma n} \cdot \ell(\rvpi(t)),
\end{split}
\end{equation*}
with probability at least $\left( \frac{1}{2} - \frac{1}{\sqrt{2\pi}} \right) \cdot \left( 1 - \frac{1}{e^2} \right) \cdot \frac{5\sqrt{2}}{48\sqrt{\pi}} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \cdot \frac{\tau}{\sigma}$. Plugging in $\tau = \frac{\delta\sigma}{2n}$, we obtain the result.
\end{proof}

\begin{lem}
\label{lem:non_smooth_convex}
	Let $\phi(x) : \sR \to \sR$ be a convex function non-smooth at $0$. Define,
\begin{equation*}
\begin{split}
	\partial\phi_{\max}{(0)} = \max\left\{ \partial\phi(0) \right\}, \quad \partial\phi_{\min}{(0)} = \min\left\{ \partial\phi(0) \right\},
\end{split}
\end{equation*}	
where $\partial\phi(0)$ is the subdifferential of $\phi$ at $0$. Then we have,
\begin{equation*}
\begin{split}
	\probability\limits_{x \sim \gU[-\tau, \tau]}\left\{ \left| \phi(x) \ge \frac{ \left( \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)} \right) \tau}{8} \right|\right\} \ge \frac{1}{8}.
\end{split}
\end{equation*}	
\end{lem}
\begin{proof}
	Denote $\rho = \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)}$, we have $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$ or $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$. If $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$, then $-\phi$ will satisfy the first case. So we prove for $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$. Define $\hat{\phi}(x) = \phi(x) - \phi(0)$. $\forall x > 0$,
\begin{equation*}
\begin{split}
	\hat{\phi}(x) \ge \hat{\phi}(0) + \frac{\rho}{2} \cdot \left( x - 0\right) = \frac{\rho x}{2} \ge 0.
\end{split}
\end{equation*}
If $\phi(0) \ge 0$, then $\forall x \in \left[\frac{\tau}{2}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| = \left| \hat{\phi}(x) + \phi(0) \right| = \hat{\phi}(x) + \phi(0) \ge \frac{\rho x}{2} \ge \frac{\rho \tau}{4}.
\end{split}
\end{equation*}
If $\phi(0) < 0$, then $\phi(x_0) = 0$ for some $x_0 > 0$. If $x_0 < \frac{\tau}{2}$, $\forall x \in \left[x_0 + \frac{\tau}{4}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| =  \phi(x) \ge \phi(x_0) + \phi^\prime(x_0) \cdot \left( x - x_0 \right) \ge \frac{\rho \tau}{8}.
\end{split}
\end{equation*}
If $x_0 \ge \frac{\tau}{2}$, then $\phi(0) \le - \frac{\rho}{2} \cdot x_0 \le - \frac{\rho\tau}{4}$, $\forall x \in \left[0, x_0 - \frac{\tau}{4} \right]$,
\begin{equation*}
	\left| \phi(x) \right| \ge \left| \frac{-\phi(0)}{x_0} \left( x - x_0 \right) \right| = \frac{-\phi(0)}{x_0} \left( x_0 - x \right) \ge \frac{\rho\tau}{8}. \qedhere
\end{equation*}
\end{proof}

\section{Policy Gradient}

\subsection{One Form}

The gradient with respect to $\rvw_r(t)$ is,
\begin{equation}
\label{eq:gradient_form_one}
\begin{split}
	\frac{d\ell}{d \rvw_r(t)} &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{d \pi_{i,k}(t)}{d \rvw_r(t)} \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{d}{d \rvw_r(t)} \left\{ \frac{\exp\left\{ o_{i,k}(t) \right\}}{\sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}}} \right\} \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{ \exp\left\{ o_{i,k}(t) \right\} \cdot a_{k,r} \cdot \rvs_i \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right) }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 } \right] } \\
	&\qquad - \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{ \exp\left\{ o_{i,k}(t) \right\} \cdot \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \cdot a_{k^\prime,r} \cdot \rvs_i \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 } \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k,r} \cdot \pi_{i,k^\prime}(t) } - \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r} \cdot \pi_{i,k^\prime}(t) } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r}  \cdot v_{k^\prime,k,i}(t) } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } },
\end{split}
\end{equation}
where $v_{k^\prime,k,i}(t)$ is defined as,
\begin{equation*}
	v_{k^\prime,k,i}(t) = \begin{cases}
    1 - \pi_{i,k^\prime}(t), & \text{if $k^\prime = k$}, \\
    - \pi_{i,k^\prime}(t), & \text{otherwise}.
  \end{cases}
\end{equation*}

\subsubsection{Another Form}

First note that, $\forall k \in [h]$,
\begin{equation*}
\begin{split}
    \frac{d \rvpi_i(t)^\top \rvtilder_i}{d o_{i,k}(t)} &= \sum\limits_{k^\prime = 1}^{h}{ \tilde{r}_{i, k^\prime} \cdot \frac{d \pi_{i,k^\prime}(t) }{d o_{i,k}(t)}} \\
    &= \sum\limits_{k^\prime = 1}^{h}{ \tilde{r}_{i, k^\prime} \cdot \frac{d }{d o_{i,k}(t)}} \left\{ \frac{\exp\left\{ o_{i,k^\prime}(t) \right\}}{\sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}}} \right\} \\
    &= \tilde{r}_{i, k} \cdot \frac{ \exp\left\{ o_{i,k}(t) \right\} \cdot \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right) - \exp\left\{ o_{i,k}(t) \right\} \cdot \exp\left\{ o_{i,k}(t) \right\} }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 } \\
    &\quad - \sum\limits_{k^\prime \not= k}{ \tilde{r}_{i, k^\prime} \cdot \frac{\exp\left\{ o_{i,k^\prime}(t) \right\} \cdot \exp\left\{ o_{i,k}(t) \right\} }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 }} \\
    &= \tilde{r}_{i, k} \cdot \left( \pi_{i,k}(t) - \pi_{i,k}(t)^2 \right) - \sum\limits_{k^\prime \not= k}{ \tilde{r}_{i, k^\prime} \cdot \pi_{i,k^\prime}(t) \cdot \pi_{i,k}(t) } \\
    &= \pi_{i,k}(t) \cdot \left( \tilde{r}_{i, k} - \rvpi_i(t)^\top \rvtilder_i \right).
\end{split}
\end{equation*}

\noindent For the vector derivative, we have,
\begin{equation*}
\begin{split}
    \frac{d \rvpi_i(t)^\top \rvtilder_i}{d \rvo_{i}(t)} &= \left( \Delta\left( \rvpi_i(t) \right) - \rvpi_i(t) \rvpi_i(t)^\top \right) \rvtilder_i .
\end{split}
\end{equation*}

\noindent Also note that, $\forall r \in [m]$, $\forall k \in [h]$,
\begin{equation*}
\begin{split}
    \frac{d o_{i,k}(t)}{d \rvw_r(t)} &= a_{k,r} \cdot \frac{d u_{i,r}(t)}{d \rvw_r(t)} \\
    &= a_{k,r} \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i.
\end{split}
\end{equation*}

\noindent Then the gradient with respect to $\rvw_r(t)$ is,
\begin{equation}
\label{eq:gradient_form_two}
\begin{split}
    \frac{d\ell}{d \rvw_r(t)} &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \frac{d \rvpi_i(t)^\top \rvtilder_i}{d \rvw_r(t)} } \\
    &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \frac{d \rvpi_i(t)^\top \rvtilder_i}{d o_{i,k}(t)}\cdot \frac{d o_{i,k}(t)}{d \rvw_r(t)} } } \\
    &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \pi_{i,k}(t) \cdot \left( \tilde{r}_{i, k} - \rvpi_i(t)^\top \rvtilder_i \right) \cdot a_{k,r} \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i  \right] }  } \\
    &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \left[ \rvtilder_i^\top \left( \Delta\left( \rvpi_i(t) \right) - \rvpi_i(t) \rvpi_i(t)^\top \right) \rva_{\cdot, r} \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i  \right] },
\end{split}
\end{equation}
where $\rva_{\cdot, r} \in \sR^h$ refers to $\rva_{\cdot, r} \triangleq \left( a_{1,r}, a_{2,r}, \dots, a_{h,r} \right)^\top$. It is easy to check that \cref{eq:gradient_form_one} and \cref{eq:gradient_form_two} are equivalent.

\subsection{Other}

\begin{lem}[Chernoff]
    Let $X_1, X_2, \dots, X_n \sim B(1, p)$ be independent Bernoulli random variables. Define $X = \sum\limits_{i=1}^{n}{ X_i  }$. Denote $\mu \triangleq \sE\left[ X \right]$. $\forall \alpha \in [0,1]$,
\begin{equation*}
    \pr\left\{ X \le (1 - \alpha) \mu \right\} \le \exp\left\{ - \frac{\alpha^2 \mu}{2} \right\}.
\end{equation*}
\end{lem}

\subsection{Online Update}

\begin{lem}
    Suppose $\left| f_t\left(\rvx\right) \right| \le 1$, and $\left| f_t\left(\rvx\right) - f_{t-1}\left(\rvx\right) \right| \le \frac{a}{\sqrt{t}}$, $\forall \rvx$, $\forall t > 0$. And $f_t$ is $b$-smooth, $\forall t > 0$, i.e., $\forall \rvx, \ \rvx^\prime$,
\begin{equation*}
\begin{split}
    f_t\left(\rvx^\prime\right) \le f_t\left(\rvx\right) + \left\langle \nabla f_t\left(\rvx^\prime\right) , \rvx^\prime - \rvx \right\rangle +  \frac{b}{2} \left\| \rvx^\prime - \rvx \right\|_2^2.
\end{split}
\end{equation*}
And $\left\| \nabla f_t\left(\rvx\right) \right\|_2 \ge f_t\left(\rvx\right) - f_t\left(\rvx_*\right)$, $\forall \rvx$, $\forall t > 0$. Then the regret of the  online gradient descent satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{\left[ f_t\left(\rvx_t\right) - f_t\left( \rvx_* \right) \right]} \le 2 \sqrt{ab} \cdot T^{\frac{3}{4}}.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
$\rvx_{t} = \rvx_{t-1} - \eta \cdot \nabla f_{t-1}\left(\rvx_{t-1}\right)$, where $\eta = \frac{1}{b}$. Denote $\delta_t \triangleq f_t\left(\rvx_t\right) - f_t\left( \rvx_* \right)$.
\begin{equation*}
\begin{split}
    \delta_t &= f_t\left(\rvx_t\right) - f_{t-1}\left(\rvx_t\right) + f_{t-1}\left(\rvx_t\right) - f_{t-1}\left(\rvx_{t-1}\right) + f_{t-1}\left(\rvx_{t-1}\right) - f_{t-1}\left( \rvx_* \right) + f_{t-1}\left( \rvx_* \right) -  f_t\left( \rvx_* \right) \\
    &\le \frac{a}{\sqrt{t}} + \left\langle \nabla f_{t-1}\left(\rvx_{t-1}\right),  \rvx_t - \rvx_{t-1} \right\rangle + \frac{b}{2}\left\| \rvx_t - \rvx_{t-1} \right\|_2^2 + \delta_{t-1} + \frac{a}{\sqrt{t}} \\
    &= \frac{2a}{\sqrt{t}} - \eta \left\| \nabla f_{t-1}\left(\rvx_{t-1}\right) \right\|_2^2 + \frac{b \eta^2}{2} \left\| \nabla f_{t-1}\left(\rvx_{t-1}\right)  \right\|_2^2 + \delta_{t-1} \\
    &= \frac{2a}{\sqrt{t}} - \frac{1}{2b} \left\| \nabla f_{t-1}\left(\rvx_{t-1}\right) \right\|_2^2 + \delta_{t-1} \\
    &\le \delta_{t-1} - \frac{1}{2b} \cdot \delta_{t-1}^2 + \frac{2a}{\sqrt{t}}.
\end{split}
\end{equation*}
Rearranging and summing up from $1$ to $T$,
\begin{equation*}
\begin{split}
    \frac{1}{2b} \cdot \sum\limits_{t=1}^{T}{ \delta_{t-1}^2} &\le \sum\limits_{t=1}^{T}{\left[ \delta_{t-1} - \delta_{t} + f_t\left(\rvx_t\right) - f_{t-1}\left(\rvx_t\right) + f_{t-1}\left( \rvx_* \right) -  f_t\left( \rvx_* \right) \right] } \\
    &= \delta_0 - \delta_{T} + f_0\left( \rvx_* \right) - f_{T}\left( \rvx_* \right) + \sum\limits_{t=1}^{T}{ \left[ f_t\left(\rvx_t\right) - f_{t-1}\left(\rvx_t\right) \right] } \\
    &\le 6 + a \sqrt{T}.
\end{split}
\end{equation*}
By the the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \frac{\sum\limits_{t=1}^{T}{ \delta_{t-1}}}{T} \le \sqrt{\frac{\sum\limits_{t=1}^{T}{ \delta_{t-1}^2}}{T}},
\end{split}
\end{equation*}
the regret is upper bounded by,
\begin{equation*}
\begin{split}
    \sum\limits_{t=1}^{T}{ \delta_{t-1}} &\le \sqrt{ T \cdot \sum\limits_{t=1}^{T}{ \delta_{t-1}^2} } \\
    &\le \sqrt{ T \cdot 2b \cdot \left( 6 + a \sqrt{T} \right) } \\
    &\le 2 \sqrt{ab} \cdot T^{\frac{3}{4}}. \qedhere
\end{split}
\end{equation*}
\end{proof}

\subsection{Sublinear Regret, Online Update, Pull all Arms}

Suppose $\rvr_i \in \left[ 0, 1\right]^h$ is the true mean loss vector at state $\rvs_i$. Define $\rvpi_i^* \triangleq \argmin\limits_{\rvpi \in \Delta^{h-1}}{ \left\{ \rvpi^\top \rvr_i \right\} }$ as the optimal policy at state $\rvs_i$. Consider the following online gradient descent method, where at each time step $t$, the agent can take every action and observe all the rewards.

\begin{algorithm}[h]
   \caption{NN Policy Gradient, Online Update, Pull all Arms}
\label{alg:policy_gradient_online_pull_all_arms}
\begin{algorithmic}
   \STATE {\bfseries Input:} State feature $\rvs_i$, learning rate $\eta > 0$, $\beta > 0$.
   \STATE Initialize $\rvw_r(0) \sim \gN\left( 0, \sigma^2 \cdot \rmI \right)$, $\forall r \in [m]$. \STATE Initialize $\rva_k \sim \gU\left\{-1, +1\right\}$, $\forall k \in [h]$.
   \STATE Initialize $\hat{\rvr}_{i}(0) \gets \rvzero$, $n_{i} \gets 0$.
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \STATE $\rmW(t+1) \leftarrow \rmW(t) - \eta \cdot \frac{d \rvpi_{i}\left(\rmW(t)\right)^\top \hat{\rvr}_i(t)}{d \rmW(t)}$.
   \STATE Take action $k$. Observe reward $R_{i,k}(t) \in \sR$, $\forall k \in [h]$.
   \STATE $n_i \gets n_i + 1$.
   \STATE $\hat{r}_{i, k}(t+1) \gets \left( 1 - \frac{1}{n_i} \right) \cdot \hat{r}_{i,k}(t) + \frac{1}{n_i} \cdot R_{i,k}(t)$, $\forall k \in [h]$.
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{thm}
\label{thm:sublinear_online_update_pull_all_arms}
    The expected regret of \cref{alg:policy_gradient_online_pull_all_arms} satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \expectation\limits_{A_t \sim \rvpi_i\left(\rmW(t)\right)} \left[ r_{i, A_t} \right] } - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \rvr_i } \le \frac{10 \sqrt{h}}{c}\sqrt{T \log{T}}.
\end{split}
\end{equation*}
\end{thm}
\begin{proof}
    If the agent uses $\rvpi_{i}\left(\rmW(t)\right)$ as the strategy at step $t$, then the expected regret can be expressed by the cumulative expected loss of $\rvpi_{i}\left(\rmW(t)\right)$, i.e.,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \expectation\limits_{A_t \sim \rvpi_i\left(\rmW(t)\right)} \left[ r_{i, A_t} \right] } - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \rvr_i } = \sum\limits_{t=0}^{T-1}{\rvpi_i\left(\rmW(t)\right)^\top \rvr_i} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \rvr_i }.
\end{split}
\end{equation*}
    Denote ${\rvpi_i^*(t)} \triangleq \argmin\limits_{\rvpi \in \Delta^{h-1}}{ \left\{ \rvpi^\top \hat{\rvr}_i(t) \right\} }$. Decompose the expected regret as follows,
\begin{equation}
\label{eq:cumulative_expected_loss_decomposition}
\begin{split}
    &\sum\limits_{t=0}^{T-1}{\rvpi_i\left(\rmW(t)\right)^\top \rvr_i} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \rvr_i } \\
    &= \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t)} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \hat{\rvr}_i(t)} \\
    &\qquad + \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \left( \rvr_i - \hat{\rvr}_i(t) \right) } + \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \left( \hat{\rvr}_i(t) - \rvr_i \right) } \\
    &= \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t)} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t)} \\
    &\qquad + \sum\limits_{t=0}^{T-1}{ \left( {\rvpi_i^*(t)} - {\rvpi_i^*} \right)^\top \hat{\rvr}_i(t) } \qquad \left( \le 0, \text{ by the optimality of } {\rvpi_i^*(t)} \right) \\
    &\qquad + \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \left( \rvr_i - \hat{\rvr}_i(t) \right) } + \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \left( \hat{\rvr}_i(t) - \rvr_i \right) } \\
    &\le \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t)} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t)} \\
    &\qquad + \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \left( \rvr_i - \hat{\rvr}_i(t) \right) } + \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \left( \hat{\rvr}_i(t) - \rvr_i \right) }.
\end{split}
\end{equation}
Denote $\omega_t \triangleq \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t) - {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t)$ as the dynamic regret at step $t$.
\begin{equation}
\label{eq:dynamic_regret_decomposition}
\begin{split}
    \omega_t &= \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t) - \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t-1) \\
    &\quad + \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t-1) - \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) \\
    &\quad + \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t-1)}^\top \hat{\rvr}_i(t-1) \quad \left( = \omega_{t-1} \right) \\
    &\quad + {\rvpi_i^*(t-1)}^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t-1) \\
    &\quad + {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t).
\end{split}
\end{equation}
Firstly, $\rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t) - \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t-1)$ can be upper bounded as,
\begin{equation}
\label{eq:first_term_upper_bound}
\begin{split}
    &\rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t) - \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t-1) \\
    &\qquad = \sum\limits_{k=1}^{h}{ \pi_{i,k}\left(\rmW(t)\right) \left( \hat{r}_{i,k}(t) - \hat{r}_{i,k}(t-1) \right) } \\
    &\qquad = \sum\limits_{k=1}^{h}{ \pi_{i,k}\left(\rmW(t)\right) \left( \frac{1}{t} \sum\limits_{s=0}^{t-1}{R_{i,k}(s)} - \frac{1}{t-1} \sum\limits_{s=0}^{t-2}{R_{i,k}(s)} \right) } \\
    &\qquad = \frac{1}{t} \sum\limits_{k=1}^{h}{ \pi_{i,k}\left(\rmW(t)\right) \left( R_{i,k}(t-1) - \hat{r}_{i,k}(t-1) \right)} \\
    &\qquad \le \left| \frac{1}{t} \sum\limits_{k=1}^{h}{ \pi_{i,k}\left(\rmW(t)\right) \left( R_{i,k}(t-1) - \hat{r}_{i,k}(t-1) \right)} \right| \\
    &\qquad \le \frac{1}{t} \cdot \left\| \rvpi_{i}\left(\rmW(t)\right) \right\|_1 \cdot \max\limits_{k \in [h]}{\left\{ \left| R_{i,k}(t-1) - \hat{r}_{i,k}(t-1) \right| \right\}} \\
    &\qquad \le \frac{1}{t}.
\end{split}
\end{equation}
Next, by \cref{lem:parameter_smoothness}, \cref{lem:gradient_lower_bound}, and let $\eta = \frac{1}{4 h m}$ (parameter smoothness + gradient lower bound),
\begin{equation}
\label{eq:second_term_upper_bound}
\begin{split}
    &\rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t-1) - \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) \\
    &\qquad \le - \left( \eta - 2 h m \eta^2 \right) \cdot \left\| \frac{d \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) }{d \rmW(t-1)} \right\|_F^2 \\
    &\qquad = - \frac{1}{8 h m} \cdot \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) }{d \rvw_r(t-1)} \right\|_2^2 } \\
    &\qquad \le - \frac{\delta^2 c^2}{8 h n^4} \left( \rvpi_i\left(\rmW(t-1)\right)^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t-1)}^\top \hat{\rvr}_i(t-1) \right)^2 \\
    &\qquad = - \frac{\delta^2 c^2}{8 h n^4} \cdot \omega_{t-1}^2.
\end{split}    
\end{equation}
By the optimality of ${\rvpi_i^*(t-1)}$,
\begin{equation}
\label{eq:fourth_term_upper_bound}
\begin{split}
    {\rvpi_i^*(t-1)}^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t-1) \le 0.
\end{split}
\end{equation}
Using similar arguments as \cref{eq:first_term_upper_bound},
\begin{equation}
\label{eq:fifth_term_upper_bound}
\begin{split}
    {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t-1) - {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t) \le \frac{1}{t}.
\end{split}
\end{equation}
Plugging \cref{eq:first_term_upper_bound}, \cref{eq:second_term_upper_bound}, \cref{eq:fourth_term_upper_bound} and \cref{eq:fifth_term_upper_bound} into \cref{eq:dynamic_regret_decomposition},
\begin{equation*}
\begin{split}
    \omega_t &\le \frac{1}{t} - \frac{\delta^2 c^2}{8 h n^4} \cdot \omega_{t-1}^2 + \omega_{t-1} + \frac{1}{t} \\
    &= - \frac{\delta^2 c^2}{8 h n^4} \cdot \omega_{t-1}^2 + \omega_{t-1} + \frac{2}{t}.
\end{split}
\end{equation*}
Rearranging and summing up from $t = 1$ to $T$,
\begin{equation*}
\begin{split}
    &\sum\limits_{t=1}^{T}{ \omega_{t-1}^2 } \le \frac{8 h n^4}{\delta^2 c^2} \sum\limits_{t=1}^{T}{\left[ \omega_{t-1} - \omega_t + \frac{2}{t} \right]} = \frac{8 h n^4}{\delta^2 c^2} \left[ \omega_0 - \omega_T + \sum\limits_{t=1}^{T}{\frac{2}{t}} \right] \\
    &\qquad \le \frac{8 h n^4}{\delta^2 c^2} \left( \omega_0 - \omega_T + 2 + 2 \log{T} \right) \\
    &\qquad \le \frac{8 h n^4}{\delta^2 c^2} \left( \omega_0 + 2 + 2 \log{T} \right) \\
    &\qquad = \frac{8 h n^4}{\delta^2 c^2} \left( \rvpi_i\left(\rmW(0)\right)^\top \hat{\rvr}_i(0) - {\rvpi_i^*(0)}^\top \hat{\rvr}_i(0) + 2 + 2 \log{T} \right) \\
    &\qquad = \frac{16 h n^4}{\delta^2 c^2} \left( 1 + \log{T} \right) \qquad \left( \hat{\rvr}_i(0) = \rvzero \right) \\
    &\qquad \le \frac{25 h n^4}{\delta^2 c^2} \cdot \log{T}.
\end{split}
\end{equation*}
By the the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation}
\label{eq:dynamic_regret_upper_bound}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvr}_i(t)} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*(t)}^\top \hat{\rvr}_i(t)} = \sum\limits_{t=0}^{T-1}{ \omega_t } &\le \sqrt{ T \cdot \sum\limits_{t=0}^{T-1}{ \omega_{t}^2 } } \\
    &\le \frac{5 n^2 \sqrt{h}}{\delta c}\sqrt{T \log{T}}.
\end{split}
\end{equation}
Finally, the last two terms in \cref{eq:cumulative_expected_loss_decomposition} are upper bounded as,
\begin{equation}
\label{eq:last_two_terms_upper_bound}
\begin{split}
    &\sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \left( \rvr_i - \hat{\rvr}_i(t) \right) } + \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \left( \hat{\rvr}_i(t) - \rvr_i \right) } \\
    &\qquad \le \sum\limits_{t=0}^{T-1}{ \left\| \rvpi_i\left(\rmW(t)\right)\right\|_1 \cdot \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty } + \sum\limits_{t=0}^{T-1}{ \left\| {\rvpi_i^*} \right\|_1 \cdot \left\| \hat{\rvr}_i(t) - \rvr_i \right\|_\infty } \\
    &\qquad = 2 \sum\limits_{t=0}^{T-1}{ \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty }.
\end{split}
\end{equation}
Denote $\hat{k}_i(t) \triangleq \argmax\limits_{k \in [h]}{ \left\{ \left| r_{i,k} - \hat{r}_{i,k}(t) \right| \right\}}$.
\begin{equation*}
\begin{split}
    \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty &= \left| r_{i, \hat{k}_i(t)} - \hat{r}_{i,\hat{k}_i(t)}(t) \right| \\
    &= \left| r_{i, \hat{k}_i(t)} - \frac{1}{t} \sum\limits_{s=0}^{t-1}{R_{i, \hat{k}_i(t)}(s)} \right|.
\end{split}
\end{equation*}
By Hoeffding's inequality, $\forall t > 0$,
\begin{equation*}
\begin{split}
    \probability{\left\{ \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty > \frac{\sqrt{\log{T}}}{\sqrt{t}} \right\}} \le 2 \exp{\left\{ - 2 t \cdot \frac{\log{T}}{t}\right\}} = \frac{2}{T^2}.
\end{split}
\end{equation*}
Therefore $\left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty$ is upper bounded by,
\begin{equation}
\label{eq:empirical_estimation_upper_bound}
\begin{split}
    &\left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty \\
    & \le \probability{\left\{ \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty > \frac{\sqrt{\log{T}}}{\sqrt{t}} \right\}} \cdot 1 + \probability{\left\{ \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty \le \frac{\sqrt{\log{T}}}{\sqrt{t}} \right\}} \cdot \frac{\sqrt{\log{T}}}{\sqrt{t}} \\
    &\le \frac{2}{T^2} + \frac{\sqrt{\log{T}}}{\sqrt{t}}.
\end{split}
\end{equation}
According to \cref{eq:last_two_terms_upper_bound} and \cref{eq:empirical_estimation_upper_bound},
\begin{equation}
\label{eq:last_two_terms_sublinear}
\begin{split}
    &\sum\limits_{t=0}^{T-1}{ \rvpi_i\left(\rmW(t)\right)^\top \left( \rvr_i - \hat{\rvr}_i(t) \right) } + \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \left( \hat{\rvr}_i(t) - \rvr_i \right) } \\
    &\qquad \le 2 \left\| \rvr_i - \hat{\rvr}_i(0) \right\|_\infty + 2 \sum\limits_{t=1}^{T-1}{ \left\| \rvr_i - \hat{\rvr}_i(t) \right\|_\infty } \\
    &\qquad \le 2 + \frac{4}{T} + 4 \sqrt{T \log{T}} \\
    &\qquad \le 5 \sqrt{T \log{T}}.
\end{split}
\end{equation}
Combining \cref{eq:cumulative_expected_loss_decomposition}, \cref{eq:dynamic_regret_upper_bound}, and \cref{eq:last_two_terms_sublinear},
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{\rvpi_i\left(\rmW(t)\right)^\top \rvr_i} - \sum\limits_{t=0}^{T-1}{ {\rvpi_i^*}^\top \rvr_i } &\le \frac{5 n^2 \sqrt{h}}{\delta c}\sqrt{T \log{T}} + 5 \sqrt{T \log{T}} \\
    &\le \frac{10 n^2 \sqrt{h}}{\delta c}\sqrt{T \log{T}}. \qedhere
\end{split}
\end{equation*}
\end{proof}

\end{document}
