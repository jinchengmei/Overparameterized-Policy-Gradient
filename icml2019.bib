@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8168--8177},
  year={2018}
}

@article{du2018gradientA,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{du2018gradientB,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}

@article{allen2018convergenceA,
  title={On the convergence rate of training recurrent neural networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1810.12065},
  year={2018}
}

@article{allen2018convergenceB,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{hoffer2018fix,
  title={Fix your classifier: the marginal value of training the last weight layer},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1801.04540},
  year={2018}
}