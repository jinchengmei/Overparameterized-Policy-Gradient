\documentclass[10pt]{article}
\usepackage[usenames]{color} %used for font color
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm}
\usepackage[utf8]{inputenc} %useful to type directly diacritic characters
\usepackage[capitalize]{cleveref}
\crefname{prop}{Proposition}{Propositions}
\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}

\def\rva{{\mathbf{a}}}
\def\rvo{{\mathbf{o}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvu{{\mathbf{u}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvg{{\mathbf{g}}}
\def\rvone{{\mathbf{1}}}
\def\rvzero{{\mathbf{0}}}
\def\rvtilder{{\tilde{\mathbf{r}}}}

\def\rvp{{\mathbf{p}}}

\def\pr{{\text{Pr}}}
\def\r{{\text{R}}}

\def\regret{{\text{Regret}}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{defi}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{remk}{Remark}


\def\rvpi{{\boldsymbol{\pi}}}

\def\rmI{{\mathbf{I}}}
\def\rmX{{\mathbf{X}}}

\def\sR{{\mathbb{R}}}
\def\sI{{\mathbb{I}}}

\def\gN{{\mathcal{N}}}
\def\gE{{\mathcal{E}}}
\def\gU{{\mathcal{U}}}


\title{Overparameterized Policy Gradient}
\author{}
\date{December 2018}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\probability}{Pr}

\begin{document}

\maketitle

\section{Notations}

Bold letters refer to vectors, and non-bold letters refer to scalars. For example, $u_{i,r} \in \sR$ is the $r$th component of vector $\rvu_i \in \sR^m$. $n$ is the total number of data points, while $m$ is the total number of nodes in each hidden layer. $h$ is the total number of arms/trajectories of each bandit/starting state.

\begin{itemize}
	\item $\rvs_i \in \sR^d$, $i \in [n]$ is a state/bandit.
	\item $\rvw_r \in \sR^d$, $r \in [m]$ is a weight vector in the first hidden layer.
	\item $u_{i,r} \triangleq \sigma(\rvw_r^\top \rvs_i)$, where $\sigma(\cdot) \triangleq \max\left\{ \cdot, 0 \right\}$.
	\item $\rva_k \in \sR^m$, $k \in [h]$ is a weight vector in the second layer. $\rva_k \sim \gN(0, \rmI)$.
	\item $o_{i,k} \triangleq \sum\limits_{r=1}^{m}{a_{k,r} \cdot u_{i,r}}$ is the logit of the $k$th arm for state $\rvs_i$.
	\item $\pi_{i,k} \triangleq \frac{\exp\left\{ o_{i,k} \right\}}{\sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime} \right\}}}$ is the  probability for choosing arm $k$ in bandit $i$.
	\item $\rvr_i \in \sR^h$ is the reward vector for bandit $i$.
	\item $\r_i^{\max} \triangleq \max\limits_{k}\left\{ r_{i,k} \right\}$ is the maximum reward of bandit $i$.
	\item The loss function $\ell \triangleq \regret(\rvpi) \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \regret(\rvpi_i) } \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \left( \r_i^{\max} - \rvpi_i^\top \rvr_i \right) }$ is the regret. Regret minimization is equivalent with maximizing the reward $\frac{1}{n} \sum\limits_{i=1}^{n}{\rvpi_i^\top \rvr_i}$ from optimization and learning perspectives.
\end{itemize}

Denote $\rvtilder_{i} \triangleq \r_i^{\max} \cdot \rvone -  \rvr_{i} \ge \rvzero$. The regret at time step $t$ is,
\begin{equation*}
\begin{split}
	\ell(t) \triangleq \regret(\rvpi(t)) &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \left( \r_i^{\max} - \rvpi_{i}(t)^\top \rvr_i \right) } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \rvpi_{i}(t)^\top \left( \r_i^{\max} \cdot \rvone - \rvr_{i} \right) } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \rvpi_{i}(t)^\top \rvtilder_{i} }.
\end{split}
\end{equation*}

\section{Results}

We analyze the policy gradient method,
\begin{itemize}
	\item Initialize $\rvw_r(0) \sim \gN(0, \sigma^2 \cdot \rmI)$, $\forall r \in [m]$.
	\item Update $\rvw_r(t+1) = \rvw_r(t) - \eta \cdot \frac{d\ell}{d \rvw_r(t)}$.
\end{itemize}

\noindent There are two data assumptions,
\begin{itemize}
	\item $\left\| \rvs_{i} -  \rvs_{j} \right\|_2 \ge \delta > 0 , \ \forall i \not= j$. There is no duplicated data.
	\item $\left\| \rvs_{i} \right\|_2 = 1, \ \forall i \in [n]$.
\end{itemize}

\noindent The main result is,
\begin{thm}
\end{thm}
\begin{proof}
\end{proof}

\subsection{Policy Gradient}

The gradient with respect to $\rvw_r$ is,
\begin{equation*}
\begin{split}
	\frac{d\ell}{d \rvw_r(t)} &= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{d \pi_{i,k}(t)}{d \rvw_r(t)} \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{d}{d \rvw_r(t)} \left\{ \frac{\exp\left\{ o_{i,k}(t) \right\}}{\sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}}} \right\} \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{ \exp\left\{ o_{i,k}(t) \right\} \cdot a_{k,r} \cdot \rvs_i \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right) }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 } \right] } \\
	&\qquad - \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}\left[ \tilde{r}_{i,k} \cdot \frac{ \exp\left\{ o_{i,k}(t) \right\} \cdot \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \cdot a_{k^\prime,r} \cdot \rvs_i \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} }{ \left( \sum\limits_{k^\prime = 1}^{h}{\exp\left\{ o_{i,k^\prime}(t) \right\}} \right)^2 } \right] } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k,r} \cdot \pi_{i,k^\prime}(t) } - \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r} \cdot \pi_{i,k^\prime}(t) } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } } \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r}  \cdot v_{k^\prime,k,i}(t) } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } },
\end{split}
\end{equation*}
where $v_{k^\prime,k,i}(t)$ is defined as,
\begin{equation*}
	v_{k^\prime,k,i}(t) = \begin{cases}
    1 - \pi_{i,k^\prime}(t), & \text{if $k^\prime = k$}, \\
    - \pi_{i,k^\prime}(t), & \text{otherwise}.
  \end{cases}
\end{equation*}

\subsection{Gradient Norm Upper Bound}

\begin{lem}
	Define the pseudo gradient as,
\begin{equation*}
	\frac{d \tilde{\ell}}{d \rvw_r(t)} \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime = 1}^{h}{ a_{k^\prime,r}  \cdot v_{k^\prime,k,i}(t) } \right) \cdot \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
	For any $\tau > 0$, with probability at least $\left( 1 - \frac{2}{e^4} \right) \cdot \left( 1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi}\sigma} \right)$, $\forall t \in \mathcal{O}\left(\frac{\tau}{\eta}\right)$,
\begin{equation}
	\frac{d\ell}{d \rvw_r(t)} = \frac{d \tilde{\ell}}{d \rvw_r(t)}.
\end{equation}
\end{lem}
\begin{proof}
The policy gradient is,
\begin{equation*}
	\frac{d\ell}{d \rvw_r(t)} = \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \left( \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
Since $a_{k,r}, a_{k^\prime,r} \sim \gN(0, 1)$, we have $a_{k,r} - a_{k^\prime,r} \sim \gN(0, 2)$, therefore $\left| a_{k,r} - a_{k^\prime,r} \right| \le 4$ with  probability at least $1 - \frac{2}{e^4} $. Then we have,
\begin{equation*}
\begin{split}
	\left\| \frac{d\ell}{d \rvw_r(t)} \right\|_2 &\le \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left| \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \cdot \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} \right| \cdot \left\| \rvs_i \right\|_2 }} \\
	&\le \frac{4}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \sum\limits_{k^\prime \not= k}^{h}{ \pi_{i,k^\prime}(t)  } \cdot \left\| \rvs_i \right\|_2  }} \\
	&\le \frac{4}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) }} \\
	&= \frac{4}{n} \cdot \sum\limits_{i=1}^{n}{ \regret(\rvpi_{i}(t)) } \\
	&= 4 \cdot \regret(\rvpi(t)).
\end{split}
\end{equation*}
Denote $\text{R}_{\max} \triangleq \max\limits_{s \ge 0}\left\{\regret\left( \rvpi(s) \right)\right\}$. $\rvw_r(t)$ will be closed to $\rvw_r(0)$,
\begin{equation*}
\begin{split}
	\left\| \rvw_r(t) - \rvw_r(0) \right\|_2 &\le \eta \cdot \sum\limits_{s=0}^{t-1}{\left\| \frac{d\ell}{d \rvw_r(s)} \right\|_2} \\
	&\le 4 \eta \cdot \sum\limits_{s=0}^{t-1}{ \regret(\rvpi(s)) } \\
	&\le 4 \eta \cdot \text{R}_{\max} \cdot t .
\end{split}
\end{equation*}
Since $\rvw_r(0)^\top \rvs_i \sim \gN(0, \sigma^2)$, $\pr\left(\left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) \le  \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$,
\begin{equation*}
\begin{split}
	\pr\left(\forall i \in [n], \left| \rvw_r(0)^\top \rvs_i \right| > \tau\right) &= 1 - \pr\left(\exists i \in [n], \left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) \\
	&\ge 1 - \sum\limits_{i=1}^{n}{ \pr\left(\left| \rvw_r(0)^\top \rvs_i \right| \le \tau\right) } \\
	&\ge 1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi}\sigma},
\end{split}
\end{equation*}
Condition on the above event happens, let $t \le \frac{\tau}{ 4 \eta \cdot \text{R}_{\max} }$, $\forall i \in [n]$,
\begin{equation*}
\begin{split}
	\left| \left( \rvw_r(t) - \rvw_r(0) \right)^\top \rvs_i \right| &\le \left\| \rvw_r(t) - \rvw_r(0) \right\|_2 \cdot \left\| \rvs_i \right\|_2 \\
	&\le 4 \eta \cdot \text{R}_{\max} \cdot t \\
	&\le \tau < \left| \rvw_r(0)^\top \rvs_i \right|,
\end{split}
\end{equation*}
which implies if $\left| \rvw_r(0)^\top \rvs_i \right| > \tau$, $\forall i \in [n]$, then,
\begin{equation*}
\begin{split}
	\sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} &= \sI\left\{ \rvw_r(0)^\top \rvs_i  + \left( \rvw_r(t) - \rvw_r(0) \right)^\top \rvs_i > 0 \right\} \\
	&= \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\}. \qedhere
\end{split}
\end{equation*}
\end{proof}

\subsection{Gradient Norm Lower Bound}

\begin{lem}
	Denote $i^*(t) \triangleq \argmax\limits_{i \in [n]}\left\{\regret(\rvpi_i(t))\right\}$, $k^*(t) \triangleq \argmax\limits_{k \in [h]}\left\{ r_{i^*(t),k} \right\} = \r_{i^*(t)}^{\max}$, i.e., the optimal arm of state $i^*(t)$. If $\pi_{i^*(t), k^*(t)}(t) > c > 0$, where $c$ is a constant, then with probability $\Omega\left( \frac{\delta}{n} \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 \ge \Omega\left( \frac{c\delta}{n^2} \right) \cdot \regret\left( \rvpi_{i^*(t)}(t) \right) \ge \Omega\left( \frac{c\delta}{n^2} \right) \cdot \regret\left( \rvpi(t) \right).
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
	 For conciseness, we denote $i^*(t)$ as $i^*$, and $k^*(t)$ as $k^*$. Rewrite $\frac{d\tilde{\ell}}{d \rvw_r(t)} = \sum\limits_{k^
	\prime=1}^{h}{ a_{k^\prime,r} \cdot \rvp_{k^\prime, r} }$, where $\rvp_{k^\prime, r} \in \sR^d$ is defined as, 
\begin{equation*}
	\rvp_{k^\prime, r} \triangleq \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^\prime,k,i}(t) \cdot \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} \cdot \rvs_i \right] } }.
\end{equation*}
By the randomness of $a_{k^\prime,r}$,
\begin{equation}
\label{eq:gradient_p_lowerbound}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 &\ge \left| a_{k^*,r} \right| \cdot \left\| \rvp_{k^*, r}\right\|_2 \\
	&\ge \left\| \rvp_{k^*, r}\right\|_2,
\end{split}
\end{equation}
with probability at least $\frac{1}{2} - \frac{1}{\sqrt{2\pi}}$. We can decompose $\rvw_r(0)$ as,
\begin{equation}
\label{eq:decompose_w}
\begin{split}
	\rvw_r(0) &= \rvw_r^\prime(0) + \rvw_r^{\prime\prime}(0) \\
	&\triangleq \left( \rmI - \rvs_{i^*}\rvs_{i^*}^\top \right) \rvw_r(0) +  \rvs_{i^*}\rvs_{i^*}^\top \rvw_r(0),
\end{split}
\end{equation}
where $\rvw_r^\prime(0) \perp \rvw_r^{\prime\prime}(0)$. Define $h_{k^*,r}$ as follows,
\begin{equation*}
\begin{split}
	h_{k^*,r} &\triangleq \rvw_r(0)^\top \rvp_{k^*, r} \\
	&= \frac{1}{n} \cdot \sum\limits_{i=1}^{n}{ \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i ) \right] } }.
\end{split}
\end{equation*}
Now decompose $h_{k^*,r}$ as follows,
\begin{equation}
\label{eq:decompose_h}
\begin{split}
	h_{k^*,r} &= \frac{1}{n} \cdot \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i^* ) \right] } \\
	&+ \frac{1}{n} \cdot \sum\limits_{i \not= i^*}{\sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot \sigma( \rvw_r(0)^\top \rvs_i ) \right] } }.
\end{split}
\end{equation}
Consider the following events,
\begin{equation*}
\begin{split}
	\gE_1 &: \left| \rvw_r(0)^\top \rvs_{i^*} \right| \le \tau, \ \text{given } \tau \in \left( 0, \sigma \right], \\
	\gE_2 &: \left| \rvw_r^\prime(0)^\top \rvs_i \right| > \tau, \ \forall i \not= i^*.
\end{split}
\end{equation*}
For $\gE_1$, note $\rvw_r(0)^\top \rvs_{i^*} \sim \gN(0, \sigma^2)$,
\begin{equation*}
\begin{split}
	\pr\left(\gE_1\right) &= \frac{1}{\sqrt{2\pi}\sigma} \int_{-\tau}^{\tau}{\exp\left\{ - \frac{x^2}{2\sigma^2} \right\} dx} \\
	&\ge \frac{1}{\sqrt{2\pi}\sigma} \int_{-\tau}^{\tau}{ \left( 1  - \frac{x^2}{2\sigma^2} \right) dx} \\
	&= \frac{\sqrt{2}}{\sqrt{\pi}\sigma} \cdot \left( \tau - \frac{\tau^3}{6\sigma^2}\right) \\
	&\ge \frac{5\sqrt{2}}{6\sqrt{\pi}} \cdot \frac{\tau}{\sigma} \in \Omega\left( \frac{\tau}{\sigma} \right).
\end{split}
\end{equation*}
For $\gE_2$, according to \cref{eq:decompose_w}, $\rvw_r^\prime(0)^\top \rvs_i \sim \gN\left(0, \left(1 - \left(\rvs_{i^*}^\top \rvs_{i} \right)^2 \right)\sigma^2 \right)$, $\forall i \not= i^*$, and $\rvw_r^\prime(0)^\top \rvs_i$ is independent with $\rvw_r^{\prime\prime}(0)^\top \rvs_{i}$.
\begin{equation*}
\begin{split}
	\pr\left(\gE_2\right) &= 1 - \pr\left( \exists i \not= i^*, \ \left| \rvw_r^\prime(0)^\top \rvs_i \right| \le \tau \right) \\
	&\ge 1 - \sum\limits_{i \not= i^*}{ \pr\left(\left| \rvw_r^\prime(0)^\top \rvs_i \right| \le \tau \right) } \\
	&\ge 1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi\left( 1 - \left(\rvs_{i^*}^\top \rvs_{i} \right)^2 \right) }\sigma} \\
	&\ge 1 - \frac{2n\tau}{\sqrt{\pi}\delta\sigma} \quad \left( \left\| \rvs_{i} -  \rvs_{j} \right\|_2 \ge \delta, \ \forall i \not= j \right).
\end{split}
\end{equation*}
Let $\tau = \frac{\delta\sigma}{2n}$, then $\pr\left(\gE_2\right) \ge 1 - \frac{1}{\sqrt{\pi}}$. Therefore, $\pr\left( \gE_1 \land \gE_2 \right) \in \Omega\left( \frac{\tau}{\sigma} \right)$. Now condition on $\gE_1 \land \gE_2$ happens, we have, $\forall i \not= i^*$,
\begin{equation*}
\begin{split}
	\sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\} &= \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i + \rvw_r^{\prime\prime}(0)^\top \rvs_{i} > 0 \right\} \\
	&= \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i > 0 \right\},
\end{split}
\end{equation*}
since $\left| \rvw_r^{\prime\prime}(0)^\top \rvs_{i} \right| = \left| \rvw_r(0)^\top \rvs_{i^*} \rvs_{i^*}^\top \rvs_{i} \right| \le \left| \rvw_r(0)^\top \rvs_{i^*} \right| \le \tau < \left| \rvw_r^\prime(0)^\top \rvs_i  \right|$. Fix $\rvw_r^\prime(0)^\top \rvs_i$, and randomly generate $x \triangleq \rvw_r(0)^\top \rvs_{i^*}$, rewrite \cref{eq:decompose_h},
\begin{equation}
\label{eq:h_alpha}
\begin{split}
	h_{k^*,r} &= \frac{1}{n} \cdot \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \cdot \sigma(x) \right] } \\
	&+ \frac{1}{n} \cdot \sum\limits_{i \not= i^*}{\sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i,k} \cdot \pi_{i,k}(t) \cdot v_{k^*,k,i}(t) \cdot ( x \cdot \rvs_{i^*}^\top \rvs_{i} + \rvw_r^\prime(0)^\top \rvs_i ) \cdot \sI\left\{ \rvw_r^\prime(0)^\top \rvs_i > 0 \right\}  \right] } }.
\end{split}
\end{equation}
Note the first part of $h_{k^*,r}$ is a convex function of $x$, and the second part is linear. For the first term in \cref{eq:h_alpha}, we have,
\begin{equation*}
\begin{split}
	\sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } &= \underbrace{\tilde{r}_{i^*,k^*} \cdot \pi_{i^*,k^*}(t) \cdot v_{k^*,k^*,i^*}(t)}_{\tilde{r}_{i^*,k^*} = \r_{i^*}^{\max} -  r_{i^*,k^*} = 0} + \sum\limits_{k\not=k^*}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \sum\limits_{k\not=k^*}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \right] } \\
	&= - \pi_{i^*,k^*}(t) \cdot \regret(\rvpi_{i^*}(t)).
\end{split}
\end{equation*}
By assumption, if $\pi_{i^*,k^*}(t) > c > 0$, then ,
\begin{equation*}
\begin{split}
	\left| \partial{h_{k^*,r}}_{\max}{(0)} - \partial{h_{k^*,r}}_{\min}{(0)} \right| &= \frac{1}{n} \cdot \left| \sum\limits_{k=1}^{h}{ \left[ \tilde{r}_{i^*,k} \cdot \pi_{i^*,k}(t) \cdot v_{k^*,k,i^*}(t) \right] } \right| \\
	&= \frac{1}{n} \cdot \pi_{i^*,k^*}(t) \cdot \regret(\rvpi_{i^*}(t)) \\
	&\ge \frac{c}{n} \cdot \regret(\rvpi(t)).
\end{split}
\end{equation*}
where $\partial{h_{k^*,r}}_{\max}{(0)} = \max\left\{ \partial{h_{k^*,r}}{(0)} \right\}$, $\partial{h_{k^*,r}}_{\min}{(0)} = \min\left\{ \partial{h_{k^*,r}}{(0)} \right\}$ are the maximum and minimum of $\partial{h_{k^*,r}}{(0)}$, i.e., the subdifferential of $h_{k^*,r}$ at $0$. Therefore, by \cref{lem:non_smooth_convex},
\begin{equation*}
\begin{split}
	\probability\limits_{x \sim  \gU[-\tau, \tau]}\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \regret(\rvpi(t)) \right\} > \frac{1}{8}.
\end{split}
\end{equation*}
Therefore, we have,
\begin{equation}
\label{eq:h_regret_lower_bound}
\begin{split}
	\probability\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \regret(\rvpi(t)) \right\} &\ge \probability\left\{ h_{k^*,r} \ge \frac{c\tau}{8n} \cdot \regret(\rvpi(t)) \middle| \gE_1 \land \gE_2 \right\} \cdot \probability\left\{ \gE_1 \land \gE_2 \right\} \\
	&\ge \frac{1}{8} \cdot \frac{5\sqrt{2}}{6\sqrt{\pi}} \cdot \frac{\tau}{\sigma} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \\
	&= \frac{5\sqrt{2}}{48\sqrt{\pi}} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \cdot \frac{\tau}{\sigma}.
\end{split}
\end{equation}
Finally, $h_{k^*,r} \sim \gN\left( 0, \left\| \rvp_{k^*, r} \right\|_2^2 \cdot \sigma^2 \right)$, with probability at least $1 - \frac{1}{e^2}$,
\begin{equation}
\label{eq:p_h_lower_bound}
\begin{split}
	h_{k^*,r} < 2 \cdot \left\| \rvp_{k^*, r} \right\|_2 \cdot \sigma.
\end{split}
\end{equation}
Combining \cref{eq:gradient_p_lowerbound}, \cref{eq:h_regret_lower_bound} and \cref{eq:p_h_lower_bound}, we have,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}}{d \rvw_r(t)} \right\|_2 &\ge \left\| \rvp_{k^*, r}\right\|_2 \\
	&> \frac{h_{k^*,r}}{2\sigma} \\
	&\ge \frac{c\tau}{16\sigma n} \cdot \regret(\rvpi(t)),
\end{split}
\end{equation*}
with probability at least $\left( \frac{1}{2} - \frac{1}{\sqrt{2\pi}} \right) \cdot \left( 1 - \frac{1}{e^2} \right) \cdot \frac{5\sqrt{2}}{48\sqrt{\pi}} \cdot \left( 1 - \frac{1}{\sqrt{\pi}} \right) \cdot \frac{\tau}{\sigma}$. Plugging in $\tau = \frac{\delta\sigma}{2n}$, we obtain the result.
\end{proof}

\begin{lem}
\label{lem:non_smooth_convex}
	Let $\phi(x) : \sR \to \sR$ be a convex function non-smooth at $0$. Define,
\begin{equation*}
\begin{split}
	\partial\phi_{\max}{(0)} = \max\left\{ \partial\phi(0) \right\}, \quad \partial\phi_{\min}{(0)} = \min\left\{ \partial\phi(0) \right\},
\end{split}
\end{equation*}	
where $\partial\phi(0)$ is the subdifferential of $\phi$ at $0$. Then we have,
\begin{equation*}
\begin{split}
	\probability\limits_{x \sim \gU[-\tau, \tau]}\left\{ \left| \phi(x) \ge \frac{ \left( \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)} \right) \tau}{8} \right|\right\} \ge \frac{1}{8}.
\end{split}
\end{equation*}	
\end{lem}
\begin{proof}
	Denote $\rho = \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)}$, we have $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$ or $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$. If $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$, then $-\phi$ will satisfy the first case. So we prove for $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$. Define $\hat{\phi}(x) = \phi(x) - \phi(0)$. $\forall x > 0$,
\begin{equation*}
\begin{split}
	\hat{\phi}(x) \ge \hat{\phi}(0) + \frac{\rho}{2} \cdot \left( x - 0\right) = \frac{\rho x}{2} \ge 0.
\end{split}
\end{equation*}
If $\phi(0) \ge 0$, then $\forall x \in \left[\frac{\tau}{2}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| = \left| \hat{\phi}(x) + \phi(0) \right| = \hat{\phi}(x) + \phi(0) \ge \frac{\rho x}{2} \ge \frac{\rho \tau}{4}.
\end{split}
\end{equation*}
If $\phi(0) < 0$, then $\phi(x_0) = 0$ for some $x_0 > 0$. If $x_0 < \frac{\tau}{2}$, $\forall x \in \left[x_0 + \frac{\tau}{4}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| =  \phi(x) \ge \phi(x_0) + \phi^\prime(x_0) \cdot \left( x - x_0 \right) \ge \frac{\rho \tau}{8}.
\end{split}
\end{equation*}
If $x_0 \ge \frac{\tau}{2}$, then $\phi(0) \le - \frac{\rho}{2} \cdot x_0 \le - \frac{\rho\tau}{4}$, $\forall x \in \left[0, x_0 - \frac{\tau}{4} \right]$,
\begin{equation*}
	\left| \phi(x) \right| \ge \left| \frac{-\phi(0)}{x_0} \left( x - x_0 \right) \right| = \frac{-\phi(0)}{x_0} \left( x_0 - x \right) \ge \frac{\rho\tau}{8}. \qedhere
\end{equation*}
\end{proof}

\if0
Chernoff bound. Let $X_1, X_2, \dots, X_n \sim B(1, p)$ be independent Bernoulli random variables. Define $X = \sum\limits_{i=1}^{n}{  }$
\fi

\begin{equation*}
\begin{split}
\end{split}
\end{equation*}


\end{document}
