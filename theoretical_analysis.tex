\section{Theoretical Analysis}
\label{sec:theoretical_analysis}

\subsection{Main Results}
\label{subsec:main_results}

We first present the main results in the bandit settings, as shown in \cref{thm:main_result}, and then show the detailed proof ideas and  intuitions.

\begin{thm}
\label{thm:main_result}
    Given a two layer NN policy $\rvpi_i$, with number of parameters $m \in \Theta\left( \frac{n^{10}}{c^4 \delta^4 \varepsilon^2} \right)$, $\eta = \frac{c^2 \delta^2}{16 n^4 h m}$, the expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies $\sum\limits_{t=0}^{T-1}{ \sE \left[ \tilde{r}_{i, A_t} \right] } \le  \frac{32n^8h}{ c^4 \delta^4} \cdot \log{T} + 2 \cdot T^{\frac{2}{3} + \beta }$, $\forall \beta > 0$.
\end{thm}
\begin{proof}
According to \cref{alg:policy_gradient_uniform_exploration}, $T$ rounds are divided into two parts, where the uniform policy and $\rvpi_i\left( \rmW(t) \right)$ are used to sample and take actions, respectively. Therefore the regret consists of two corresponding parts, i.e., $\sum\limits_{t=0}^{T-1}{ \sE \left[ \tilde{r}_{i, A_t} \right] } =$
\begin{equation*}
\begin{split}
     &\sum\limits_{t=0}^{T^{\frac{2}{3} + \beta}}{ \expectation\limits_{A_t \sim \gU\left[h\right]}{ \left[ \tilde{r}_{i, A_t} \right] }} + \sum\limits_{t=T^{\frac{2}{3} + \beta} + 1}^{T-1}{ \rvpi_i\left( \rmW(t) \right)^\top \rvtilder_i } \\
     &\le \sum\limits_{t=0}^{T^{\frac{2}{3} + \beta}}{ 1 } + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i } \\
     &\qquad + \sum\limits_{t=T^{\frac{2}{3} + \beta}}^{T-1}{ \rvpi_i\left( \rmW(t) \right)^\top \left( \rvtilder_i -  \hat{\rvtilder}_i \right) }  \\
     &\le T^{\frac{2}{3} + \beta } + 1 + \frac{2n^4}{\eta m c^2 \delta^2} \cdot \sum\limits_{t=1}^{T-1}{ \frac{1}{t} } +  \sum\limits_{t=0}^{T-1}{ \left\| \hat{\rvtilder}_i - \rvtilder_i \right\|_\infty } \\
     &\le 2 T^{\frac{2}{3} + \beta } + \frac{32n^8h}{ c^4 \delta^4}\log{T} -O\left(1 \right) + 2 T \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\},
\end{split}
\end{equation*}
where the first equation is by \cref{eq:vanilla_policy_gradient_expected_regret}, the first inequality is by $\rvtilder_i \in \left[ 0, 1\right]^h$, and the second inequality is according to \cref{thm:surrogate_expected_loss_convergence}. The third inequality is by \cref{thm:loss_estimation_hoeffding} and H{\"o}lder's inequality. Since $2 T \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} \in o(1)$, $\forall \beta > 0$, we obtain the expected regret upper bound.
\end{proof}

\cref{alg:policy_gradient_uniform_exploration} divides $T$ steps into two parts. In the first exploring phase, the agent uniformly samples actions without learning the NN policy. Intuitively, at the beginning, when the loss estimation is very inaccurate, early updating can probably hurt the NN policy.  While in the second playing-learning phase, since the loss estimation is good, the NN policy will keep reducing its surrogate expected loss, which is highly related to its true expected loss, which is identical with the expected regret of this phase.

\subsection{Exploring Phase}
\label{subsec:exploring_phase}

The exploring phase of \cref{alg:policy_gradient_uniform_exploration} provides us good estimations of the true mean loss/reward as follows.
\begin{thm}
\label{thm:loss_estimation_hoeffding}
    After the exploring phase of \cref{alg:policy_gradient_uniform_exploration},
\begin{equation*}
    \left\| \hat{\rvtilder}_i - \rvtilder_i \right\|_\infty \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} + T^{\beta - \frac{1}{3}}.
\end{equation*}
\end{thm}
\begin{proof}
    The $k$th action is sampled $\frac{T^{\frac{2}{3} + \beta} }{h}$ times, $\forall k \in [h]$. By Hoeffding's inequality, $\forall k \in [h]$,
\begin{equation}
\label{eq:loss_estimation_hoeffding}
\begin{split}
    &\pr\left\{ \left| \hat{\tilde{r}}_{i, k} - \tilde{r}_{i,k} \right| > T^{\beta - \frac{1}{3}} \right\} = \pr\left\{ \left| \hat{r}_{i, k} - r_{i,k} \right| > T^{\beta - \frac{1}{3}} \right\} \\
    &\le 2 \exp\left\{ - 2 \cdot  \frac{T^{\frac{2}{3} + \beta}}{h} \cdot T^{2\beta - \frac{2}{3}} \right\} = 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\}.
\end{split}
\end{equation}
Denote $k^* = \argmax\limits_{k \in [h]}{ \left| \hat{\tilde{r}}_{i, k} - \tilde{r}_{i,k} \right| }$. Then $\left\| \hat{\rvtilder}_i - \rvtilder_i \right\|_\infty =$
\begin{equation*}
\begin{split}
    &\left| \hat{\tilde{r}}_{i, k^*} - \tilde{r}_{i,k^*} \right| \le 2 \exp\left\{ - \frac{2}{h} \cdot  T^{3\beta} \right\} \cdot 1 + 1 \cdot T^{\beta - \frac{1}{3}},
\end{split}
\end{equation*}
where the last inequality is by \cref{eq:loss_estimation_hoeffding}, $\left| \hat{\tilde{r}}_{i, k^*} - \tilde{r}_{i,k^*} \right| \le 1$, and $\pr\left\{ \left| \hat{\tilde{r}}_{i, k} - \tilde{r}_{i,k} \right| \le T^{\beta - \frac{1}{3}} \right\} \le 1$. 
\end{proof}

\subsection{Playing-Learning Phase}
\label{subsec:playing_learning_phase}

The good estimation of the true mean loss $\hat{\rvtilder}_i$ obtained at the end of the exploring phase will be used to train the NN policy $\rvpi_i\left( \rmW(t) \right)$. We carefully combine the overparameterized NN optimization theory for supervized learning \citep{li2018learning,allen2018convergenceB} with the optimal exploration conditions in RL (explained later on in \cref{subsec:exploration_in_policy_learning}), and show that the ``surrogate" expected loss $\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i$ converges at rate $O\left(\frac{1}{t} \right)$. We first prove the main result, then show some intuitions with lemmas, the proofs of which can be found in the appendix.

\begin{thm}
\label{thm:surrogate_expected_loss_convergence}
    Assume $m \in \Theta\left( \frac{n^{10}}{c^4 \delta^4 \varepsilon^2} \right)$, $\eta = \frac{c_t^2 \delta^2}{16 n^4 h m}$, During the playing-learning phase, $\forall t \ge T^{\frac{2}{3} + \beta}$, denote $t^\prime \triangleq t - T^{\frac{2}{3} + \beta} \ge 0$. After $t^\prime = \frac{2n^4}{\eta m c^2 \delta^2 \varepsilon}$ iterations, $\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \le \varepsilon$, $\forall \varepsilon > 0$.
\end{thm}
\begin{proof}
    By \cref{lem:gradient_coupling}, let $\tau = \frac{\sigma}{n}$, there are $\Omega\left( m \right)$ of $\rvw_r(t)$ such that $\frac{d\tilde{\ell}(t)}{d \rvw_r(t)} = \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)}$, $\forall t \in O\left( \frac{\sigma}{\eta n} \right)$. Let $\rmW(t+1) = \rmW(t) - \eta \cdot \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)}$, by \cref{lem:parameter_smoothness},
\begin{equation*}
\begin{split}
    &\rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i - \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \\
    &\le - \eta \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2 + 8 h m \eta^2 \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2 \\
    &= - \eta \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)} \right\|_2^2 } \\
    &\qquad + 8 h m \eta^2 \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)} \right\|_2^2 } \\
    &\le - \left( \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \right)^2 \cdot \left[ \frac{\eta m c_t^2 \delta^2}{n^4} - 8 \eta^2 h m^2 \right] \\
    &= - \left( \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \right)^2 \cdot \left( \frac{\eta m c_t^2 \delta^2}{2n^4} \right).
\end{split}
\end{equation*}
Divided by $\left( \rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i\right) \cdot \left( \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \right)$,
\begin{equation*}
\begin{split}
    &\frac{1}{\rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i} - \frac{1}{\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i} \ge \\
    &\frac{\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{\rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i} \cdot \left( \frac{\eta m c_t^2 \delta^2}{2n^4} \right) \ge \frac{\eta m c_t^2 \delta^2}{2n^4}.
\end{split}
\end{equation*}
Denote $c \triangleq \min\limits_{t \ge 0}{\left\{ c_t \right\}} > 0$ because $\rvpi_i\left( \rmW(t) \right)$ is a softmax policy.
Sum up the inequality from $0$ to $t$,
\begin{equation*}
\begin{split}
    \frac{1}{\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i} \ge \frac{\eta m c^2 \delta^2}{2n^4} \cdot t.
\end{split}
\end{equation*}
Therefore, after $t =  \frac{2n^4}{\eta m c^2 \delta^2 \varepsilon}$ policy gradient updates, $\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \le \varepsilon$. And the $t$th policy gradient update in the play-learning phase corresponds to $t + T^{\frac{2}{3} + \beta}$ in \cref{alg:policy_gradient_uniform_exploration}. Finally, make sure that all the update iterations terminate within $\frac{\tau}{\eta}$, i.e.,
Let $\frac{2n^4}{\eta m c^2 \delta^2 \varepsilon} \le \frac{\sigma}{n \eta} = \frac{1}{n \eta \sqrt{m}}$, we obtain the results.
\end{proof}

\cref{thm:surrogate_expected_loss_convergence} relies on two arguments. First, the surrogate expected loss is smooth in the logit space, and small policy gradient updates preserve the signs of ReLU outputs, therefore highly correlates the logit derivative and the policy gradient. Second, by the overparameterization theory, gradient norm is lower bounded by expected loss around initialization, which means there is no bad local minima near the randomly initialized NN policy $\rvpi_i\left( \rmW(0) \right)$.

\begin{lem}
\label{lem:logit_smoothness}
Let $\rvpi_i\left( \rvo^\prime \right)$ and $\rvpi_i\left( \rvo \right)$ be softmax policies of logit vectors $\rvo^\prime, \rvo \in \sR^h$, respectively.
\begin{equation*}
\small
    \rvpi_i\left( \rvo^\prime \right)^\top \hat{\rvtilder}_i \le \rvpi_i\left( \rvo \right)^\top \hat{\rvtilder}_i + \left\langle \frac{d \rvpi_i\left( \rvo \right)^\top \hat{\rvtilder}_i}{d \rvo}, \rvo^\prime - \rvo \right\rangle + 2 \left\| \rvo^\prime - \rvo \right\|_2^2.
\end{equation*}
\end{lem}
Let $\rvo(t+1) = \rvo(t) - \eta \cdot \frac{d \rvpi_i\left( \rvo(t) \right)^\top \hat{\rvtilder}_i}{d \rvo(t)}$, by \cref{lem:logit_smoothness},
\begin{equation*}
\begin{split}
\small
    \rvpi_i\left( \rvo(t+1) \right)^\top \hat{\rvtilder}_i \le \rvpi_i\left( \rvo(t) \right)^\top \hat{\rvtilder}_i - \left( \eta - 2 \eta^2 \right) \left\| \frac{d \rvpi_i\left( \rvo(t) \right)^\top \hat{\rvtilder}_i}{d \rvo(t)} \right\|_2^2.
\end{split}
\end{equation*}
Note the logit derivative norm is lower bounded by loss,
\begin{equation}
\label{eq:logit_derivative_lower_bound}
\begin{split}
    \left\| \frac{d \rvpi_i\left( \rvo \right)^\top \hat{\rvtilder}_i}{d \rvo} \right\|_2 &\ge \left| \pi_{i,\hat{k}_i^*} \cdot \left( \hat{\tilde{r}}_{i,\hat{k}_i^*} - \rvpi_i\left( \rvo \right)^\top \hat{\rvtilder}_i \right) \right| \\
    &= \pi_{i,\hat{k}_i^*} \cdot \rvpi_i\left( \rvo \right)^\top \hat{\rvtilder}_i,
\end{split}
\end{equation}
where $\hat{k}_i^* \triangleq \argmax\limits_{ k \in [h]}{\left\{ \hat{r}_{i, k} \right\}}$, thus $\hat{\tilde{r}}_{i,\hat{k}_i^*} = \max\limits_{k \in [h]}{\left\{ \hat{r}_{i, k} \right\}} - \hat{r}_{i, \hat{k}_i^*} = 0$. Let $\eta = \frac{1}{4}$, by \cref{lem:logit_smoothness} and \cref{eq:logit_derivative_lower_bound}, we have $\rvpi_i\left( \rvo(t) \right)^\top \hat{\rvtilder}_i \le \frac{8}{c^2 t}$, $\forall t > 0$, where $c \triangleq \min\limits_{t \ge 0}{ \left\{ \pi(t)_{i, \hat{k}_i^*}\right\}}  > 0$ because $\rvpi_i\left( \rvo \right)$ is softmax transform of $\rvo$. It is intuitively reasonable that small $c$ means rarely exploring the optimal action $\hat{k}_i^*$, which will lead to long convergent time.

However, \cref{lem:logit_smoothness} is not good enouth because in practice, $\rvpi_i$ is updated in the parameter space rather than the logit space. When the parameters are updated from $\rmW^\top \triangleq \left[ \rvw_1, \rvw_2, \dots, \rvw_m \right]$ to ${\rmW^\prime}^\top \triangleq \left[ \rvw_1^\prime, \rvw_2^\prime, \dots, \rvw_m^\prime \right]$,
\begin{equation*}
\begin{split}
    \rvo^\prime - \rvo = \rmA \left[ \sigma \left( \rmW^\prime \rvs_i \right) - \sigma \left( \rmW \rvs_i \right) \right].
\end{split}
\end{equation*}
The difficulty is that after updating the parameters, possibly many signs in the ReLU components also change. This can be circumvented by restraining the updates around the initialization \citep{li2018learning}.

\begin{lem}
\label{lem:gradient_coupling}
	Define the pseudo policy gradient as,
\begin{equation*}
\begin{split}
\small
	\frac{d \tilde{\ell}(t)}{d \rmW(t)} \triangleq \tilde{\rmD} \rmA^\top \left[ \Delta\left( \rvpi_i\left(\rmW(t)\right) \right) - \rvpi_i\left(\rmW(t)\right) \rvpi_i\left(\rmW(t)\right)^\top \right] \hat{\rvtilder}_i \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD}_{k,k} \triangleq \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\}$, $\forall k \in [m]$, is a diagonal matrix. Note the true policy gradient is, $\frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \triangleq $
\begin{equation*}
\begin{split}
\small
    \rmD(t) \rmA^\top \left[ \Delta\left( \rvpi_i\left(\rmW(t)\right) \right) - \rvpi_i\left(\rmW(t)\right) \rvpi_i\left(\rmW(t)\right)^\top \right] \hat{\rvtilder}_i \rvs^\top.
\end{split}
\end{equation*}
where $\rmD(t)_{k,k} \triangleq \sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\}$, $\forall k \in [m]$. For any $\tau > 0$, with probability at least $1 - \frac{\sqrt{2}n\tau}{\sqrt{\pi}\sigma}$, $\forall t \in O\left(\frac{\tau}{\eta}\right)$, $\forall r \in [m]$,
\begin{equation*}
	\frac{d\tilde{\ell}(t)}{d \rvw_r(t)} = \frac{d \rvpi_i\left(\rmW(t)\right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW(t)$.
\end{lem}

\cref{lem:gradient_coupling} implies that for bounded numbers of policy gradient updates, the signs of the ReLUs will not change, i.e., $\sI\left\{ \rvw_r(t)^\top \rvs_i > 0 \right\} = \sI\left\{ \rvw_r(0)^\top \rvs_i > 0 \right\}$. Combine \cref{lem:logit_smoothness} with \cref{lem:gradient_coupling}, we have the smoothness property of the surrogate expected loss in the parameter space.
\begin{lem}
\label{lem:parameter_smoothness}
    $\rmW(t+1) = \rmW(t) - \eta \cdot \frac{d \rvpi_i(t)^\top \hat{\rvtilder}_i}{d \rmW(t)}$, $t \in O\left( \frac{\tau}{\eta}\right)$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
\small
    &\rvpi_i\left( \rmW(t+1) \right)^\top \hat{\rvtilder}_i \le \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i \\
    &- \eta \cdot \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2 + 8 h m \eta^2 \left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rmW(t)} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}

To use \cref{lem:parameter_smoothness} we need upper bound the last term in \cref{eq:parameter_smoothness} by the policy surrogate expected loss.
\begin{lem}
\label{lem:gradient_upper_bound}
$\forall r \in [m]$, $\forall t \ge 0$,
\begin{equation*}
\begin{split}
	\left\| \frac{d \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i}{d \rvw_r(t)} \right\|_2 \le \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_i.
\end{split}
\end{equation*}
\end{lem}

Now by the key insight of the recent progresses of the overparameterized NN optimization theory, with constants probability, the pseudo gradient norm is lower bounded by the objective \citep{li2018learning}. However, unlike the supervised learning, RL has exploration issue, as shown in \cref{subsec:exploration_in_policy_learning}. Our result contains an exploration related term, which is consistent with \cref{eq:logit_derivative_lower_bound}, making guarantees for exploring the optimal action necessary during learning.

\begin{lem}
\label{lem:gradient_lower_bound}
	Denote $\hat{k}_i^* \triangleq \argmax\limits_{ k \in [h]}{\left\{ \hat{r}_{i, k} \right\}}$, i.e., the optimal action at state $\rvs_{i}$ using the estimated reward $\rvhat_i$. If $\pi\left(\rmW(t)\right)_{i, \hat{k}_i^*} > c_t > 0$, with probability at least $\Omega\left( \frac{\delta}{n} \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}(t)}{d \rvw_r(t)} \right\|_2 \ge \Omega\left( \frac{\delta}{n^2} \right) \cdot c_t \cdot  \rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_{i}.
\end{split}
\end{equation*}
\end{lem}

\cref{lem:gradient_lower_bound} generalizes the overparameterized NN optimization theory into the RL settings. By \cref{lem:gradient_lower_bound}, whenever the policy surrogate expected loss $\rvpi_i\left( \rmW(t) \right)^\top \hat{\rvtilder}_{i}$ is large, with enough exploration of the suggorate optimal action ($\pi\left(\rmW(t)\right)_{i, \hat{k}_i^*} > c_t > 0$), with constant probability, the pseudo policy gradient norm will also be large. Therefore, combining \cref{lem:gradient_lower_bound} with \cref{lem:gradient_coupling}, the true policy gradient norm is also large, which is necessary for using \cref{lem:parameter_smoothness}. Applying all the stated lemmas, the policy surrogate expected loss converges as shown in \cref{thm:surrogate_expected_loss_convergence}.

\subsection{Exploration in Policy Learning}
\label{subsec:exploration_in_policy_learning}

In \cref{subsec:vanilla_policy_gradient}, it is claimed that the vanilla policy gradient method will suffer the lack of exploration issue. Now we provide some intuitions. Consider the derivative of the true expected loss with respect to the logits,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi_i\left( \rvo \right)^\top \rvtilder_i}{d \rvo} = \left[ \Delta\left( \rvpi_i \right) - \rvpi_i \rvpi_i^\top \right] \rvtilder_i,
\end{split}
\end{equation}
where $\Delta\left( \rvpi_i \right)$ is a diagonal matrix with $\Delta\left( \rvpi_i \right)_{k,k} = \pi_{i,k}$, $\forall k \in [h]$. For the $k$th action, the derivative value is $\pi_{i,k} \cdot \left( \tilde{r}_{i,k} - \rvpi_i^\top \rvtilder_i \right)$. Suppose the $k$th action is worth learning, i.e, $\rvpi_i^\top \rvtilder_i - \tilde{r}_{i,k} > 0$ is large, meaning this action will occur loss $\tilde{r}_{i,k}$ much smaller than the expected loss of the current policy, so the agent should increase its action logit. But if $\pi_{i,k}$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi_{i,k}$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi_{i,k}$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{i,k}(t) > c_t > 0$ should hold for each time step $t$, where $c_t$ is a constant (cannot be something like $\frac{1}{t^\alpha}$, $\alpha> 0$). In particular, to learn an optimal policy, $\pi_{i,k_i^*}(t) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $\tilde{r}_{i,k_i^*} = 0$, and $c \triangleq \min\limits_{t \ge 0}{\left\{  c_t \right\}}$.

Consider policy update using the logit derivatives \cref{eq:logit_derivative} with the true loss as the objective. With learning rate $\eta > 0$, for each action $k \in [h]$, the logit increment between two consecutive time steps is,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o\left( t+1 \right)_{k} - o\left( t \right)_{k} = \eta \cdot \pi\left( \rvo(t)\right)_{i,k} \cdot \left( \rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} \right),
\end{split}
\end{equation*}
which mean as long as $\pi\left( \rvo(t)\right)_{i,k} > 0$, for any valuable action $k$ with its true action loss smaller than the current policy expected loss, i.e., $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} > 0$, at the next time step $t+1$, the policy logit for this action will increase. While for any bad actions with $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k} < 0$, their logits will decrease.

Now consider the initialized policy $\rvpi_i\left( \rvo(0) \right)$, which is (very close to) the uniform policy, with $\pi\left( \rvo(0)\right)_{i,k_i^*} \approx \frac{1}{h} \in \Omega(1)$. Also note that $\rvpi_i\left( \rvo(t) \right)^\top \rvtilder_i - \tilde{r}_{i,k_i^*}$ is larger than any other action $k$, which is consistent with intuition because the optimal action is the most valuable for learning. With the same learning rate, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
$\forall t \ge 0$,
\begin{equation*}
    \pi\left( \rvo(t+1)\right)_{i,k_i^*} \ge \pi\left( \rvo(t)\right)_{i,k_i^*} \in \Omega(1).
\end{equation*}
\end{lem}

With this result in mind, \cref{alg:policy_gradient_uniform_exploration} actually does similar things. First, after the exploring phase, it obtains a good estimation $\hat{\rvtilder}_i$ of the true loss $\rvtilder_i$, thus the surrogate expected loss will be closed to the true expected loss. Second, the initialized policy $\rvpi_i\left( \rmW(0) \right)$ is very closed to the uniform policy with $\pi\left( \rmW(0)\right)_{i, \hat{k}_i^*} \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is restrained around initialization \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, after each policy gradient update, the optimal action logit will increase more than any other  suboptimal actions, which implies similar results with \cref{lem:optimal_probability_increse_logit_space} as follows,
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
$\forall t \ge 0$,
\begin{equation*}
    \pi\left( \rmW(t+1)\right)_{i,\hat{k}_i^*} \ge \pi\left( \rmW(t)\right)_{i,\hat{k}_i^*} \in \Omega(1).
\end{equation*}
\end{lem}
By \cref{lem:optimal_probability_increse_parameter_space}, we can safely replace the $c$ and $c_t$ values in the main result \cref{thm:main_result} and all the other intermediate lemmas and theorems, without incurring any additional regret dependent on $T$.